<!doctype html>
<html lang="zh"><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/fireworks.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>NLP基础 - CheaSim Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="cheasim&#039;s blog"><meta name="msapplication-TileImage" content="/img/1.gif"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cheasim&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="nlp 深度学习基础将每一个模型以  简单介绍 解决的问题 代码 优缺点 使用tips  来归类。比较现代的会分析分析。 tf-idf全称(term frequency-inverse document frequency)，TF指的是词频，IDF指的是逆文本频率指数。他们的计算公式如下： $TF_w&amp;#x3D;\cfrac{N_w}{N}$，一个词在该句子中出现的频率。 $IDF_w&amp;#x3D;\log{\cfra"><meta property="og:type" content="blog"><meta property="og:title" content="NLP基础"><meta property="og:url" content="https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html"><meta property="og:site_name" content="CheaSim Blog"><meta property="og:description" content="nlp 深度学习基础将每一个模型以  简单介绍 解决的问题 代码 优缺点 使用tips  来归类。比较现代的会分析分析。 tf-idf全称(term frequency-inverse document frequency)，TF指的是词频，IDF指的是逆文本频率指数。他们的计算公式如下： $TF_w&amp;#x3D;\cfrac{N_w}{N}$，一个词在该句子中出现的频率。 $IDF_w&amp;#x3D;\log{\cfra"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg"><meta property="og:image" content="https://github.com/YZHANG1270/Markdown_pic/blob/master/2018/11/RNN_01/001.png?raw=true"><meta property="og:image" content="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png"><meta property="og:image" content="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg"><meta property="article:published_time" content="2021-01-12T13:55:19.000Z"><meta property="article:modified_time" content="2021-01-17T05:53:46.298Z"><meta property="article:author" content="CheaSim"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html"},"headline":"CheaSim Blog","image":["https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg","https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png","https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg"],"datePublished":"2021-01-12T13:55:19.000Z","dateModified":"2021-01-17T05:53:46.298Z","author":{"@type":"Person","name":"CheaSim"},"description":"nlp 深度学习基础将每一个模型以  简单介绍 解决的问题 代码 优缺点 使用tips  来归类。比较现代的会分析分析。 tf-idf全称(term frequency-inverse document frequency)，TF指的是词频，IDF指的是逆文本频率指数。他们的计算公式如下： $TF_w&#x3D;\\cfrac{N_w}{N}$，一个词在该句子中出现的频率。 $IDF_w&#x3D;\\log{\\cfra"}</script><link rel="canonical" href="https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html"><link rel="icon" href="/img/1.gif"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-R7VVCLV2ZB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-R7VVCLV2ZB');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-2-tablet is-8-desktop is-10-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-12T13:55:19.000Z" title="2021-01-12T13:55:19.000Z">2021-01-12</time>发表</span><span class="level-item"><time dateTime="2021-01-17T05:53:46.298Z" title="2021-01-17T05:53:46.298Z">2021-01-17</time>更新</span><span class="level-item">30 分钟读完 (大约4480个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP基础</h1><div class="content"><h1 id="nlp-深度学习基础"><a href="#nlp-深度学习基础" class="headerlink" title="nlp 深度学习基础"></a>nlp 深度学习基础</h1><p>将每一个模型以</p>
<ol>
<li>简单介绍</li>
<li>解决的问题</li>
<li>代码</li>
<li>优缺点</li>
<li>使用tips</li>
</ol>
<p>来归类。比较现代的会分析分析。</p>
<h2 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h2><p>全称(term frequency-inverse document frequency)，TF指的是词频，IDF指的是逆文本频率指数。他们的计算公式如下：</p>
<p>$TF_w=\cfrac{N_w}{N}$，一个词在该句子中出现的频率。</p>
<p>$IDF_w=\log{\cfrac{Y}{Y_w+1}}$，$Y_w$是所有文档中包含该词的文档个数。+1是方式分母为0.</p>
<p>TF-IDF的<strong>思想</strong>在于，一个词如果在一段小文本中出现得越多，那么他对这段文本的权重就越大，但是如果在所有的文本中，他出现的次数都很多，就像计算信息熵一样，在所有情况下出现的概率很大时，那么词的信息就很少。所有使用IDF来抵消一些常用词的影响。综上，计算公式为</p>
<p>$TFIDF_w=TF_w \times IDF_w$</p>
<blockquote>
<p>注意，对于不同的样本的同一个词，$TF$可能是不同的，但是$IDF$是相同的。</p>
</blockquote>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>TF-IDF相当于在以前把关键字作为短文本表示的基础上加入了一个正则化，削弱了高频词的权重。在一些简单的文本匹配(对于给定的问题，与已知文本的词语将TFIDF加和得到相似度)，文本分类上可以起到一定的效果。</p>
<hr>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFIDF</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    self.documents_list = documents_list</span><br><span class="line">    self.tf = []</span><br><span class="line">    self.idf = &#123;&#125;</span><br><span class="line">    df = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> documents_list:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> word <span class="keyword">in</span> document:</span><br><span class="line">        temp[word] = temp.get(word, <span class="number">0</span>) + <span class="number">1.</span>/<span class="built_in">len</span>(document)</span><br><span class="line">     	self.tf.append(temp)</span><br><span class="line">      <span class="comment"># 出现过的词，都+1</span></span><br><span class="line">   		<span class="keyword">for</span> k <span class="keyword">in</span> temp.keys():</span><br><span class="line">        df[k] = df.get(k, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> df.items():</span><br><span class="line">      self.idf[k] = np.log(<span class="built_in">len</span>(documents_list) / (v + <span class="number">1</span>))</span><br><span class="line">    self.tfidf = []</span><br><span class="line">		<span class="keyword">for</span> tf_sentence <span class="keyword">in</span> self.tf:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> tf_sentence.items():</span><br><span class="line">        temp[k] = v * self.idf[k]</span><br><span class="line">      self.tfidf.append(temp)</span><br><span class="line">tfidf = TFIDF([<span class="string">&#x27;I have a pen&#x27;</span>.split(), <span class="string">&#x27;I have an apple&#x27;</span>.split(), <span class="string">&#x27;Bang, apple pen&#x27;</span>.split()])</span><br><span class="line">print(tfidf.tf)</span><br><span class="line">print(tfidf.idf)</span><br><span class="line">print(tfidf.tfidf)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>一种无监督的生成句子词语向量的方法。</li>
<li>可以很快地找到一句话的关键字</li>
<li>耗费的计算资源较少。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>句子向量，或者词语向量没有上下文信息，词语与词语之间的位置关系也没有融入到表示当中。</li>
<li>会将生僻词作为关键词，但其实生僻词意义不大。</li>
<li>人名地名比较难以区分。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113017752">https://zhuanlan.zhihu.com/p/113017752</a></p>
</blockquote>
<p>ps: python dict是真好用。</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>word2vec说明白了也就是和TFIDF一样的<strong>将词语使用一个$f(x)$映射到数值的向量空间当中</strong>。由于词语不像是像素点有着天然的数值表示，word2vec针对词语转化为计算机可以理解的表示。</p>
<p>word2vec的<strong>思想</strong>是一个词的意思由它旁边的词构成。就像老话说的好，物以类聚，人以群分。但其实word2vec的损失函数和模型解决的问题是有一点割裂开的。word2vec想解决的问题是生成稠密的word embedding，而优化损失函数的目的是让两个词在window下的关系符合文本。损失函数是想，在一个窗口下，如果我中心词是”帅哥”,那么模型应该能推测前两个词是”我”和”是”。因为在样本中”我是帅哥”出现过很多次。之后由于损失函数的计算中，预测的条件概率是通过词向量的相似度来计算的，所以在优化模型中，也就达成了相似词汇产生相似词向量的目的。</p>
<p>word2vec又分为两种模型：</p>
<ol>
<li>skip-gram： 使用中心词周围的词来预测中心词。</li>
<li>CBOW： 使用中心词来预测周围的词。</li>
</ol>
<p>每一个词汇表示成为两个$d$维的向量，用来计算条件概率。$v_i \in \mathbb{R^d}$.之后每一个window下在中心词预测上下文的条件概率就是。</p>
<p>$$P(O=o|C=c)=\cfrac{\exp(u_o^Tv_c)}{\sum_{w \in Vocab} \exp(u_w^Tv_c)}$$</p>
<p>使用极大似然估计就是，$L(\theta)=\prod_{t=1}^T \prod_{-m \leq j \leq m ,j \neq 0} P(w_{t+j} | w_t; \theta)$</p>
<p>之后对极大似然估计常规操作，取log再正负相反，从而作为损失函数求最小。</p>
<p>![image-20210113160811012](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113160811012.png)</p>
<p>$W_{V\times N}$就是由中心词汇组成的矩阵，$W’_{N \times V}$就是上下文词汇表示组成的矩阵。</p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip gram"></a>skip gram</h3><p>每一个词汇表示成为两个$d$维的向量，用来计算条件概率。$v_i \in \mathbb{R^d}$.这里我们直接想象成深度学习的模型，那么我们的输入是一个one hot embedding，输出是维度为词表的向量。之后我们要使得向量在上下文词上的值接近为1(在激活归一化之后)。由于词表一般很长，所以训练skip gram的时候有一个trick。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>使用平均加权的one hot embedding输入上下文词汇，去预测中心词汇。</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>对于整个词表计算一次softmax的开销是很大的$O(|V|)$其中$|V|&gt;&gt;|d|$，所以我们需要构建一种不一样的softmax来处理这个问题。这个就是Hierarchical softmax 等级制的softmax，它首先会将词表进行分层，构建成一颗平衡二叉树，🌲上的节点就是我们要判断的word，我们知道平衡二叉树的深度是$O(\log n)$。经过$\log n$次的判断之后，就可以计算损失函数了。一次forward复杂度是$O(\log |V| * d<em>2)$，其中2是在左右选择，相比于$O(|V|+d</em>|V|)$减少了很多。计算P需要把从根节点到叶子节点上的每个节点挨个算一遍概率。</p>
<p>![image-20210113164654789](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113164654789.png)</p>
<h4 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h4><p>在一次训练的时候，skip gram 只会输入一个词，很稀疏，浪费了其他的embedding训练。在训练的时候，不使用矩阵直接乘，而是使用挑选比如(1+10)10个负样本更新矩阵。挑选的公式为出现评率比较大的。</p>
<p>$$P(w_i)=\cfrac{f(w_i)^{0.75}}{\sum_{j=0}^nf(w_j)^{0.75}}$$</p>
<h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>解决one-hot embedding中过于稀疏，以及难以表达语义特征的问题。</p>
<hr>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2Vec</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>



<hr>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li><p>无监督训练生成词向量</p>
</li>
<li><p>对于相似的词汇有着很好的解释性，Man - King = Woman - Queen</p>
</li>
</ul>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><p>无法一词多义。</p>
</li>
<li><p>训练时没有加入位置信息，训练效率较低。</p>
</li>
</ul>
<h2 id="text-cnn"><a href="#text-cnn" class="headerlink" title="text-cnn"></a>text-cnn</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">Convolutional Naural Networks for Sentence Classification</a></p>
<p>拷贝忍者卡卡sei，直接抄CV的CNN就完事了。我们可以将一句话利用word embedding看成是一副图像，比如长度为10的句子，词向量维度为300。那么这个句子的输入就是$10 \times 300$的矩阵。之后我们就可以像图像一样处理文本了。</p>
<p><img src="https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg" alt="image-20210113173917546"></p>
<p>模型可以分为三层。</p>
<ol>
<li>输入层是一个$k \times n$的矩阵</li>
<li>卷积层与CV有一些区别，因为我们需要把词向量看做是一个整体，所以不会在横（纵？）方向上进行卷积，卷积窗口只会上下移动。核大小为$filter_size \times embedding_size$.文中定义<code>filiter_size</code>为[3,4,5]。将局部的信息聚合。每一个不同的卷积核都会生成不同的<code>feature map</code>，比如输入是$10 \times 300$，之后经过128个大小为3的卷积操作，会生成128个维度为10的向量。</li>
<li>pooling层，由于要处理变长文本，所以是对每一个feature map上取最大值作为输出，所以最终得到的是一个128维度的向量。</li>
<li>FFN和Softmax 常规操作，分类模型获得每一类的概率。</li>
</ol>
<h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>将CNN引入到NLP当中，从而减少了模型的参数，并在CNN在捕捉局部信息时有奇效。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul>
<li>跨时代地提出了CNN在NLP领域的应用</li>
<li>实验做得很详细，针对预训练，随机生成的词向量都进行了比对。（是不是这个给bert一点思考，不需要一个word embedding，随机初始化就好了）</li>
</ul>
<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>CNN卷积对于句子来说还是太小了。没有全局信息。一个CNN只能估计5-gram的信息</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102426363">https://zhuanlan.zhihu.com/p/102426363</a></p>
</blockquote>
<h2 id="rnn-lstm"><a href="#rnn-lstm" class="headerlink" title="rnn lstm"></a>rnn lstm</h2><p>RNN想对于CNN模型来说多了很多变种。首先来说一下RNN的<strong>思想</strong>吧。RNN灵感来源于人类进行阅读过程中，会从左到右一个字一个字地读入文字，之后再得到自己的理解。那么是否有模型能够捕捉这种从左到右的时序信息呢？那就是RNN(Recurrent Neural Network)。RNN由于结构精巧有很多变种。</p>
<p><img src="https://github.com/YZHANG1270/Markdown_pic/blob/master/2018/11/RNN_01/001.png?raw=true" alt="rnn"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM使用门机制来进行有效地方式了梯度爆炸或者梯度消失。他三个门的公式分别是输入门，遗忘门，输出门。</p>
<p>$f_i=$</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/">http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/</a></p>
</blockquote>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>代码来自transformers==3.3.1</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>BERT模型的输入由三部分得到，token embdding, segment embedding, position embedding。</p>
<h4 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h4><p>对于所有文字来说，计算机都是无法理解的，需要转化为浮点向量或者整型向量。BERT采用的是WordPiece tokenization，是一种数据驱动的分词算法，他以char作为最小的粒度，不断地寻找出现最多以char为单位组成的token，之后将word进行分词，分为一个一个的token。比如ing这个会经常出现在英文当中，所以WordPiece 会吧”I am playing the computer games”分为”I am play ##ing the computer games”。为了解决OOV问题。词表中有30522个词。</p>
<p>在WordPiece 分词的基础上，之后会加入4个特殊词汇[CLS],[SEP],[PAD],[UNK]。[CLS]加入到句首，不参与预训练，针对下游任务进行fine-tune。[SEP]作为句尾以及分段标志。[PAD]是填充，使得句子长度一样，方便批处理，[UNK]是表明不在词表当中。将他们转换成one-hot embedding之后，接一个embedding层，将词转化为最初的词向量。</p>
<h4 id="segment-embedding"><a href="#segment-embedding" class="headerlink" title="segment embedding"></a>segment embedding</h4><p>segment embedding 仅仅作为区分两个句子来使用。在预训练中，还要使用预测句子是否相邻作为预训练任务之一。在输入时，也会经过一个线性层来形成segment embedding.</p>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>position embedding 纯使用nn.embedding 获得。训练了一个位置<em>词表</em>。$W\in \mathbb{R^{512 \times 768}}$。输入就是[0,1,2,…,len_seq-1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, :seq_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h3 id="transformer-encoder-层"><a href="#transformer-encoder-层" class="headerlink" title="transformer encoder 层"></a>transformer encoder 层</h3><p>transformer encoder层主要如下图所示</p>
<p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png" alt="image"></p>
<p>使用一个上面所说的输入，经过<code>Multi-Head Attention</code>，在通过残差连接以及<code>Layer Normalization</code>，之后通过<code>FFN</code>以及又一个残差连接作为输出。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>BERT使用的是自注意力机制，即用于$Q,K,V$全部源自同一个向量。注意力机制使用了<strong>上下文</strong>的信息来对每一个token进行表示。计算机通过利用上下文的信息，对每一个token进行理解。比如“冬天到了，天气变冷了。”BERT会根据大量该类的文本，将冬天和冷的语义进行融合。Attention值的计算公式如下</p>
<p>$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_k}})V \in \mathbb{R^{len\times d}}$$</p>
<p>其中，$Q,K,V\in \mathbb{R^{d\times k}},Q=xW_q,K=xW_k,V=xW_v$，$d$是隐层维度这里可以注意到，因为softmax是非线性的，所以这里的矩阵变换是没法单纯使用线性变换用$Wx$代替的。</p>
<hr>
<p><strong>多头</strong>在哪里<strong>多头</strong>。直接将原来的$x$进行转化后切分，详情看下图就懂了。每一个attention生成的维度都不高，拼起来就跟原来一样了。这张图有点问题，其实BERT没有$W^O$，因为$Z_0,…,Z_7$拼起来正好是$Z$。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg" alt="img1"></p>
<blockquote>
<p>BERT 实现中，多头的目的是降低参数的个数，增加表达能力。 类似于CNN多个卷积核？</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function) [1,1,1,0,0] -&gt; [0,0,0,-10000,-10000]</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>ps: 代码里实现attention_mask使用了加法，因为softmax中针对e^0也会输出1，所以我们要对于忽视的token进行$e^{-inf}$，才能使他在softmax之后的权重为0。</p>
<h3 id="add-and-norm"><a href="#add-and-norm" class="headerlink" title="add and norm"></a>add and norm</h3><p>常见的残差网络方式梯度消失，增加模型的训练，打破了网络的对称性，提升了网络的表征能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="FFN-and-Add-Norm"><a href="#FFN-and-Add-Norm" class="headerlink" title="FFN and Add Norm"></a>FFN and Add Norm</h3><p>先经过中间层3072，hidden_size扩大4倍。之后再经过一个缩小了。注意这里最后才有一个dropout。激活函数用的gelu，比relu稍微缓和了一些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="最后处理"><a href="#最后处理" class="headerlink" title="最后处理"></a>最后处理</h3><p>综上，把这个bert_layer叠个12层就行了。但是在最后输出的时候[CLS]有一个特殊的处理。输出的时候会经过一个线性层+一个<code>tanh</code>激活。</p>
<h3 id="NER-token-classification"><a href="#NER-token-classification" class="headerlink" title="NER  token classification"></a>NER  token classification</h3><p>在每一个token对应的输出加入一个线性分类层，对应所有的实体类型标签比如B-PER,I-PER。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109250703">https://zhuanlan.zhihu.com/p/109250703</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
</blockquote>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP基础</p><p><a href="https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>CheaSim</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-01-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-01-17</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5fb51469ef5d2e00120143cc&amp;product=sop" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/img/alipay.JPG" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.JPG" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">基于BERT的知识库问答系统</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html"><span class="level-item">如何在colab用pytorch_lightning白嫖TPU</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://www.cheasim.com/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html';
            this.page.identifier = 'uncategorized/2021/01/12/NLP基础.html';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'www-cheasim-com' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/1.gif" alt="Chea Sim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chea Sim</p><p class="is-size-6 is-block">Student from Z</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Ning Bo</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">99</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">83</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/CheaSim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/CheaSim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/www.cheasim.com"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1600/"><span class="level-start"><span class="level-item">1600</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/acm/"><span class="level-start"><span class="level-item">acm</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile" href="/categories/acm%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">acm模板</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="level-start"><span class="level-item">ac自动机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cf/"><span class="level-start"><span class="level-item">cf</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/cf1500/"><span class="level-start"><span class="level-item">cf1500</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/dfs/"><span class="level-start"><span class="level-item">dfs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/dp/"><span class="level-start"><span class="level-item">dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hash/"><span class="level-start"><span class="level-item">hash</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/html/"><span class="level-start"><span class="level-item">html</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pat/"><span class="level-start"><span class="level-item">pat</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%B8%93%E9%A1%B9%E7%BB%83%E4%B9%A0/"><span class="level-start"><span class="level-item">专项练习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%BF%9D%E7%A0%94/"><span class="level-start"><span class="level-item">保研</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">前端</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">学习笔记</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">学习计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">开发</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%90%AC%E7%A0%96/"><span class="level-start"><span class="level-item">搬砖</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A6%82%E7%8E%87dp/"><span class="level-start"><span class="level-item">概率dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">模板</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="level-start"><span class="level-item">线段树</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">考研</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94%E8%B7%AF%E6%BC%AB%E6%BC%AB/"><span class="level-start"><span class="level-item">考研路漫漫</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="level-start"><span class="level-item">蓝桥杯</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E8%A7%A3/"><span class="level-start"><span class="level-item">论解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-20T07:26:03.000Z">2021-01-20</time></p><p class="title"><a href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-14T06:15:14.000Z">2021-01-14</time></p><p class="title"><a href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-12T13:55:19.000Z">2021-01-12</time></p><p class="title"><a href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-07T09:40:38.000Z">2021-01-07</time></p><p class="title"><a href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-11T09:07:57.000Z">2020-12-11</time></p><p class="title"><a href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LCA/"><span class="tag">LCA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QA/"><span class="tag">QA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/acm/"><span class="tag">acm</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="tag">ac自动机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ajax/"><span class="tag">ajax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bert/"><span class="tag">bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bilstm/"><span class="tag">bilstm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cf/"><span class="tag">cf</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/colab/"><span class="tag">colab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crf/"><span class="tag">crf</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dfs/"><span class="tag">dfs</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dgl/"><span class="tag">dgl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dp/"><span class="tag">dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dptree/"><span class="tag">dptree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/echarts/"><span class="tag">echarts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gnn/"><span class="tag">gnn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/greedy/"><span class="tag">greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hash/"><span class="tag">hash</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hdoj/"><span class="tag">hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/html/"><span class="tag">html</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/js/"><span class="tag">js</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lca/"><span class="tag">lca</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pat/"><span class="tag">pat</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/re/"><span class="tag">re</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/struts2/"><span class="tag">struts2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tokenizer/"><span class="tag">tokenizer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86/"><span class="tag">二分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E7%94%9F/"><span class="tag">人生</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%9D%E7%A0%94/"><span class="tag">保研</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AB%E6%95%B0%E7%A0%81/"><span class="tag">八数码</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2hash/"><span class="tag">字符串hash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="tag">学习计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"><span class="tag">小技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tag">开发</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%BD%E5%8F%96/"><span class="tag">抽取</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/"><span class="tag">最大子段和</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%84%E9%80%A0/"><span class="tag">构造</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E5%BD%A2dp/"><span class="tag">树形dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"><span class="tag">树状数组</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8Chdoj/"><span class="tag">树状数组，hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/"><span class="tag">树链剖分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87dp/"><span class="tag">概率dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%8B%9F/"><span class="tag">模拟</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%9D%BF/"><span class="tag">模板</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%86%E6%90%9C/"><span class="tag">爆搜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%9B%E5%AE%A2%E7%BD%91/"><span class="tag">牛客网</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%8A%B6%E5%8E%8Bdp/"><span class="tag">状压dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="tag">线段树</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"><span class="tag">组合数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%86%E8%8A%82/"><span class="tag">细节</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%A9%E7%82%B9/"><span class="tag">缩点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C/"><span class="tag">网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%B5%9B/"><span class="tag">网络赛</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8C%83%E5%9B%B4/"><span class="tag">范围</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="tag">蓝桥杯</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A1%A5%E9%A2%98/"><span class="tag">补题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%BE%8B/"><span class="tag">规律</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/"><span class="tag">计算几何</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E8%A7%A3/"><span class="tag">论解</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%AA%E5%BF%83/"><span class="tag">贪心</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">配置</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%8F%E6%80%A7/"><span class="tag">随性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/"><span class="tag">预处理</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 CheaSim</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>