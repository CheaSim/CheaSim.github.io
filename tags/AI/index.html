<!doctype html>
<html lang="zh"><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/fireworks.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>标签: AI - CheaSim Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="cheasim&#039;s blog"><meta name="msapplication-TileImage" content="/img/1.gif"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cheasim&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="ACM chicken"><meta property="og:type" content="blog"><meta property="og:title" content="CheaSim Blog"><meta property="og:url" content="https://www.cheasim.com/"><meta property="og:site_name" content="CheaSim Blog"><meta property="og:description" content="ACM chicken"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.cheasim.com/img/og_image.png"><meta property="article:author" content="CheaSim"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cheasim.com"},"headline":"CheaSim Blog","image":["https://www.cheasim.com/img/og_image.png"],"author":{"@type":"Person","name":"CheaSim"},"description":"ACM chicken"}</script><link rel="icon" href="/img/1.gif"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-R7VVCLV2ZB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-R7VVCLV2ZB');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">AI</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-07T11:33:57.000Z" title="2020-12-07T11:33:57.000Z">2020-12-07</time>发表</span><span class="level-item"><time dateTime="2021-01-06T07:45:59.695Z" title="2021-01-06T07:45:59.695Z">2021-01-06</time>更新</span><span class="level-item">4 分钟读完 (大约637个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/07/pytorch-cheat-list.html">pytorch cheat_list</a></h1><div class="content"><h1 id="pytorch-操作小计"><a href="#pytorch-操作小计" class="headerlink" title="pytorch 操作小计"></a>pytorch 操作小计</h1><blockquote>
<p>torch==1.7.0</p>
</blockquote>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><h3 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h3><p>将List[tensor]变成tensor。<code>torch.stack(tensors,dim=0,out=None)</code>Concatenates a sequence of tensors along a new dimension.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">a = torch.stack([b, b], dim = <span class="number">0</span>) <span class="comment"># a.shape = (2,4)</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-gather"><a href="#torch-gather" class="headerlink" title="torch.gather"></a>torch.gather</h3><blockquote>
<p>torch.gather(<em>input</em>, <em>dim</em>, <em>index</em>, *, <em>sparse_grad=False</em>, <em>out=None</em>) → Tensor</p>
</blockquote>
<p>英文解释为Gathers values along an axis specified by dim.</p>
<p>看到比较有道理的应用场景是，在变长序列中gather到最后一个或者说倒数第几个元素。一般变长序列为<code>inputs = [[1,2,3,0,0], [2,3,4,5,0]]</code>。这时候想获得最后一个元素就可以。需要注意的点是，输出的tensor和index是相同<code>shape</code>的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">         	[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]])</span><br><span class="line">index = torch.tensor([[<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.long)</span><br><span class="line">last_inputs = torch.gather(inputs, dim=<span class="number">1</span>, index)</span><br><span class="line"><span class="string">&quot;&quot;&quot;tensor([[3],</span></span><br><span class="line"><span class="string">           [5]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="torch-expand"><a href="#torch-expand" class="headerlink" title="torch.expand"></a>torch.expand</h3><p>将tensor扩展维度，自动复制，十分好用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">5</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#tensor([[3, 1, 1],</span></span><br><span class="line"><span class="comment">#        [1, 2, 4]])</span></span><br><span class="line">a = a.unsqueeze(<span class="number">2</span>).expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[3, 3, 3],</span></span><br><span class="line"><span class="string">         [1, 1, 1],</span></span><br><span class="line"><span class="string">         [1, 1, 1]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[1, 1, 1],</span></span><br><span class="line"><span class="string">         [2, 2, 2],</span></span><br><span class="line"><span class="string">         [4, 4, 4]]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat"></a>torch.repeat</h3><p><code>repeat(*sizes) -&gt; Tensor</code>， 重复复制tensor在指定的维度上。其实有点类似于广播操作了？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># x.shape=[3]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>) <span class="comment"># x.shape=[4,6]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># x.shape=[4,2,3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="F-softmax"><a href="#F-softmax" class="headerlink" title="F.softmax"></a>F.softmax</h3><p><code>F.softmax(Tensor, dim=None)</code> 对于多维度矩阵就是 einsum(‘ijk -&gt; jk’, a) = torch.ones(a.shape[1:])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">a = F.softmax(a, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p>之前一直依赖着<code>huggingface</code>的模型加载<code>from_pretrained</code>，但其实在一般任务场景下，使用<code>torch.load</code>的时候会更多，所以记录一下<code>torch.load</code>方法的使用场景。</p>
<h3 id="torch-load-amp-torch-save"><a href="#torch-load-amp-torch-save" class="headerlink" title="torch.load &amp; torch.save"></a>torch.load &amp; torch.save</h3><p>一般我们将模型的参数保存，而不会去保存整个模型的结构。这里如果需要<strong>部分</strong>加载参数，可以使用<code>strict=False</code>。这里需要注意加载的是<strong>字典dict</strong>，不是模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model ... after training</span></span><br><span class="line">torch.save(model.state_dict(), cached_file_path)</span><br><span class="line">model_state = torch.load(cached_file_path)</span><br><span class="line">model.load_state_dict(model_state, strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>













<h2 id="奇淫技巧"><a href="#奇淫技巧" class="headerlink" title="奇淫技巧"></a>奇淫技巧</h2><h3 id="whole-word-mask"><a href="#whole-word-mask" class="headerlink" title="whole word mask"></a>whole word mask</h3><p>在bert或者其他语言模型中，对一段文本需要先进行tokenize分词操作，而分英文单词的时候，由于OOV问题，会将有些word分成token级别的，比如将<code>trying</code>分成<code>try</code>,<code>##ing</code>。而我们比如在建图或者以word为粒度的时候，就需要将token的输出平均给word了。那么如何操作呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoder_output = encoder_outputs[i]  <span class="comment"># [slen, bert_hidden_size]</span></span><br><span class="line">word_num = <span class="number">123</span></span><br><span class="line">word_index=(torch.arange(word_num) + <span class="number">1</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">words = pos_id[i].unsqueeze(<span class="number">0</span>).expand(mention_num, <span class="number">-1</span>)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = (mention_index == mentions).<span class="built_in">float</span>()  <span class="comment"># [mention_num, slen]</span></span><br><span class="line"><span class="comment"># average word -&gt; mention</span></span><br><span class="line">word_total_numbers = torch.<span class="built_in">sum</span>(select_metrix, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = torch.where(word_total_numbers &gt; <span class="number">0</span>, select_metrix / word_total_numbers, select_metrix)</span><br><span class="line">x = torch.mm(select_metrix, encoder_output)</span><br></pre></td></tr></table></figure>



</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-11-18T09:05:20.000Z" title="2020-11-18T09:05:20.000Z">2020-11-18</time>发表</span><span class="level-item"><time dateTime="2020-12-04T13:21:10.677Z" title="2020-12-04T13:21:10.677Z">2020-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E8%A7%A3/">论解</a></span><span class="level-item">9 分钟读完 (大约1379个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%AE%BA%E8%A7%A3/2020/11/18/ALBERT-%E6%9B%B4%E5%B0%8F%E4%BD%86%E6%98%AF%E6%9B%B4%E6%85%A2%EF%BC%9F.html">ALBERT 更小但是更慢？</a></h1><div class="content"><h1 id="ALBERT-更小但是更慢？"><a href="#ALBERT-更小但是更慢？" class="headerlink" title="ALBERT 更小但是更慢？"></a>ALBERT 更小但是更慢？</h1><p>最近由于参加阅读理解比赛，所以大量测试各种模型，惊奇地发现原本现在阅读理解比赛中SOTA的模型居然是不起眼并且以小模型闻名的<code>ALBERT</code>。这让我对这个“小”模型产生了好奇。从而写一下这份的论文笔记。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>模型越大下游效果越强是众所周知的道理，但是由于硬件设备和显存所限，所以模型不能无限制得放大。这篇文章提出了一个全面领先BERT模型的<code>ALBERT</code>，在比<code>BERT-LARGE</code>参数小的情况下超过了它。</p>
<h2 id="有何区别"><a href="#有何区别" class="headerlink" title="有何区别"></a>有何区别</h2><h3 id="1-embedding-参数减少"><a href="#1-embedding-参数减少" class="headerlink" title="1. embedding 参数减少"></a>1. embedding 参数减少</h3><p>在从one-hot embedding到hidden size embedding有一个$V \times H$的全连接层，这里使用了一个trick，加了一个hidden layer，从而使得全连接层变成了$V\times E + E\times H$。这样子我们就可以用一个很大的$H$了，比如在<code>xxlarge</code>上就是$H=4096$。</p>
<h3 id="2-层间参数共享"><a href="#2-层间参数共享" class="headerlink" title="2.层间参数共享"></a>2.层间参数共享</h3><p>很简单，就是原来模型类似于$F(x) = f_n(f_{n-1}(…f_1(x)))$，但是现在变成了$F(x)=f(f(…f(x)))$。我也在想，虽然$f(x)$是一个非线性的，但这种形式是不是可以有函数去拟合$F(x)$，毕竟重复$f(x)$这不能优化吗？ 去压缩<code>ALBERT</code>模型的大小。</p>
<h3 id="3-SOP"><a href="#3-SOP" class="headerlink" title="3. SOP"></a>3. SOP</h3><p>提出了一个新的self supervised learning 的 objective，既SOP(sentence ordering objectives)。类似于BERT预测两个句子是否是连续的，ALBERT需要预测打乱句子的顺序。</p>
<p><strong>并在在对比中，SOP对于RACE也就是阅读理解任务提高了2.3个点，很哇塞</strong></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分具体暂且不表。我理解的有几点</p>
<ol>
<li>额外的领域内预训练是有益的，但是领域外可能会有害</li>
<li>dropout在模型不会over-fit的情况下其实可以忽略，在batch normalization和dropout可能会损害模型的性能。</li>
<li>hidden size 4096 有可能是ALBERT 性能强的主要原因。</li>
<li>虽然层间参数共享，理论上可以无限深，但是实验发现24层并没有12层效果好。特别宽也没有特别好，这都是玄学调参，很难人工判断。</li>
<li>按理来说fffff(x) 可能会导致每层之间的输出过于相似，但在这里实验发现，并没有。难道是embed layer就很强了？ <strong>猜测</strong></li>
</ol>
<h2 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h2><ol>
<li>稀疏矩阵优化，attention 魔改</li>
<li>SOP是否可以泛用。</li>
</ol>
<hr>
<h2 id="模型解读"><a href="#模型解读" class="headerlink" title="模型解读"></a>模型解读</h2><h3 id="主类"><a href="#主类" class="headerlink" title="主类"></a>主类</h3><p>forward先经过<code>embeddings</code>层再经过<code>encoder</code>层。这里注意，默认输入是用了最后一个隐层所有token的输出再经过一个线性+<code>tanh</code>的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlbertModel</span>(<span class="params">AlbertPreTrainedModel</span>):</span></span><br><span class="line">    config_class = AlbertConfig</span><br><span class="line">    load_tf_weights = load_tf_weights_in_albert</span><br><span class="line">    base_model_prefix = <span class="string">&quot;albert&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.config = config</span><br><span class="line">        self.embeddings = AlbertEmbeddings(config)</span><br><span class="line">        self.encoder = AlbertTransformer(config)</span><br><span class="line">        <span class="keyword">if</span> add_pooling_layer:</span><br><span class="line">            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">            self.pooler_activation = nn.Tanh()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.pooler = <span class="literal">None</span></span><br><span class="line">            self.pooler_activation = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_resize_token_embeddings</span>(<span class="params">self, new_num_tokens</span>):</span></span><br><span class="line">        old_embeddings = self.embeddings.word_embeddings</span><br><span class="line">        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)</span><br><span class="line">        self.embeddings.word_embeddings = new_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prunes heads of the model.</span></span><br><span class="line"><span class="string">        heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125;</span></span><br><span class="line"><span class="string">        ALBERT has a different architecture in that its layers are shared across groups, which then has inner groups.</span></span><br><span class="line"><span class="string">        If an ALBERT model has 12 hidden layers and 2 hidden groups, with two inner groups, there</span></span><br><span class="line"><span class="string">        is a total of 4 different layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,</span></span><br><span class="line"><span class="string">        while [2,3] correspond to the two inner groups of the second hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Any layer with in index other than [0,1,2,3] will result in an error.</span></span><br><span class="line"><span class="string">        See base class PreTrainedModel for more information about head pruning</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            group_idx = <span class="built_in">int</span>(layer / self.config.inner_group_num)</span><br><span class="line">            inner_group_idx = <span class="built_in">int</span>(layer - group_idx * self.config.inner_group_num)</span><br><span class="line">            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING.format(&quot;batch_size, sequence_length&quot;))</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(</span></span><br><span class="line">        tokenizer_class=_TOKENIZER_FOR_DOC,</span><br><span class="line">        checkpoint=<span class="string">&quot;albert-base-v2&quot;</span>,</span><br><span class="line">        output_type=BaseModelOutputWithPooling,</span><br><span class="line">        config_class=_CONFIG_FOR_DOC,</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        extended_attention_mask = attention_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line">        extended_attention_mask = (<span class="number">1.0</span> - extended_attention_mask) * <span class="number">-10000.0</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, <span class="number">0</span>])) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPooling(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div></div><!--!--><div class="column column-right is-4-tablet is-3-desktop is-3-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/1.gif" alt="Chea Sim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chea Sim</p><p class="is-size-6 is-block">Student from Z</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Ning Bo</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">99</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">83</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/CheaSim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/CheaSim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/www.cheasim.com"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1600/"><span class="level-start"><span class="level-item">1600</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/acm/"><span class="level-start"><span class="level-item">acm</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile" href="/categories/acm%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">acm模板</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="level-start"><span class="level-item">ac自动机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cf/"><span class="level-start"><span class="level-item">cf</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/cf1500/"><span class="level-start"><span class="level-item">cf1500</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/dfs/"><span class="level-start"><span class="level-item">dfs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/dp/"><span class="level-start"><span class="level-item">dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hash/"><span class="level-start"><span class="level-item">hash</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/html/"><span class="level-start"><span class="level-item">html</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pat/"><span class="level-start"><span class="level-item">pat</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%B8%93%E9%A1%B9%E7%BB%83%E4%B9%A0/"><span class="level-start"><span class="level-item">专项练习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%BF%9D%E7%A0%94/"><span class="level-start"><span class="level-item">保研</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">前端</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">学习笔记</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">学习计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">开发</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%90%AC%E7%A0%96/"><span class="level-start"><span class="level-item">搬砖</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A6%82%E7%8E%87dp/"><span class="level-start"><span class="level-item">概率dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">模板</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="level-start"><span class="level-item">线段树</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">考研</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94%E8%B7%AF%E6%BC%AB%E6%BC%AB/"><span class="level-start"><span class="level-item">考研路漫漫</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="level-start"><span class="level-item">蓝桥杯</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E8%A7%A3/"><span class="level-start"><span class="level-item">论解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-20T07:26:03.000Z">2021-01-20</time></p><p class="title"><a href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-14T06:15:14.000Z">2021-01-14</time></p><p class="title"><a href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-12T13:55:19.000Z">2021-01-12</time></p><p class="title"><a href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-07T09:40:38.000Z">2021-01-07</time></p><p class="title"><a href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-11T09:07:57.000Z">2020-12-11</time></p><p class="title"><a href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LCA/"><span class="tag">LCA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QA/"><span class="tag">QA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/acm/"><span class="tag">acm</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="tag">ac自动机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ajax/"><span class="tag">ajax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bert/"><span class="tag">bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bilstm/"><span class="tag">bilstm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cf/"><span class="tag">cf</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/colab/"><span class="tag">colab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crf/"><span class="tag">crf</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dfs/"><span class="tag">dfs</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dgl/"><span class="tag">dgl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dp/"><span class="tag">dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dptree/"><span class="tag">dptree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/echarts/"><span class="tag">echarts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gnn/"><span class="tag">gnn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/greedy/"><span class="tag">greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hash/"><span class="tag">hash</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hdoj/"><span class="tag">hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/html/"><span class="tag">html</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/js/"><span class="tag">js</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lca/"><span class="tag">lca</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pat/"><span class="tag">pat</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/re/"><span class="tag">re</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/struts2/"><span class="tag">struts2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tokenizer/"><span class="tag">tokenizer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86/"><span class="tag">二分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E7%94%9F/"><span class="tag">人生</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%9D%E7%A0%94/"><span class="tag">保研</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AB%E6%95%B0%E7%A0%81/"><span class="tag">八数码</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2hash/"><span class="tag">字符串hash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="tag">学习计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"><span class="tag">小技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tag">开发</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%BD%E5%8F%96/"><span class="tag">抽取</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/"><span class="tag">最大子段和</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%84%E9%80%A0/"><span class="tag">构造</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E5%BD%A2dp/"><span class="tag">树形dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"><span class="tag">树状数组</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8Chdoj/"><span class="tag">树状数组，hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/"><span class="tag">树链剖分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87dp/"><span class="tag">概率dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%8B%9F/"><span class="tag">模拟</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%9D%BF/"><span class="tag">模板</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%86%E6%90%9C/"><span class="tag">爆搜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%9B%E5%AE%A2%E7%BD%91/"><span class="tag">牛客网</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%8A%B6%E5%8E%8Bdp/"><span class="tag">状压dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="tag">线段树</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"><span class="tag">组合数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%86%E8%8A%82/"><span class="tag">细节</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%A9%E7%82%B9/"><span class="tag">缩点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C/"><span class="tag">网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%B5%9B/"><span class="tag">网络赛</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8C%83%E5%9B%B4/"><span class="tag">范围</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="tag">蓝桥杯</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A1%A5%E9%A2%98/"><span class="tag">补题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%BE%8B/"><span class="tag">规律</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/"><span class="tag">计算几何</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E8%A7%A3/"><span class="tag">论解</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%AA%E5%BF%83/"><span class="tag">贪心</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">配置</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%8F%E6%80%A7/"><span class="tag">随性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/"><span class="tag">预处理</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 CheaSim</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>