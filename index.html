<!doctype html>
<html lang="zh"><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/fireworks.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>CheaSim Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="cheasim&#039;s blog"><meta name="msapplication-TileImage" content="/img/1.gif"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cheasim&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="ACM chicken"><meta property="og:type" content="blog"><meta property="og:title" content="CheaSim Blog"><meta property="og:url" content="https://www.cheasim.com/"><meta property="og:site_name" content="CheaSim Blog"><meta property="og:description" content="ACM chicken"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.cheasim.com/img/og_image.png"><meta property="article:author" content="CheaSim"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cheasim.com"},"headline":"CheaSim Blog","image":["https://www.cheasim.com/img/og_image.png"],"author":{"@type":"Person","name":"CheaSim"},"description":"ACM chicken"}</script><link rel="icon" href="/img/1.gif"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-R7VVCLV2ZB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-R7VVCLV2ZB');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-20T07:26:03.000Z" title="2021-01-20T07:26:03.000Z">2021-01-20</time>发表</span><span class="level-item"><time dateTime="2021-03-08T01:57:05.711Z" title="2021-03-08T01:57:05.711Z">2021-03-08</time>更新</span><span class="level-item">5 分钟读完 (大约820个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></h1><div class="content"><h1 id="UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-读书笔记"><a href="#UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-读书笔记" class="headerlink" title="UNIFIEDQA: Crossing Format Boundaries with a Single QA System 读书笔记"></a>UNIFIEDQA: Crossing Format Boundaries with a Single QA System 读书笔记</h1><p>针对不同形式的QA做了一个整合的工作。对于抽取式、多选式、对错形式的QA数据集，使用了一个单一的模型进行训练，并且测试。</p>
<p>使用的模型是T-5、BART。</p></div><a class="article-more button is-small is-size-7" href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-14T06:15:14.000Z" title="2021-01-14T06:15:14.000Z">2021-01-14</time>发表</span><span class="level-item"><time dateTime="2021-03-08T01:57:19.715Z" title="2021-03-08T01:57:19.715Z">2021-03-08</time>更新</span><span class="level-item">19 分钟读完 (大约2847个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></h1><div class="content"><h1 id="毕设复习"><a href="#毕设复习" class="headerlink" title="毕设复习"></a>毕设复习</h1><p>由于是基于知识图谱的领域内问答系统，所以分为两个步骤，不是end2end。</p>
<ol>
<li>命名实体识别</li>
<li>属性映射步骤</li>
</ol>
<p>实体识别是为了找到问题中的实体，属性映射是为了找到实体对应在知识库中的属性。输出的结果是一个规则构成的“「实体」的「属性」是「尾实体」”</p>
<p>命名实体识别是通过BERT+CRF。</p>
<p>属性映射分为两步</p>
<ol>
<li>通过规则，在知识库中找到实体的所有属性，之后和原句匹配，匹配成功作为属性输出</li>
<li>匹配不成果，将所有属性以“「问题」「属性」”计算分数，取匹配分数最高的作为答案输出。</li>
</ol></div><a class="article-more button is-small is-size-7" href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-12T13:55:19.000Z" title="2021-01-12T13:55:19.000Z">2021-01-12</time>发表</span><span class="level-item"><time dateTime="2021-03-08T01:56:12.917Z" title="2021-03-08T01:56:12.917Z">2021-03-08</time>更新</span><span class="level-item">30 分钟读完 (大约4481个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></h1><div class="content"><h1 id="nlp-深度学习基础"><a href="#nlp-深度学习基础" class="headerlink" title="nlp 深度学习基础"></a>nlp 深度学习基础</h1><p>将每一个模型以</p>
<ol>
<li>简单介绍</li>
<li>解决的问题</li>
<li>代码</li>
<li>优缺点</li>
<li>使用tips</li>
</ol>
<p>来归类。比较现代的会分析分析。</p></div><a class="article-more button is-small is-size-7" href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-07T09:40:38.000Z" title="2021-01-07T09:40:38.000Z">2021-01-07</time>发表</span><span class="level-item"><time dateTime="2021-03-08T01:57:27.419Z" title="2021-03-08T01:57:27.419Z">2021-03-08</time>更新</span><span class="level-item">1 分钟读完 (大约169个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></h1><div class="content"><h1 id="如何在colab用pytorch-lightning白嫖TPU"><a href="#如何在colab用pytorch-lightning白嫖TPU" class="headerlink" title="如何在colab用pytorch_lightning白嫖TPU"></a>如何在colab用pytorch_lightning白嫖TPU</h1><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>我们得熟悉<code>pytorch_lightning</code>以及<code>colab</code>的操作。在使用TPU之前，先挂载好我们的云端磁盘以及配置好pytorch需要的TPU环境。</p></div><a class="article-more button is-small is-size-7" href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-11T09:07:57.000Z" title="2020-12-11T09:07:57.000Z">2020-12-11</time>发表</span><span class="level-item"><time dateTime="2020-12-11T13:40:29.706Z" title="2020-12-11T13:40:29.706Z">2020-12-11</time>更新</span><span class="level-item">2 分钟读完 (大约237个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></h1><div class="content"><h1 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h1><blockquote>
<p>Transformers==4.0.0</p>
<p>由于每次调用bert等模型都需要使用模型的tokenizer，所以写个笔记，方便自己以及后人查阅，（其实看官方文档也可以，但是英文的看着头疼。有错误也请留言吧。</p>
</blockquote>
<h2 id="base-PreTrainedTokenizerBase"><a href="#base-PreTrainedTokenizerBase" class="headerlink" title="base : PreTrainedTokenizerBase"></a>base : <code>PreTrainedTokenizerBase</code></h2><p>作为基类，该类有着所有tokenizer都具有的方法还有属性。</p>
<h3 id="PreTrainedTokenizer"><a href="#PreTrainedTokenizer" class="headerlink" title="PreTrainedTokenizer"></a>PreTrainedTokenizer</h3><h2 id="使用multiprocess-加速tokenize"><a href="#使用multiprocess-加速tokenize" class="headerlink" title="使用multiprocess 加速tokenize"></a>使用multiprocess 加速tokenize</h2><p>仿照这个就完事了。<code>partial</code>可以固定函数中的参数，简直是专门为多进程准备的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squad_convert_example_to_features_init</span>(<span class="params">tokenizer_for_convert: PreTrainedTokenizerBase</span>):</span></span><br><span class="line">    <span class="keyword">global</span> tokenizer</span><br><span class="line">    tokenizer = tokenizer_for_convert</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = []</span><br><span class="line"></span><br><span class="line">threads = <span class="built_in">min</span>(threads, cpu_count())</span><br><span class="line"><span class="keyword">with</span> Pool(threads, initializer=squad_convert_example_to_features_init, initargs=				(tokenizer,)) <span class="keyword">as</span> p:</span><br><span class="line">  annotate_ = partial(</span><br><span class="line">    tokenzier.encode_plus, <span class="comment"># batch_encode_plus 不知道为什么不work</span></span><br><span class="line">    max_seq_length=max_seq_length,</span><br><span class="line">    doc_stride=doc_stride,</span><br><span class="line">    max_query_length=max_query_length,</span><br><span class="line">    padding_strategy=padding_strategy,</span><br><span class="line">    is_training=is_training,</span><br><span class="line">  )</span><br><span class="line">  features = <span class="built_in">list</span>(</span><br><span class="line">    tqdm(</span><br><span class="line">      p.imap(annotate_, examples, chunksize=<span class="number">32</span>),</span><br><span class="line">      total=<span class="built_in">len</span>(examples),</span><br><span class="line">      desc=<span class="string">&quot;convert squad examples to features&quot;</span>,</span><br><span class="line">      disable=<span class="keyword">not</span> tqdm_enabled,</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-07T11:33:57.000Z" title="2020-12-07T11:33:57.000Z">2020-12-07</time>发表</span><span class="level-item"><time dateTime="2021-01-06T07:45:59.695Z" title="2021-01-06T07:45:59.695Z">2021-01-06</time>更新</span><span class="level-item">4 分钟读完 (大约637个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/07/pytorch-cheat-list.html">pytorch cheat_list</a></h1><div class="content"><h1 id="pytorch-操作小计"><a href="#pytorch-操作小计" class="headerlink" title="pytorch 操作小计"></a>pytorch 操作小计</h1><blockquote>
<p>torch==1.7.0</p>
</blockquote>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><h3 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h3><p>将List[tensor]变成tensor。<code>torch.stack(tensors,dim=0,out=None)</code>Concatenates a sequence of tensors along a new dimension.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">a = torch.stack([b, b], dim = <span class="number">0</span>) <span class="comment"># a.shape = (2,4)</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-gather"><a href="#torch-gather" class="headerlink" title="torch.gather"></a>torch.gather</h3><blockquote>
<p>torch.gather(<em>input</em>, <em>dim</em>, <em>index</em>, *, <em>sparse_grad=False</em>, <em>out=None</em>) → Tensor</p>
</blockquote>
<p>英文解释为Gathers values along an axis specified by dim.</p>
<p>看到比较有道理的应用场景是，在变长序列中gather到最后一个或者说倒数第几个元素。一般变长序列为<code>inputs = [[1,2,3,0,0], [2,3,4,5,0]]</code>。这时候想获得最后一个元素就可以。需要注意的点是，输出的tensor和index是相同<code>shape</code>的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">         	[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]])</span><br><span class="line">index = torch.tensor([[<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.long)</span><br><span class="line">last_inputs = torch.gather(inputs, dim=<span class="number">1</span>, index)</span><br><span class="line"><span class="string">&quot;&quot;&quot;tensor([[3],</span></span><br><span class="line"><span class="string">           [5]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="torch-expand"><a href="#torch-expand" class="headerlink" title="torch.expand"></a>torch.expand</h3><p>将tensor扩展维度，自动复制，十分好用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">5</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#tensor([[3, 1, 1],</span></span><br><span class="line"><span class="comment">#        [1, 2, 4]])</span></span><br><span class="line">a = a.unsqueeze(<span class="number">2</span>).expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[3, 3, 3],</span></span><br><span class="line"><span class="string">         [1, 1, 1],</span></span><br><span class="line"><span class="string">         [1, 1, 1]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[1, 1, 1],</span></span><br><span class="line"><span class="string">         [2, 2, 2],</span></span><br><span class="line"><span class="string">         [4, 4, 4]]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat"></a>torch.repeat</h3><p><code>repeat(*sizes) -&gt; Tensor</code>， 重复复制tensor在指定的维度上。其实有点类似于广播操作了？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># x.shape=[3]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>) <span class="comment"># x.shape=[4,6]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># x.shape=[4,2,3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="F-softmax"><a href="#F-softmax" class="headerlink" title="F.softmax"></a>F.softmax</h3><p><code>F.softmax(Tensor, dim=None)</code> 对于多维度矩阵就是 einsum(‘ijk -&gt; jk’, a) = torch.ones(a.shape[1:])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">a = F.softmax(a, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p>之前一直依赖着<code>huggingface</code>的模型加载<code>from_pretrained</code>，但其实在一般任务场景下，使用<code>torch.load</code>的时候会更多，所以记录一下<code>torch.load</code>方法的使用场景。</p>
<h3 id="torch-load-amp-torch-save"><a href="#torch-load-amp-torch-save" class="headerlink" title="torch.load &amp; torch.save"></a>torch.load &amp; torch.save</h3><p>一般我们将模型的参数保存，而不会去保存整个模型的结构。这里如果需要<strong>部分</strong>加载参数，可以使用<code>strict=False</code>。这里需要注意加载的是<strong>字典dict</strong>，不是模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model ... after training</span></span><br><span class="line">torch.save(model.state_dict(), cached_file_path)</span><br><span class="line">model_state = torch.load(cached_file_path)</span><br><span class="line">model.load_state_dict(model_state, strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>













<h2 id="奇淫技巧"><a href="#奇淫技巧" class="headerlink" title="奇淫技巧"></a>奇淫技巧</h2><h3 id="whole-word-mask"><a href="#whole-word-mask" class="headerlink" title="whole word mask"></a>whole word mask</h3><p>在bert或者其他语言模型中，对一段文本需要先进行tokenize分词操作，而分英文单词的时候，由于OOV问题，会将有些word分成token级别的，比如将<code>trying</code>分成<code>try</code>,<code>##ing</code>。而我们比如在建图或者以word为粒度的时候，就需要将token的输出平均给word了。那么如何操作呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoder_output = encoder_outputs[i]  <span class="comment"># [slen, bert_hidden_size]</span></span><br><span class="line">word_num = <span class="number">123</span></span><br><span class="line">word_index=(torch.arange(word_num) + <span class="number">1</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">words = pos_id[i].unsqueeze(<span class="number">0</span>).expand(mention_num, <span class="number">-1</span>)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = (mention_index == mentions).<span class="built_in">float</span>()  <span class="comment"># [mention_num, slen]</span></span><br><span class="line"><span class="comment"># average word -&gt; mention</span></span><br><span class="line">word_total_numbers = torch.<span class="built_in">sum</span>(select_metrix, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = torch.where(word_total_numbers &gt; <span class="number">0</span>, select_metrix / word_total_numbers, select_metrix)</span><br><span class="line">x = torch.mm(select_metrix, encoder_output)</span><br></pre></td></tr></table></figure>



</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-04T12:42:54.000Z" title="2020-12-04T12:42:54.000Z">2020-12-04</time>发表</span><span class="level-item"><time dateTime="2020-12-04T12:44:25.242Z" title="2020-12-04T12:44:25.242Z">2020-12-04</time>更新</span><span class="level-item">几秒读完 (大约22个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/04/dgl-%E5%AE%9E%E4%BE%8B%E5%AD%A6%E4%B9%A0.html">dgl 实例学习</a></h1><div class="content"><h1 id="学习开源项目"><a href="#学习开源项目" class="headerlink" title="学习开源项目"></a>学习开源项目</h1><p><a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">项目传送门</a></p>
<h2 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-03T13:15:59.000Z" title="2020-12-03T13:15:59.000Z">2020-12-03</time>发表</span><span class="level-item"><time dateTime="2020-12-11T07:58:48.893Z" title="2020-12-11T07:58:48.893Z">2020-12-11</time>更新</span><span class="level-item">1 分钟读完 (大约154个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/03/Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读</a></h1><div class="content"><h1 id="Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-论文解读"><a href="#Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-论文解读" class="headerlink" title="Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读"></a>Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.05092.pdf">论文传送门</a>，<a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">代码传送门</a></p>
<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>从对话数据集中抽取出实体之间的联系。给定一段对话，输出指定两个实体之间的关系，总共有<code>37</code>中不同的关系。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>针对对话中不同的信息进行embedding，</p>
<ol>
<li><code>utterance</code> ，对话中的<code>话</code>使用<code>LSTM</code>模型进行encode。</li>
<li><code>speaker</code> </li>
<li><code>argument</code></li>
<li><code>entity-type</code></li>
<li><code>word</code></li>
</ol>
<p>由着四种<code>embedding</code>来构建图。之后拼接学到的<code>argument embedding</code>通过分类器输出关系。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-11-18T09:05:20.000Z" title="2020-11-18T09:05:20.000Z">2020-11-18</time>发表</span><span class="level-item"><time dateTime="2020-12-04T13:21:10.677Z" title="2020-12-04T13:21:10.677Z">2020-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E8%A7%A3/">论解</a></span><span class="level-item">9 分钟读完 (大约1379个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%AE%BA%E8%A7%A3/2020/11/18/ALBERT-%E6%9B%B4%E5%B0%8F%E4%BD%86%E6%98%AF%E6%9B%B4%E6%85%A2%EF%BC%9F.html">ALBERT 更小但是更慢？</a></h1><div class="content"><h1 id="ALBERT-更小但是更慢？"><a href="#ALBERT-更小但是更慢？" class="headerlink" title="ALBERT 更小但是更慢？"></a>ALBERT 更小但是更慢？</h1><p>最近由于参加阅读理解比赛，所以大量测试各种模型，惊奇地发现原本现在阅读理解比赛中SOTA的模型居然是不起眼并且以小模型闻名的<code>ALBERT</code>。这让我对这个“小”模型产生了好奇。从而写一下这份的论文笔记。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>模型越大下游效果越强是众所周知的道理，但是由于硬件设备和显存所限，所以模型不能无限制得放大。这篇文章提出了一个全面领先BERT模型的<code>ALBERT</code>，在比<code>BERT-LARGE</code>参数小的情况下超过了它。</p>
<h2 id="有何区别"><a href="#有何区别" class="headerlink" title="有何区别"></a>有何区别</h2><h3 id="1-embedding-参数减少"><a href="#1-embedding-参数减少" class="headerlink" title="1. embedding 参数减少"></a>1. embedding 参数减少</h3><p>在从one-hot embedding到hidden size embedding有一个$V \times H$的全连接层，这里使用了一个trick，加了一个hidden layer，从而使得全连接层变成了$V\times E + E\times H$。这样子我们就可以用一个很大的$H$了，比如在<code>xxlarge</code>上就是$H=4096$。</p>
<h3 id="2-层间参数共享"><a href="#2-层间参数共享" class="headerlink" title="2.层间参数共享"></a>2.层间参数共享</h3><p>很简单，就是原来模型类似于$F(x) = f_n(f_{n-1}(…f_1(x)))$，但是现在变成了$F(x)=f(f(…f(x)))$。我也在想，虽然$f(x)$是一个非线性的，但这种形式是不是可以有函数去拟合$F(x)$，毕竟重复$f(x)$这不能优化吗？ 去压缩<code>ALBERT</code>模型的大小。</p>
<h3 id="3-SOP"><a href="#3-SOP" class="headerlink" title="3. SOP"></a>3. SOP</h3><p>提出了一个新的self supervised learning 的 objective，既SOP(sentence ordering objectives)。类似于BERT预测两个句子是否是连续的，ALBERT需要预测打乱句子的顺序。</p>
<p><strong>并在在对比中，SOP对于RACE也就是阅读理解任务提高了2.3个点，很哇塞</strong></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分具体暂且不表。我理解的有几点</p>
<ol>
<li>额外的领域内预训练是有益的，但是领域外可能会有害</li>
<li>dropout在模型不会over-fit的情况下其实可以忽略，在batch normalization和dropout可能会损害模型的性能。</li>
<li>hidden size 4096 有可能是ALBERT 性能强的主要原因。</li>
<li>虽然层间参数共享，理论上可以无限深，但是实验发现24层并没有12层效果好。特别宽也没有特别好，这都是玄学调参，很难人工判断。</li>
<li>按理来说fffff(x) 可能会导致每层之间的输出过于相似，但在这里实验发现，并没有。难道是embed layer就很强了？ <strong>猜测</strong></li>
</ol>
<h2 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h2><ol>
<li>稀疏矩阵优化，attention 魔改</li>
<li>SOP是否可以泛用。</li>
</ol>
<hr>
<h2 id="模型解读"><a href="#模型解读" class="headerlink" title="模型解读"></a>模型解读</h2><h3 id="主类"><a href="#主类" class="headerlink" title="主类"></a>主类</h3><p>forward先经过<code>embeddings</code>层再经过<code>encoder</code>层。这里注意，默认输入是用了最后一个隐层所有token的输出再经过一个线性+<code>tanh</code>的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlbertModel</span>(<span class="params">AlbertPreTrainedModel</span>):</span></span><br><span class="line">    config_class = AlbertConfig</span><br><span class="line">    load_tf_weights = load_tf_weights_in_albert</span><br><span class="line">    base_model_prefix = <span class="string">&quot;albert&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.config = config</span><br><span class="line">        self.embeddings = AlbertEmbeddings(config)</span><br><span class="line">        self.encoder = AlbertTransformer(config)</span><br><span class="line">        <span class="keyword">if</span> add_pooling_layer:</span><br><span class="line">            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">            self.pooler_activation = nn.Tanh()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.pooler = <span class="literal">None</span></span><br><span class="line">            self.pooler_activation = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_resize_token_embeddings</span>(<span class="params">self, new_num_tokens</span>):</span></span><br><span class="line">        old_embeddings = self.embeddings.word_embeddings</span><br><span class="line">        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)</span><br><span class="line">        self.embeddings.word_embeddings = new_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prunes heads of the model.</span></span><br><span class="line"><span class="string">        heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125;</span></span><br><span class="line"><span class="string">        ALBERT has a different architecture in that its layers are shared across groups, which then has inner groups.</span></span><br><span class="line"><span class="string">        If an ALBERT model has 12 hidden layers and 2 hidden groups, with two inner groups, there</span></span><br><span class="line"><span class="string">        is a total of 4 different layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,</span></span><br><span class="line"><span class="string">        while [2,3] correspond to the two inner groups of the second hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Any layer with in index other than [0,1,2,3] will result in an error.</span></span><br><span class="line"><span class="string">        See base class PreTrainedModel for more information about head pruning</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            group_idx = <span class="built_in">int</span>(layer / self.config.inner_group_num)</span><br><span class="line">            inner_group_idx = <span class="built_in">int</span>(layer - group_idx * self.config.inner_group_num)</span><br><span class="line">            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING.format(&quot;batch_size, sequence_length&quot;))</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(</span></span><br><span class="line">        tokenizer_class=_TOKENIZER_FOR_DOC,</span><br><span class="line">        checkpoint=<span class="string">&quot;albert-base-v2&quot;</span>,</span><br><span class="line">        output_type=BaseModelOutputWithPooling,</span><br><span class="line">        config_class=_CONFIG_FOR_DOC,</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        extended_attention_mask = attention_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line">        extended_attention_mask = (<span class="number">1.0</span> - extended_attention_mask) * <span class="number">-10000.0</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, <span class="number">0</span>])) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPooling(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-12-25T06:17:24.000Z" title="2019-12-25T06:17:24.000Z">2019-12-25</time>发表</span><span class="level-item"><time dateTime="2019-12-25T06:57:28.797Z" title="2019-12-25T06:57:28.797Z">2019-12-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%80%83%E7%A0%94/">考研</a></span><span class="level-item">9 分钟读完 (大约1333个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%80%83%E7%A0%94/2019/12/25/%E8%80%83%E7%A0%94%E9%9A%8F%E6%83%B3.html">考研随想</a></h1><div class="content"><h1 id="考研感想"><a href="#考研感想" class="headerlink" title="考研感想"></a>考研感想</h1><h2 id="感触"><a href="#感触" class="headerlink" title="感触"></a>感触</h2><p>现在是12月25号，考完研的第三天。在今天之前的前一个星期里，我每天都是7点左右自然醒。而终于在今天，我睡到了九点，但依然没有睡饱。我想考研带给我了很多，不只是那摞起来像山一样高的辅导书，还有一些随之带来的习惯的改变。</p>
<h2 id="考研开始"><a href="#考研开始" class="headerlink" title="考研开始"></a>考研开始</h2><h3 id="高考之后"><a href="#高考之后" class="headerlink" title="高考之后"></a>高考之后</h3><p>最开始听说考研应该是高考出成绩之后吧。那个时候因为考的太差，是真的对自己的人生失去了信心。之后听说到了大学以后，还可以靠着考研”翻身“（哪有什么翻身）。之后就下定决心，在大学期间冲冲冲，以考研考上浙大为目标。</p>
<h3 id="得知专业之后"><a href="#得知专业之后" class="headerlink" title="得知专业之后"></a>得知专业之后</h3><p>高考分数出来之后，还要进行选学校和选专业，但是因为分是在是太低了。只能随便选选了。最后来到njtech的这个环境科学专业。本着干一行爱一行的心态，我在网上搜索了很多关于环境科学的信息。但是越搜索我的心是越来越拔凉拔凉的。作为环化生材四大坑之首，环境绝对算得上是以先辈们的血一般地事实展示了环境专业的不受待见。之后，在知乎的熏染之下，我选择了转专业！</p>
<h3 id="大一"><a href="#大一" class="headerlink" title="大一"></a>大一</h3><p>大一的过程可以总结为：守望先锋+刷绩点。除此之外，啥活动都没有参加。</p>
<h3 id="转专业以后"><a href="#转专业以后" class="headerlink" title="转专业以后"></a>转专业以后</h3><p>终于在大一下的时候，以环境科学专业第一的成绩转入了计算机科学与技术学院。并且加入了在大一的时候就听说过的acm比赛。但是投身于acm比赛的过程中，我也忘记了我进入大学的初始动机—-考研浙大。</p>
<h3 id="acm区域赛3铜"><a href="#acm区域赛3铜" class="headerlink" title="acm区域赛3铜"></a>acm区域赛3铜</h3><p>可能因为暑假的半学半玩，也可能是因为自己的水品不行智商不够。我们队在2018年的下半年，去了三场区域赛（包括一场EC-Final）全部都拿了铜奖。还记得第一个铜的时候，心里是低落的。但是在听说有学长保研南大，有数理学院的学长保研的浙大计算机直博，并且在网上看了一下，保研中打acm的还算是少数，我保研的心情又高涨了起来。</p>
<h3 id="2019年4月份"><a href="#2019年4月份" class="headerlink" title="2019年4月份"></a>2019年4月份</h3><p>4，5月份是acm邀请赛开始的时候，但也是保研申请学校夏令营的高峰期，这时候不管是哪里的学生，都在疯狂地给老师发邮件，咨询保研的事项。但是我由于acm还要打个邀请赛，精力确实也分了一些。结果就上了个上科大和浙软的夏令营。（其实还是因为acm牌子不够响，也没有啥子科研经历）</p>
<h3 id="暑假"><a href="#暑假" class="headerlink" title="暑假"></a>暑假</h3><p>暑假的时候，去了浙软的夏令营，并且考研数学过了高数，英语单词也背了不少。结果夏令营当场没给我优秀营员，但是在后面的优秀营员名单中是有我的。不过我们学院保研要求的是对方学校的确保接受证明，这tm谁有啊。只能灰溜溜地准备去考研了。</p>
<h3 id="8月"><a href="#8月" class="headerlink" title="8月"></a>8月</h3><p>此时正是考研的冲刺期，但是我考浙大计科，怕是去送命了，（浙大计科408考4门专业课，就算是再扎实的基础，也要个长时间的准备，而且我的数学一直属于刷个绩点的水品，并不能应付考研这个高强度考试。）所以根据我看到的浙软合作企业的项目，我决定tm考浙软，软件就软件，起码也是个浙大。</p>
<h3 id="8-12月"><a href="#8-12月" class="headerlink" title="8-12月"></a>8-12月</h3><p>冲冲冲！疯狂复习，虽然有时候也松懈个1天，但是这四个月我的考研复习效率还是可以的。养成了一些学习的习惯，比如写感想，摘录错题。</p>
<h2 id="考研中"><a href="#考研中" class="headerlink" title="考研中"></a>考研中</h2><p>转眼之间就到了考研的时间了，12月21日-22日。</p>
<h3 id="酒店中凌乱"><a href="#酒店中凌乱" class="headerlink" title="酒店中凌乱"></a>酒店中凌乱</h3><p>虽然我的考点离学校蛮近的，但是我还是选择了订了个酒店，既可以中午的时候躺一会儿，也可以防止意外。所以我在周四的晚上，还去试睡了一下。结果就tm出现意外了。</p>
<blockquote>
<p>论酒店隔音不好+旁边是一对激情似火的情侣的住房体验</p>
</blockquote>
<p>所以我毅然决然地选择周五这个最重要的睡觉时间在宿舍里睡。</p>
<h3 id="周五"><a href="#周五" class="headerlink" title="周五"></a>周五</h3><p>宿舍中辗转难眠，因为周六考的是政治和英语，所以有大量的需要背诵的内容。我选择了在10.30睡觉，之后早上6.20左右爬起来再过一遍。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/10/">10</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-3-desktop is-3-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/1.gif" alt="Chea Sim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chea Sim</p><p class="is-size-6 is-block">Student from Z</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Ning Bo</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">99</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">83</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/CheaSim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/CheaSim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/www.cheasim.com"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1600/"><span class="level-start"><span class="level-item">1600</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/acm/"><span class="level-start"><span class="level-item">acm</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile" href="/categories/acm%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">acm模板</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="level-start"><span class="level-item">ac自动机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cf/"><span class="level-start"><span class="level-item">cf</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/cf1500/"><span class="level-start"><span class="level-item">cf1500</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/dfs/"><span class="level-start"><span class="level-item">dfs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/dp/"><span class="level-start"><span class="level-item">dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hash/"><span class="level-start"><span class="level-item">hash</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/html/"><span class="level-start"><span class="level-item">html</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pat/"><span class="level-start"><span class="level-item">pat</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%B8%93%E9%A1%B9%E7%BB%83%E4%B9%A0/"><span class="level-start"><span class="level-item">专项练习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%BF%9D%E7%A0%94/"><span class="level-start"><span class="level-item">保研</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">前端</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">学习笔记</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">学习计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">开发</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%90%AC%E7%A0%96/"><span class="level-start"><span class="level-item">搬砖</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A6%82%E7%8E%87dp/"><span class="level-start"><span class="level-item">概率dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">模板</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="level-start"><span class="level-item">线段树</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">考研</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94%E8%B7%AF%E6%BC%AB%E6%BC%AB/"><span class="level-start"><span class="level-item">考研路漫漫</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="level-start"><span class="level-item">蓝桥杯</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E8%A7%A3/"><span class="level-start"><span class="level-item">论解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-20T07:26:03.000Z">2021-01-20</time></p><p class="title"><a href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-14T06:15:14.000Z">2021-01-14</time></p><p class="title"><a href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-12T13:55:19.000Z">2021-01-12</time></p><p class="title"><a href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-07T09:40:38.000Z">2021-01-07</time></p><p class="title"><a href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-11T09:07:57.000Z">2020-12-11</time></p><p class="title"><a href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LCA/"><span class="tag">LCA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QA/"><span class="tag">QA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/acm/"><span class="tag">acm</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="tag">ac自动机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ajax/"><span class="tag">ajax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bert/"><span class="tag">bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bilstm/"><span class="tag">bilstm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cf/"><span class="tag">cf</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/colab/"><span class="tag">colab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crf/"><span class="tag">crf</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dfs/"><span class="tag">dfs</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dgl/"><span class="tag">dgl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dp/"><span class="tag">dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dptree/"><span class="tag">dptree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/echarts/"><span class="tag">echarts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gnn/"><span class="tag">gnn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/greedy/"><span class="tag">greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hash/"><span class="tag">hash</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hdoj/"><span class="tag">hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/html/"><span class="tag">html</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/js/"><span class="tag">js</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lca/"><span class="tag">lca</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pat/"><span class="tag">pat</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/re/"><span class="tag">re</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/struts2/"><span class="tag">struts2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tokenizer/"><span class="tag">tokenizer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86/"><span class="tag">二分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E7%94%9F/"><span class="tag">人生</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%9D%E7%A0%94/"><span class="tag">保研</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AB%E6%95%B0%E7%A0%81/"><span class="tag">八数码</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2hash/"><span class="tag">字符串hash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="tag">学习计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"><span class="tag">小技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tag">开发</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%BD%E5%8F%96/"><span class="tag">抽取</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/"><span class="tag">最大子段和</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%84%E9%80%A0/"><span class="tag">构造</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E5%BD%A2dp/"><span class="tag">树形dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"><span class="tag">树状数组</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8Chdoj/"><span class="tag">树状数组，hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/"><span class="tag">树链剖分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87dp/"><span class="tag">概率dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%8B%9F/"><span class="tag">模拟</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%9D%BF/"><span class="tag">模板</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%86%E6%90%9C/"><span class="tag">爆搜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%9B%E5%AE%A2%E7%BD%91/"><span class="tag">牛客网</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%8A%B6%E5%8E%8Bdp/"><span class="tag">状压dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="tag">线段树</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"><span class="tag">组合数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%86%E8%8A%82/"><span class="tag">细节</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%A9%E7%82%B9/"><span class="tag">缩点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C/"><span class="tag">网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%B5%9B/"><span class="tag">网络赛</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8C%83%E5%9B%B4/"><span class="tag">范围</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="tag">蓝桥杯</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A1%A5%E9%A2%98/"><span class="tag">补题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%BE%8B/"><span class="tag">规律</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/"><span class="tag">计算几何</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E8%A7%A3/"><span class="tag">论解</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%AA%E5%BF%83/"><span class="tag">贪心</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">配置</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%8F%E6%80%A7/"><span class="tag">随性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/"><span class="tag">预处理</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 CheaSim</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>