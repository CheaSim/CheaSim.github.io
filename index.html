<!doctype html>
<html lang="zh"><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/fireworks.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>CheaSim Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="cheasim&#039;s blog"><meta name="msapplication-TileImage" content="/img/1.gif"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cheasim&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="ACM chicken"><meta property="og:type" content="blog"><meta property="og:title" content="CheaSim Blog"><meta property="og:url" content="https://www.cheasim.com/"><meta property="og:site_name" content="CheaSim Blog"><meta property="og:description" content="ACM chicken"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.cheasim.com/img/og_image.png"><meta property="article:author" content="CheaSim"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cheasim.com"},"headline":"CheaSim Blog","image":["https://www.cheasim.com/img/og_image.png"],"author":{"@type":"Person","name":"CheaSim"},"description":"ACM chicken"}</script><link rel="icon" href="/img/1.gif"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-R7VVCLV2ZB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-R7VVCLV2ZB');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="æœç´¢" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-14T06:15:14.000Z" title="2021-01-14T06:15:14.000Z">2021-01-14</time>å‘è¡¨</span><span class="level-item"><time dateTime="2021-01-15T07:00:44.187Z" title="2021-01-15T07:00:44.187Z">2021-01-15</time>æ›´æ–°</span><span class="level-item">19 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦2846ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">åŸºäºBERTçš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ</a></h1><div class="content"><h1 id="æ¯•è®¾å¤ä¹ "><a href="#æ¯•è®¾å¤ä¹ " class="headerlink" title="æ¯•è®¾å¤ä¹ "></a>æ¯•è®¾å¤ä¹ </h1><p>ç”±äºæ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„é¢†åŸŸå†…é—®ç­”ç³»ç»Ÿï¼Œæ‰€ä»¥åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œä¸æ˜¯end2endã€‚</p>
<ol>
<li>å‘½åå®ä½“è¯†åˆ«</li>
<li>å±æ€§æ˜ å°„æ­¥éª¤</li>
</ol>
<p>å®ä½“è¯†åˆ«æ˜¯ä¸ºäº†æ‰¾åˆ°é—®é¢˜ä¸­çš„å®ä½“ï¼Œå±æ€§æ˜ å°„æ˜¯ä¸ºäº†æ‰¾åˆ°å®ä½“å¯¹åº”åœ¨çŸ¥è¯†åº“ä¸­çš„å±æ€§ã€‚è¾“å‡ºçš„ç»“æœæ˜¯ä¸€ä¸ªè§„åˆ™æ„æˆçš„â€œã€Œå®ä½“ã€çš„ã€Œå±æ€§ã€æ˜¯ã€Œå°¾å®ä½“ã€â€</p>
<p>å‘½åå®ä½“è¯†åˆ«æ˜¯é€šè¿‡BERT+CRFã€‚</p>
<p>å±æ€§æ˜ å°„åˆ†ä¸ºä¸¤æ­¥</p>
<ol>
<li>é€šè¿‡è§„åˆ™ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°å®ä½“çš„æ‰€æœ‰å±æ€§ï¼Œä¹‹åå’ŒåŸå¥åŒ¹é…ï¼ŒåŒ¹é…æˆåŠŸä½œä¸ºå±æ€§è¾“å‡º</li>
<li>åŒ¹é…ä¸æˆæœï¼Œå°†æ‰€æœ‰å±æ€§ä»¥â€œã€Œé—®é¢˜ã€ã€Œå±æ€§ã€â€è®¡ç®—åˆ†æ•°ï¼Œå–åŒ¹é…åˆ†æ•°æœ€é«˜çš„ä½œä¸ºç­”æ¡ˆè¾“å‡ºã€‚</li>
</ol>
<p>1.15 æå®š å®ä½“å±æ€§å¯¹é½æ–¹é¢çŸ¥è¯†ç‚¹ã€‚ ä¹‹åå†æå®šé¢è¯•RNN + é¢è¯•5é“é¢˜ + leetcode10é“é¢˜ã€‚ é“¾è¡¨ å­—ç¬¦ä¸²ç€é‡ã€‚</p>
<h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul>
<li>å®ä½“é“¾æ¥æ²¡æœ‰å®ç°ï¼Œåªæ˜¯åŒ¹é…+æ•°æ®åº“æŸ¥è¯¢ã€‚</li>
</ul>
<h2 id="æ•°æ®é¢„å¤„ç†"><a href="#æ•°æ®é¢„å¤„ç†" class="headerlink" title="æ•°æ®é¢„å¤„ç†"></a>æ•°æ®é¢„å¤„ç†</h2><p>ç”±äºæ•°æ®æœ¬èº«æ˜¯ä¸€ä¸ªé—®ç­”æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆå¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆä¸‰å…ƒç»„å¯¹ã€‚å¤§æ¦‚æœ‰600wä¸ªå®ä½“ï¼Œè®­ç»ƒçš„ä¸€ä¸ª1pochéœ€è¦5å°æ—¶ã€‚é—®é¢˜æ•°é‡25000ã€‚</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;question id=1&gt;	ã€Šæœºæ¢°è®¾è®¡åŸºç¡€ã€‹è¿™æœ¬ä¹¦çš„ä½œè€…æ˜¯è°ï¼Ÿ</span><br><span class="line">&lt;triple id=1&gt;	æœºæ¢°è®¾è®¡åŸºç¡€ ||| ä½œè€… ||| æ¨å¯æ¡¢ï¼Œç¨‹å…‰è•´ï¼Œæä»²ç”Ÿ</span><br><span class="line">&lt;answer id=1&gt;	æ¨å¯æ¡¢ï¼Œç¨‹å…‰è•´ï¼Œæä»²ç”Ÿ</span><br><span class="line">==================================================</span><br><span class="line">&lt;question id=2&gt;	ã€Šé«˜ç­‰æ•°å­¦ã€‹æ˜¯å“ªä¸ªå‡ºç‰ˆç¤¾å‡ºç‰ˆçš„ï¼Ÿ</span><br><span class="line">&lt;triple id=2&gt;	é«˜ç­‰æ•°å­¦ ||| å‡ºç‰ˆç¤¾ ||| æ­¦æ±‰å¤§å­¦å‡ºç‰ˆç¤¾</span><br><span class="line">&lt;answer id=2&gt;	æ­¦æ±‰å¤§å­¦å‡ºç‰ˆç¤¾</span><br><span class="line">==================================================</span><br><span class="line">&lt;question id=3&gt;	ã€Šçº¿æ€§ä»£æ•°ã€‹è¿™æœ¬ä¹¦çš„å‡ºç‰ˆæ—¶é—´æ˜¯ä»€ä¹ˆï¼Ÿ</span><br><span class="line">&lt;triple id=3&gt;	çº¿æ€§ä»£æ•° ||| å‡ºç‰ˆæ—¶é—´ ||| 2013-12-30</span><br><span class="line">&lt;answer id=3&gt;	2013-12-30</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure>

<p>æˆ‘ä»¬é€šè¿‡é—®é¢˜</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>ä»£ç æ¥è‡ªtransformers==3.3.1</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>BERTæ¨¡å‹çš„è¾“å…¥ç”±ä¸‰éƒ¨åˆ†å¾—åˆ°ï¼Œtoken embdding, segment embedding, position embeddingã€‚</p>
<h4 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h4><p>å¯¹äºæ‰€æœ‰æ–‡å­—æ¥è¯´ï¼Œè®¡ç®—æœºéƒ½æ˜¯æ— æ³•ç†è§£çš„ï¼Œéœ€è¦è½¬åŒ–ä¸ºæµ®ç‚¹å‘é‡æˆ–è€…æ•´å‹å‘é‡ã€‚BERTé‡‡ç”¨çš„æ˜¯WordPiece tokenizationï¼Œæ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„åˆ†è¯ç®—æ³•ï¼Œä»–ä»¥charä½œä¸ºæœ€å°çš„ç²’åº¦ï¼Œä¸æ–­åœ°å¯»æ‰¾å‡ºç°æœ€å¤šä»¥charä¸ºå•ä½ç»„æˆçš„tokenï¼Œä¹‹åå°†wordè¿›è¡Œåˆ†è¯ï¼Œåˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªçš„tokenã€‚æ¯”å¦‚ingè¿™ä¸ªä¼šç»å¸¸å‡ºç°åœ¨è‹±æ–‡å½“ä¸­ï¼Œæ‰€ä»¥WordPiece ä¼šå§â€I am playing the computer gamesâ€åˆ†ä¸ºâ€I am play ##ing the computer gamesâ€ã€‚ä¸ºäº†è§£å†³OOVé—®é¢˜ã€‚è¯è¡¨ä¸­æœ‰30522ä¸ªè¯ã€‚</p>
<p>åœ¨WordPiece åˆ†è¯çš„åŸºç¡€ä¸Šï¼Œä¹‹åä¼šåŠ å…¥4ä¸ªç‰¹æ®Šè¯æ±‡[CLS],[SEP],[PAD],[UNK]ã€‚[CLS]åŠ å…¥åˆ°å¥é¦–ï¼Œä¸å‚ä¸é¢„è®­ç»ƒï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œfine-tuneã€‚[SEP]ä½œä¸ºå¥å°¾ä»¥åŠåˆ†æ®µæ ‡å¿—ã€‚[PAD]æ˜¯å¡«å……ï¼Œä½¿å¾—å¥å­é•¿åº¦ä¸€æ ·ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†ï¼Œ[UNK]æ˜¯è¡¨æ˜ä¸åœ¨è¯è¡¨å½“ä¸­ã€‚å°†ä»–ä»¬è½¬æ¢æˆone-hot embeddingä¹‹åï¼Œæ¥ä¸€ä¸ªembeddingå±‚ï¼Œå°†è¯è½¬åŒ–ä¸ºæœ€åˆçš„è¯å‘é‡ã€‚</p>
<h4 id="segment-embedding"><a href="#segment-embedding" class="headerlink" title="segment embedding"></a>segment embedding</h4><p>segment embedding ä»…ä»…ä½œä¸ºåŒºåˆ†ä¸¤ä¸ªå¥å­æ¥ä½¿ç”¨ã€‚åœ¨é¢„è®­ç»ƒä¸­ï¼Œè¿˜è¦ä½¿ç”¨é¢„æµ‹å¥å­æ˜¯å¦ç›¸é‚»ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ä¹‹ä¸€ã€‚åœ¨è¾“å…¥æ—¶ï¼Œä¹Ÿä¼šç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚æ¥å½¢æˆsegment embedding.</p>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>position embedding çº¯ä½¿ç”¨nn.embedding è·å¾—ã€‚è®­ç»ƒäº†ä¸€ä¸ªä½ç½®<em>è¯è¡¨</em>ã€‚$W\in \mathbb{R^{512 \times 768}}$ã€‚è¾“å…¥å°±æ˜¯[0,1,2,â€¦,len_seq-1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, :seq_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h3 id="transformer-encoder-å±‚"><a href="#transformer-encoder-å±‚" class="headerlink" title="transformer encoder å±‚"></a>transformer encoder å±‚</h3><p>transformer encoderå±‚ä¸»è¦å¦‚ä¸‹å›¾æ‰€ç¤º</p>
<p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png" alt="image"></p>
<p>ä½¿ç”¨ä¸€ä¸ªä¸Šé¢æ‰€è¯´çš„è¾“å…¥ï¼Œç»è¿‡<code>Multi-Head Attention</code>ï¼Œåœ¨é€šè¿‡æ®‹å·®è¿æ¥ä»¥åŠ<code>Layer Normalization</code>ï¼Œä¹‹åé€šè¿‡<code>FFN</code>ä»¥åŠåˆä¸€ä¸ªæ®‹å·®è¿æ¥ä½œä¸ºè¾“å‡ºã€‚</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>BERTä½¿ç”¨çš„æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå³ç”¨äº$Q,K,V$å…¨éƒ¨æºè‡ªåŒä¸€ä¸ªå‘é‡ã€‚æ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨äº†<strong>ä¸Šä¸‹æ–‡</strong>çš„ä¿¡æ¯æ¥å¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œè¡¨ç¤ºã€‚è®¡ç®—æœºé€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œç†è§£ã€‚æ¯”å¦‚â€œå†¬å¤©åˆ°äº†ï¼Œå¤©æ°”å˜å†·äº†ã€‚â€BERTä¼šæ ¹æ®å¤§é‡è¯¥ç±»çš„æ–‡æœ¬ï¼Œå°†å†¬å¤©å’Œå†·çš„è¯­ä¹‰è¿›è¡Œèåˆã€‚Attentionå€¼çš„è®¡ç®—å…¬å¼å¦‚ä¸‹</p>
<p>$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_k}})V \in \mathbb{R^{len\times d}}$$</p>
<p>å…¶ä¸­ï¼Œ$Q,K,V\in \mathbb{R^{d\times k}},Q=xW_q,K=xW_k,V=xW_v$ï¼Œ$d$æ˜¯éšå±‚ç»´åº¦è¿™é‡Œå¯ä»¥æ³¨æ„åˆ°ï¼Œå› ä¸ºsoftmaxæ˜¯éçº¿æ€§çš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„çŸ©é˜µå˜æ¢æ˜¯æ²¡æ³•å•çº¯ä½¿ç”¨çº¿æ€§å˜æ¢ç”¨$Wx$ä»£æ›¿çš„ã€‚</p>
<hr>
<p><strong>å¤šå¤´</strong>åœ¨å“ªé‡Œ<strong>å¤šå¤´</strong>ã€‚ç›´æ¥å°†åŸæ¥çš„$x$è¿›è¡Œè½¬åŒ–ååˆ‡åˆ†ï¼Œè¯¦æƒ…çœ‹ä¸‹å›¾å°±æ‡‚äº†ã€‚æ¯ä¸€ä¸ªattentionç”Ÿæˆçš„ç»´åº¦éƒ½ä¸é«˜ï¼Œæ‹¼èµ·æ¥å°±è·ŸåŸæ¥ä¸€æ ·äº†ã€‚è¿™å¼ å›¾æœ‰ç‚¹é—®é¢˜ï¼Œå…¶å®BERTæ²¡æœ‰$W^O$ï¼Œå› ä¸º$Z_0,â€¦,Z_7$æ‹¼èµ·æ¥æ­£å¥½æ˜¯$Z$ã€‚</p>
<p><img src="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg" alt="img1"></p>
<blockquote>
<p>BERT å®ç°ä¸­ï¼Œå¤šå¤´çš„ç›®çš„æ˜¯é™ä½å‚æ•°çš„ä¸ªæ•°ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›ã€‚ ç±»ä¼¼äºCNNå¤šä¸ªå·ç§¯æ ¸ï¼Ÿ</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function) [1,1,1,0,0] -&gt; [0,0,0,-10000,-10000]</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>ps: ä»£ç é‡Œå®ç°attention_maskä½¿ç”¨äº†åŠ æ³•ï¼Œå› ä¸ºsoftmaxä¸­é’ˆå¯¹e^0ä¹Ÿä¼šè¾“å‡º1ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å¯¹äºå¿½è§†çš„tokenè¿›è¡Œ$e^{-inf}$ï¼Œæ‰èƒ½ä½¿ä»–åœ¨softmaxä¹‹åçš„æƒé‡ä¸º0ã€‚</p>
<h3 id="add-and-norm"><a href="#add-and-norm" class="headerlink" title="add and norm"></a>add and norm</h3><p>å¸¸è§çš„æ®‹å·®ç½‘ç»œæ–¹å¼æ¢¯åº¦æ¶ˆå¤±ï¼Œå¢åŠ æ¨¡å‹çš„è®­ç»ƒï¼Œæ‰“ç ´äº†ç½‘ç»œçš„å¯¹ç§°æ€§ï¼Œæå‡äº†ç½‘ç»œçš„è¡¨å¾èƒ½åŠ›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="FFN-and-Add-Norm"><a href="#FFN-and-Add-Norm" class="headerlink" title="FFN and Add Norm"></a>FFN and Add Norm</h3><p>å…ˆç»è¿‡ä¸­é—´å±‚3072ï¼Œhidden_sizeæ‰©å¤§4å€ã€‚ä¹‹åå†ç»è¿‡ä¸€ä¸ªç¼©å°äº†ã€‚æ³¨æ„è¿™é‡Œæœ€åæ‰æœ‰ä¸€ä¸ªdropoutã€‚æ¿€æ´»å‡½æ•°ç”¨çš„geluï¼Œæ¯”reluç¨å¾®ç¼“å’Œäº†ä¸€äº›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="æœ€åå¤„ç†"><a href="#æœ€åå¤„ç†" class="headerlink" title="æœ€åå¤„ç†"></a>æœ€åå¤„ç†</h3><p>ç»¼ä¸Šï¼ŒæŠŠè¿™ä¸ªbert_layerå ä¸ª12å±‚å°±è¡Œäº†ã€‚ä½†æ˜¯åœ¨æœ€åè¾“å‡ºçš„æ—¶å€™[CLS]æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å¤„ç†ã€‚è¾“å‡ºçš„æ—¶å€™ä¼šç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚+ä¸€ä¸ª<code>tanh</code>æ¿€æ´»ã€‚</p>
<h3 id="NER-token-classification"><a href="#NER-token-classification" class="headerlink" title="NER  token classification"></a>NER  token classification</h3><p>åœ¨æ¯ä¸€ä¸ªtokenå¯¹åº”çš„è¾“å‡ºåŠ å…¥ä¸€ä¸ªçº¿æ€§åˆ†ç±»å±‚ï¼Œå¯¹åº”æ‰€æœ‰çš„å®ä½“ç±»å‹æ ‡ç­¾æ¯”å¦‚B-PER,I-PERã€‚</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109250703">https://zhuanlan.zhihu.com/p/109250703</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
</blockquote>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><p>CRF Conditional Random Fieldæ¡ä»¶éšæœºåœºã€‚æ˜¯ä¸€ç§æ— å‘å›¾æ¨¡å‹ï¼Œåœ¨ç»™å®šéœ€è¦æ ‡è®°çš„è§‚æµ‹åºåˆ—çš„æ¡ä»¶ä¸‹ï¼Œè®¡ç®—æ•´ä¸€ä¸ªåºåˆ—çš„è”åˆæ¦‚ç‡ã€‚</p>
<p>$\Theta(x_1,â€¦,x_m,s_1,â€¦,s_m) \in \mathbb{R^d}$</p>
<p>ç”±äºBERTæ¨¡å‹åªä¼šé’ˆå¯¹æ¯ä¸€ä¸ªtokenè€Œä¸æ˜¯ä¸€ä¸ªå®ä½“è¾“å‡ºæ ‡ç­¾æ¦‚ç‡ï¼Œè€Œè¾ƒå°‘åœ°è€ƒè™‘åˆ°tokenæ ‡ç­¾ä¹‹é—´çš„å…³ç³»ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å¢å¼ºæ¨¡å‹å¯¹äºç›¸é‚»tokenæ ‡ç­¾ä¹‹é—´å…³ç³»çš„ç†è§£ã€‚æ¯”å¦‚â€æ­å·æ˜¯æµ™æ±Ÿçœçš„çœä¼šåŸå¸‚â€ï¼Œâ€œæ­â€å’Œâ€å·â€å¯èƒ½éƒ½è¢«è¯†åˆ«ä¸ºåœ°åï¼Œä½†æ˜¯è¿™é‡Œåº”è¯¥è¯†åˆ«â€œæ­å·â€ä¸€ä¸ªæ•´ä½“çš„åœ°åï¼Œå°±ä¸åº”è¯¥æ˜¯â€œæ­â€-B-LOCï¼Œâ€œå·â€-B-LOCè€Œåº”è¯¥æ˜¯â€œæ­â€-B-LOCï¼Œâ€œå·â€-I-LOCã€‚</p>
<p>CRFçš„æŸå¤±å‡½æ•°ä¸º$l(\theta)=\cfrac{P_{RealPath}}{P_1+P_2+â€¦+P_N}$</p>
<p>æ¡ä»¶éšæœºåœºä¸º$P(y|x)=\exp[\sum^{SeqLen}_{k=1}\lambda_k \sum^{Cond}_{i=2}t_k(y_{i-1},y_i,x,i)+\sum_l \mu_l \sum_i s_l(y_i,x,i)]$</p>
<p>$t_k$æ˜¯è½¬ç§»ç‰¹å¾å‡½æ•°ï¼Œ$s_l$æ˜¯çŠ¶æ€ç‰¹å¾å‡½æ•°ã€‚</p>
<p>ç”±äºBERTå·²ç»äº§ç”Ÿäº†çŠ¶æ€ç‰¹å¾å‡½æ•°ï¼Œå³æ¯ä¸€ä¸ªtokençš„æ ‡ç­¾æ¦‚ç‡å€¼ï¼ŒCRFåªéœ€è¦å»æ±‚è½¬ç§»ç‰¹å¾å‡½æ•°ï¼Œå³ä¸€ä¸ªé•¿åº¦ä¸ºæ ‡ç­¾ä¸ªæ•°çš„è½¬ç§»çŸ©é˜µå³å¯ã€‚</p>
<h3 id="ä¼˜ç‚¹"><a href="#ä¼˜ç‚¹" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h3><ul>
<li>CRFç›¸å¯¹äºHMMä½¿ç”¨äº†ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œä¸å•å•åªä¾æ®å‰ä¸€ä¸ªçš„çŠ¶æ€æ¥é¢„æµ‹åä¸€ä¸ªçš„çŠ¶æ€ã€‚</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94457579">https://zhuanlan.zhihu.com/p/94457579</a></p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-12T13:55:19.000Z" title="2021-01-12T13:55:19.000Z">2021-01-12</time>å‘è¡¨</span><span class="level-item"><time dateTime="2021-01-14T14:39:09.518Z" title="2021-01-14T14:39:09.518Z">2021-01-14</time>æ›´æ–°</span><span class="level-item">30 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦4429ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLPåŸºç¡€</a></h1><div class="content"><h1 id="nlp-æ·±åº¦å­¦ä¹ åŸºç¡€"><a href="#nlp-æ·±åº¦å­¦ä¹ åŸºç¡€" class="headerlink" title="nlp æ·±åº¦å­¦ä¹ åŸºç¡€"></a>nlp æ·±åº¦å­¦ä¹ åŸºç¡€</h1><p>å°†æ¯ä¸€ä¸ªæ¨¡å‹ä»¥</p>
<ol>
<li>ç®€å•ä»‹ç»</li>
<li>è§£å†³çš„é—®é¢˜</li>
<li>ä»£ç </li>
<li>ä¼˜ç¼ºç‚¹</li>
<li>ä½¿ç”¨tips</li>
</ol>
<p>æ¥å½’ç±»ã€‚æ¯”è¾ƒç°ä»£çš„ä¼šåˆ†æåˆ†æã€‚</p>
<h2 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h2><p>å…¨ç§°(term frequency-inverse document frequency)ï¼ŒTFæŒ‡çš„æ˜¯è¯é¢‘ï¼ŒIDFæŒ‡çš„æ˜¯é€†æ–‡æœ¬é¢‘ç‡æŒ‡æ•°ã€‚ä»–ä»¬çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p>
<p>$TF_w=\cfrac{N_w}{N}$ï¼Œä¸€ä¸ªè¯åœ¨è¯¥å¥å­ä¸­å‡ºç°çš„é¢‘ç‡ã€‚</p>
<p>$IDF_w=\log{\cfrac{Y}{Y_w+1}}$ï¼Œ$Y_w$æ˜¯æ‰€æœ‰æ–‡æ¡£ä¸­åŒ…å«è¯¥è¯çš„æ–‡æ¡£ä¸ªæ•°ã€‚+1æ˜¯æ–¹å¼åˆ†æ¯ä¸º0.</p>
<p>TF-IDFçš„<strong>æ€æƒ³</strong>åœ¨äºï¼Œä¸€ä¸ªè¯å¦‚æœåœ¨ä¸€æ®µå°æ–‡æœ¬ä¸­å‡ºç°å¾—è¶Šå¤šï¼Œé‚£ä¹ˆä»–å¯¹è¿™æ®µæ–‡æœ¬çš„æƒé‡å°±è¶Šå¤§ï¼Œä½†æ˜¯å¦‚æœåœ¨æ‰€æœ‰çš„æ–‡æœ¬ä¸­ï¼Œä»–å‡ºç°çš„æ¬¡æ•°éƒ½å¾ˆå¤šï¼Œå°±åƒè®¡ç®—ä¿¡æ¯ç†µä¸€æ ·ï¼Œåœ¨æ‰€æœ‰æƒ…å†µä¸‹å‡ºç°çš„æ¦‚ç‡å¾ˆå¤§æ—¶ï¼Œé‚£ä¹ˆè¯çš„ä¿¡æ¯å°±å¾ˆå°‘ã€‚æ‰€æœ‰ä½¿ç”¨IDFæ¥æŠµæ¶ˆä¸€äº›å¸¸ç”¨è¯çš„å½±å“ã€‚ç»¼ä¸Šï¼Œè®¡ç®—å…¬å¼ä¸º</p>
<p>$TFIDF_w=TF_w \times IDF_w$</p>
<blockquote>
<p>æ³¨æ„ï¼Œå¯¹äºä¸åŒçš„æ ·æœ¬çš„åŒä¸€ä¸ªè¯ï¼Œ$TF$å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œä½†æ˜¯$IDF$æ˜¯ç›¸åŒçš„ã€‚</p>
</blockquote>
<h3 id="è§£å†³çš„é—®é¢˜"><a href="#è§£å†³çš„é—®é¢˜" class="headerlink" title="è§£å†³çš„é—®é¢˜"></a>è§£å†³çš„é—®é¢˜</h3><p>TF-IDFç›¸å½“äºåœ¨ä»¥å‰æŠŠå…³é”®å­—ä½œä¸ºçŸ­æ–‡æœ¬è¡¨ç¤ºçš„åŸºç¡€ä¸ŠåŠ å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–ï¼Œå‰Šå¼±äº†é«˜é¢‘è¯çš„æƒé‡ã€‚åœ¨ä¸€äº›ç®€å•çš„æ–‡æœ¬åŒ¹é…(å¯¹äºç»™å®šçš„é—®é¢˜ï¼Œä¸å·²çŸ¥æ–‡æœ¬çš„è¯è¯­å°†TFIDFåŠ å’Œå¾—åˆ°ç›¸ä¼¼åº¦)ï¼Œæ–‡æœ¬åˆ†ç±»ä¸Šå¯ä»¥èµ·åˆ°ä¸€å®šçš„æ•ˆæœã€‚</p>
<hr>
<p>ä»£ç </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFIDF</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    self.documents_list = documents_list</span><br><span class="line">    self.tf = []</span><br><span class="line">    self.idf = &#123;&#125;</span><br><span class="line">    df = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> documents_list:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> word <span class="keyword">in</span> document:</span><br><span class="line">        temp[word] = temp.get(word, <span class="number">0</span>) + <span class="number">1.</span>/<span class="built_in">len</span>(document)</span><br><span class="line">     	self.tf.append(temp)</span><br><span class="line">      <span class="comment"># å‡ºç°è¿‡çš„è¯ï¼Œéƒ½+1</span></span><br><span class="line">   		<span class="keyword">for</span> k <span class="keyword">in</span> temp.keys():</span><br><span class="line">        df[k] = df.get(k, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> df.items():</span><br><span class="line">      self.idf[k] = np.log(<span class="built_in">len</span>(documents_list) / (v + <span class="number">1</span>))</span><br><span class="line">    self.tfidf = []</span><br><span class="line">		<span class="keyword">for</span> tf_sentence <span class="keyword">in</span> self.tf:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> tf_sentence.items():</span><br><span class="line">        temp[k] = v * self.idf[k]</span><br><span class="line">      self.tfidf.append(temp)</span><br><span class="line">tfidf = TFIDF([<span class="string">&#x27;I have a pen&#x27;</span>.split(), <span class="string">&#x27;I have an apple&#x27;</span>.split(), <span class="string">&#x27;Bang, apple pen&#x27;</span>.split()])</span><br><span class="line">print(tfidf.tf)</span><br><span class="line">print(tfidf.idf)</span><br><span class="line">print(tfidf.tfidf)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="ä¼˜ç‚¹"><a href="#ä¼˜ç‚¹" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h3><ul>
<li>ä¸€ç§æ— ç›‘ç£çš„ç”Ÿæˆå¥å­è¯è¯­å‘é‡çš„æ–¹æ³•ã€‚</li>
<li>å¯ä»¥å¾ˆå¿«åœ°æ‰¾åˆ°ä¸€å¥è¯çš„å…³é”®å­—</li>
<li>è€—è´¹çš„è®¡ç®—èµ„æºè¾ƒå°‘ã€‚</li>
</ul>
<h3 id="ç¼ºç‚¹"><a href="#ç¼ºç‚¹" class="headerlink" title="ç¼ºç‚¹"></a>ç¼ºç‚¹</h3><ul>
<li>å¥å­å‘é‡ï¼Œæˆ–è€…è¯è¯­å‘é‡æ²¡æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯è¯­ä¸è¯è¯­ä¹‹é—´çš„ä½ç½®å…³ç³»ä¹Ÿæ²¡æœ‰èå…¥åˆ°è¡¨ç¤ºå½“ä¸­ã€‚</li>
<li>ä¼šå°†ç”Ÿåƒ»è¯ä½œä¸ºå…³é”®è¯ï¼Œä½†å…¶å®ç”Ÿåƒ»è¯æ„ä¹‰ä¸å¤§ã€‚</li>
<li>äººååœ°åæ¯”è¾ƒéš¾ä»¥åŒºåˆ†ã€‚</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113017752">https://zhuanlan.zhihu.com/p/113017752</a></p>
</blockquote>
<p>ps: python dictæ˜¯çœŸå¥½ç”¨ã€‚</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>word2vecè¯´æ˜ç™½äº†ä¹Ÿå°±æ˜¯å’ŒTFIDFä¸€æ ·çš„<strong>å°†è¯è¯­ä½¿ç”¨ä¸€ä¸ª$f(x)$æ˜ å°„åˆ°æ•°å€¼çš„å‘é‡ç©ºé—´å½“ä¸­</strong>ã€‚ç”±äºè¯è¯­ä¸åƒæ˜¯åƒç´ ç‚¹æœ‰ç€å¤©ç„¶çš„æ•°å€¼è¡¨ç¤ºï¼Œword2vecé’ˆå¯¹è¯è¯­è½¬åŒ–ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„è¡¨ç¤ºã€‚</p>
<p>word2vecçš„<strong>æ€æƒ³</strong>æ˜¯ä¸€ä¸ªè¯çš„æ„æ€ç”±å®ƒæ—è¾¹çš„è¯æ„æˆã€‚å°±åƒè€è¯è¯´çš„å¥½ï¼Œç‰©ä»¥ç±»èšï¼Œäººä»¥ç¾¤åˆ†ã€‚ä½†å…¶å®word2vecçš„æŸå¤±å‡½æ•°å’Œæ¨¡å‹è§£å†³çš„é—®é¢˜æ˜¯æœ‰ä¸€ç‚¹å‰²è£‚å¼€çš„ã€‚word2vecæƒ³è§£å†³çš„é—®é¢˜æ˜¯ç”Ÿæˆç¨ å¯†çš„word embeddingï¼Œè€Œä¼˜åŒ–æŸå¤±å‡½æ•°çš„ç›®çš„æ˜¯è®©ä¸¤ä¸ªè¯åœ¨windowä¸‹çš„å…³ç³»ç¬¦åˆæ–‡æœ¬ã€‚æŸå¤±å‡½æ•°æ˜¯æƒ³ï¼Œåœ¨ä¸€ä¸ªçª—å£ä¸‹ï¼Œå¦‚æœæˆ‘ä¸­å¿ƒè¯æ˜¯â€å¸…å“¥â€,é‚£ä¹ˆæ¨¡å‹åº”è¯¥èƒ½æ¨æµ‹å‰ä¸¤ä¸ªè¯æ˜¯â€æˆ‘â€å’Œâ€æ˜¯â€ã€‚å› ä¸ºåœ¨æ ·æœ¬ä¸­â€æˆ‘æ˜¯å¸…å“¥â€å‡ºç°è¿‡å¾ˆå¤šæ¬¡ã€‚ä¹‹åç”±äºæŸå¤±å‡½æ•°çš„è®¡ç®—ä¸­ï¼Œé¢„æµ‹çš„æ¡ä»¶æ¦‚ç‡æ˜¯é€šè¿‡è¯å‘é‡çš„ç›¸ä¼¼åº¦æ¥è®¡ç®—çš„ï¼Œæ‰€ä»¥åœ¨ä¼˜åŒ–æ¨¡å‹ä¸­ï¼Œä¹Ÿå°±è¾¾æˆäº†ç›¸ä¼¼è¯æ±‡äº§ç”Ÿç›¸ä¼¼è¯å‘é‡çš„ç›®çš„ã€‚</p>
<p>word2vecåˆåˆ†ä¸ºä¸¤ç§æ¨¡å‹ï¼š</p>
<ol>
<li>skip-gramï¼š ä½¿ç”¨ä¸­å¿ƒè¯å‘¨å›´çš„è¯æ¥é¢„æµ‹ä¸­å¿ƒè¯ã€‚</li>
<li>CBOWï¼š ä½¿ç”¨ä¸­å¿ƒè¯æ¥é¢„æµ‹å‘¨å›´çš„è¯ã€‚</li>
</ol>
<p>æ¯ä¸€ä¸ªè¯æ±‡è¡¨ç¤ºæˆä¸ºä¸¤ä¸ª$d$ç»´çš„å‘é‡ï¼Œç”¨æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ã€‚$v_i \in \mathbb{R^d}$.ä¹‹åæ¯ä¸€ä¸ªwindowä¸‹åœ¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡çš„æ¡ä»¶æ¦‚ç‡å°±æ˜¯ã€‚</p>
<p>$$P(O=o|C=c)=\cfrac{\exp(u_o^Tv_c)}{\sum_{w \in Vocab} \exp(u_w^Tv_c)}$$</p>
<p>ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯ï¼Œ$L(\theta)=\prod_{t=1}^T \prod_{-m \leq j \leq m ,j \neq 0} P(w_{t+j} | w_t; \theta)$</p>
<p>ä¹‹åå¯¹æå¤§ä¼¼ç„¶ä¼°è®¡å¸¸è§„æ“ä½œï¼Œå–logå†æ­£è´Ÿç›¸åï¼Œä»è€Œä½œä¸ºæŸå¤±å‡½æ•°æ±‚æœ€å°ã€‚</p>
<p>![image-20210113160811012](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113160811012.png)</p>
<p>$W_{V\times N}$å°±æ˜¯ç”±ä¸­å¿ƒè¯æ±‡ç»„æˆçš„çŸ©é˜µï¼Œ$Wâ€™_{N \times V}$å°±æ˜¯ä¸Šä¸‹æ–‡è¯æ±‡è¡¨ç¤ºç»„æˆçš„çŸ©é˜µã€‚</p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip gram"></a>skip gram</h3><p>æ¯ä¸€ä¸ªè¯æ±‡è¡¨ç¤ºæˆä¸ºä¸¤ä¸ª$d$ç»´çš„å‘é‡ï¼Œç”¨æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ã€‚$v_i \in \mathbb{R^d}$.è¿™é‡Œæˆ‘ä»¬ç›´æ¥æƒ³è±¡æˆæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸€ä¸ªone hot embeddingï¼Œè¾“å‡ºæ˜¯ç»´åº¦ä¸ºè¯è¡¨çš„å‘é‡ã€‚ä¹‹åæˆ‘ä»¬è¦ä½¿å¾—å‘é‡åœ¨ä¸Šä¸‹æ–‡è¯ä¸Šçš„å€¼æ¥è¿‘ä¸º1(åœ¨æ¿€æ´»å½’ä¸€åŒ–ä¹‹å)ã€‚ç”±äºè¯è¡¨ä¸€èˆ¬å¾ˆé•¿ï¼Œæ‰€ä»¥è®­ç»ƒskip gramçš„æ—¶å€™æœ‰ä¸€ä¸ªtrickã€‚</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>ä½¿ç”¨å¹³å‡åŠ æƒçš„one hot embeddingè¾“å…¥ä¸Šä¸‹æ–‡è¯æ±‡ï¼Œå»é¢„æµ‹ä¸­å¿ƒè¯æ±‡ã€‚</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>å¯¹äºæ•´ä¸ªè¯è¡¨è®¡ç®—ä¸€æ¬¡softmaxçš„å¼€é”€æ˜¯å¾ˆå¤§çš„$O(|V|)$å…¶ä¸­$|V|&gt;&gt;|d|$ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ç§ä¸ä¸€æ ·çš„softmaxæ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚è¿™ä¸ªå°±æ˜¯Hierarchical softmax ç­‰çº§åˆ¶çš„softmaxï¼Œå®ƒé¦–å…ˆä¼šå°†è¯è¡¨è¿›è¡Œåˆ†å±‚ï¼Œæ„å»ºæˆä¸€é¢—å¹³è¡¡äºŒå‰æ ‘ï¼ŒğŸŒ²ä¸Šçš„èŠ‚ç‚¹å°±æ˜¯æˆ‘ä»¬è¦åˆ¤æ–­çš„wordï¼Œæˆ‘ä»¬çŸ¥é“å¹³è¡¡äºŒå‰æ ‘çš„æ·±åº¦æ˜¯$O(\log n)$ã€‚ç»è¿‡$\log n$æ¬¡çš„åˆ¤æ–­ä¹‹åï¼Œå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°äº†ã€‚ä¸€æ¬¡forwardå¤æ‚åº¦æ˜¯$O(\log |V| * d<em>2)$ï¼Œå…¶ä¸­2æ˜¯åœ¨å·¦å³é€‰æ‹©ï¼Œç›¸æ¯”äº$O(|V|+d</em>|V|)$å‡å°‘äº†å¾ˆå¤šã€‚è®¡ç®—Péœ€è¦æŠŠä»æ ¹èŠ‚ç‚¹åˆ°å¶å­èŠ‚ç‚¹ä¸Šçš„æ¯ä¸ªèŠ‚ç‚¹æŒ¨ä¸ªç®—ä¸€éæ¦‚ç‡ã€‚</p>
<p>![image-20210113164654789](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113164654789.png)</p>
<h4 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h4><p>åœ¨ä¸€æ¬¡è®­ç»ƒçš„æ—¶å€™ï¼Œskip gram åªä¼šè¾“å…¥ä¸€ä¸ªè¯ï¼Œå¾ˆç¨€ç–ï¼Œæµªè´¹äº†å…¶ä»–çš„embeddingè®­ç»ƒã€‚åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œä¸ä½¿ç”¨çŸ©é˜µç›´æ¥ä¹˜ï¼Œè€Œæ˜¯ä½¿ç”¨æŒ‘é€‰æ¯”å¦‚(1+10)10ä¸ªè´Ÿæ ·æœ¬æ›´æ–°çŸ©é˜µã€‚æŒ‘é€‰çš„å…¬å¼ä¸ºå‡ºç°è¯„ç‡æ¯”è¾ƒå¤§çš„ã€‚</p>
<p>$$P(w_i)=\cfrac{f(w_i)^{0.75}}{\sum_{j=0}^nf(w_j)^{0.75}}$$</p>
<h3 id="è§£å†³çš„é—®é¢˜-1"><a href="#è§£å†³çš„é—®é¢˜-1" class="headerlink" title="è§£å†³çš„é—®é¢˜"></a>è§£å†³çš„é—®é¢˜</h3><p>è§£å†³one-hot embeddingä¸­è¿‡äºç¨€ç–ï¼Œä»¥åŠéš¾ä»¥è¡¨è¾¾è¯­ä¹‰ç‰¹å¾çš„é—®é¢˜ã€‚</p>
<hr>
<h3 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a>ä»£ç </h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2Vec</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>



<hr>
<h3 id="ä¼˜ç‚¹-1"><a href="#ä¼˜ç‚¹-1" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h3><ul>
<li><p>æ— ç›‘ç£è®­ç»ƒç”Ÿæˆè¯å‘é‡</p>
</li>
<li><p>å¯¹äºç›¸ä¼¼çš„è¯æ±‡æœ‰ç€å¾ˆå¥½çš„è§£é‡Šæ€§ï¼ŒMan - King = Woman - Queen</p>
</li>
</ul>
<h3 id="ç¼ºç‚¹-1"><a href="#ç¼ºç‚¹-1" class="headerlink" title="ç¼ºç‚¹"></a>ç¼ºç‚¹</h3><ul>
<li><p>æ— æ³•ä¸€è¯å¤šä¹‰ã€‚</p>
</li>
<li><p>è®­ç»ƒæ—¶æ²¡æœ‰åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œè®­ç»ƒæ•ˆç‡è¾ƒä½ã€‚</p>
</li>
</ul>
<h2 id="text-cnn"><a href="#text-cnn" class="headerlink" title="text-cnn"></a>text-cnn</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">Convolutional Naural Networks for Sentence Classification</a></p>
<p>æ‹·è´å¿è€…å¡å¡seiï¼Œç›´æ¥æŠ„CVçš„CNNå°±å®Œäº‹äº†ã€‚æˆ‘ä»¬å¯ä»¥å°†ä¸€å¥è¯åˆ©ç”¨word embeddingçœ‹æˆæ˜¯ä¸€å‰¯å›¾åƒï¼Œæ¯”å¦‚é•¿åº¦ä¸º10çš„å¥å­ï¼Œè¯å‘é‡ç»´åº¦ä¸º300ã€‚é‚£ä¹ˆè¿™ä¸ªå¥å­çš„è¾“å…¥å°±æ˜¯$10 \times 300$çš„çŸ©é˜µã€‚ä¹‹åæˆ‘ä»¬å°±å¯ä»¥åƒå›¾åƒä¸€æ ·å¤„ç†æ–‡æœ¬äº†ã€‚</p>
<p><img src="https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg" alt="image-20210113173917546"></p>
<p>æ¨¡å‹å¯ä»¥åˆ†ä¸ºä¸‰å±‚ã€‚</p>
<ol>
<li>è¾“å…¥å±‚æ˜¯ä¸€ä¸ª$k \times n$çš„çŸ©é˜µ</li>
<li>å·ç§¯å±‚ä¸CVæœ‰ä¸€äº›åŒºåˆ«ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æŠŠè¯å‘é‡çœ‹åšæ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œæ‰€ä»¥ä¸ä¼šåœ¨æ¨ªï¼ˆçºµï¼Ÿï¼‰æ–¹å‘ä¸Šè¿›è¡Œå·ç§¯ï¼Œå·ç§¯çª—å£åªä¼šä¸Šä¸‹ç§»åŠ¨ã€‚æ ¸å¤§å°ä¸º$filter_size \times embedding_size$.æ–‡ä¸­å®šä¹‰<code>filiter_size</code>ä¸º[3,4,5]ã€‚å°†å±€éƒ¨çš„ä¿¡æ¯èšåˆã€‚æ¯ä¸€ä¸ªä¸åŒçš„å·ç§¯æ ¸éƒ½ä¼šç”Ÿæˆä¸åŒçš„<code>feature map</code>ï¼Œæ¯”å¦‚è¾“å…¥æ˜¯$10 \times 300$ï¼Œä¹‹åç»è¿‡128ä¸ªå¤§å°ä¸º3çš„å·ç§¯æ“ä½œï¼Œä¼šç”Ÿæˆ128ä¸ªç»´åº¦ä¸º10çš„å‘é‡ã€‚</li>
<li>poolingå±‚ï¼Œç”±äºè¦å¤„ç†å˜é•¿æ–‡æœ¬ï¼Œæ‰€ä»¥æ˜¯å¯¹æ¯ä¸€ä¸ªfeature mapä¸Šå–æœ€å¤§å€¼ä½œä¸ºè¾“å‡ºï¼Œæ‰€ä»¥æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä¸€ä¸ª128ç»´åº¦çš„å‘é‡ã€‚</li>
<li>FFNå’ŒSoftmax å¸¸è§„æ“ä½œï¼Œåˆ†ç±»æ¨¡å‹è·å¾—æ¯ä¸€ç±»çš„æ¦‚ç‡ã€‚</li>
</ol>
<h3 id="è§£å†³çš„é—®é¢˜-2"><a href="#è§£å†³çš„é—®é¢˜-2" class="headerlink" title="è§£å†³çš„é—®é¢˜"></a>è§£å†³çš„é—®é¢˜</h3><p>å°†CNNå¼•å…¥åˆ°NLPå½“ä¸­ï¼Œä»è€Œå‡å°‘äº†æ¨¡å‹çš„å‚æ•°ï¼Œå¹¶åœ¨CNNåœ¨æ•æ‰å±€éƒ¨ä¿¡æ¯æ—¶æœ‰å¥‡æ•ˆã€‚</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="ä¼˜ç‚¹-2"><a href="#ä¼˜ç‚¹-2" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h3><ul>
<li>è·¨æ—¶ä»£åœ°æå‡ºäº†CNNåœ¨NLPé¢†åŸŸçš„åº”ç”¨</li>
<li>å®éªŒåšå¾—å¾ˆè¯¦ç»†ï¼Œé’ˆå¯¹é¢„è®­ç»ƒï¼Œéšæœºç”Ÿæˆçš„è¯å‘é‡éƒ½è¿›è¡Œäº†æ¯”å¯¹ã€‚ï¼ˆæ˜¯ä¸æ˜¯è¿™ä¸ªç»™bertä¸€ç‚¹æ€è€ƒï¼Œä¸éœ€è¦ä¸€ä¸ªword embeddingï¼Œéšæœºåˆå§‹åŒ–å°±å¥½äº†ï¼‰</li>
</ul>
<h3 id="ç¼ºç‚¹-2"><a href="#ç¼ºç‚¹-2" class="headerlink" title="ç¼ºç‚¹"></a>ç¼ºç‚¹</h3><ul>
<li>CNNå·ç§¯å¯¹äºå¥å­æ¥è¯´è¿˜æ˜¯å¤ªå°äº†ã€‚æ²¡æœ‰å…¨å±€ä¿¡æ¯ã€‚ä¸€ä¸ªCNNåªèƒ½ä¼°è®¡5-gramçš„ä¿¡æ¯</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102426363">https://zhuanlan.zhihu.com/p/102426363</a></p>
</blockquote>
<h2 id="rnn-lstm"><a href="#rnn-lstm" class="headerlink" title="rnn lstm"></a>rnn lstm</h2><p>RNNæƒ³å¯¹äºCNNæ¨¡å‹æ¥è¯´å¤šäº†å¾ˆå¤šå˜ç§ã€‚é¦–å…ˆæ¥è¯´ä¸€ä¸‹RNNçš„<strong>æ€æƒ³</strong>å§ã€‚RNNçµæ„Ÿæ¥æºäºäººç±»è¿›è¡Œé˜…è¯»è¿‡ç¨‹ä¸­ï¼Œä¼šä»å·¦åˆ°å³ä¸€ä¸ªå­—ä¸€ä¸ªå­—åœ°è¯»å…¥æ–‡å­—ï¼Œä¹‹åå†å¾—åˆ°è‡ªå·±çš„ç†è§£ã€‚é‚£ä¹ˆæ˜¯å¦æœ‰æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¿™ç§ä»å·¦åˆ°å³çš„æ—¶åºä¿¡æ¯å‘¢ï¼Ÿé‚£å°±æ˜¯RNN(Recurrent Neural Network)ã€‚RNNç”±äºç»“æ„ç²¾å·§æœ‰å¾ˆå¤šå˜ç§ã€‚</p>
<p><img src="https://github.com/YZHANG1270/Markdown_pic/blob/master/2018/11/RNN_01/001.png?raw=true" alt="rnn"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/">http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/</a></p>
</blockquote>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>ä»£ç æ¥è‡ªtransformers==3.3.1</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>BERTæ¨¡å‹çš„è¾“å…¥ç”±ä¸‰éƒ¨åˆ†å¾—åˆ°ï¼Œtoken embdding, segment embedding, position embeddingã€‚</p>
<h4 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h4><p>å¯¹äºæ‰€æœ‰æ–‡å­—æ¥è¯´ï¼Œè®¡ç®—æœºéƒ½æ˜¯æ— æ³•ç†è§£çš„ï¼Œéœ€è¦è½¬åŒ–ä¸ºæµ®ç‚¹å‘é‡æˆ–è€…æ•´å‹å‘é‡ã€‚BERTé‡‡ç”¨çš„æ˜¯WordPiece tokenizationï¼Œæ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„åˆ†è¯ç®—æ³•ï¼Œä»–ä»¥charä½œä¸ºæœ€å°çš„ç²’åº¦ï¼Œä¸æ–­åœ°å¯»æ‰¾å‡ºç°æœ€å¤šä»¥charä¸ºå•ä½ç»„æˆçš„tokenï¼Œä¹‹åå°†wordè¿›è¡Œåˆ†è¯ï¼Œåˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªçš„tokenã€‚æ¯”å¦‚ingè¿™ä¸ªä¼šç»å¸¸å‡ºç°åœ¨è‹±æ–‡å½“ä¸­ï¼Œæ‰€ä»¥WordPiece ä¼šå§â€I am playing the computer gamesâ€åˆ†ä¸ºâ€I am play ##ing the computer gamesâ€ã€‚ä¸ºäº†è§£å†³OOVé—®é¢˜ã€‚è¯è¡¨ä¸­æœ‰30522ä¸ªè¯ã€‚</p>
<p>åœ¨WordPiece åˆ†è¯çš„åŸºç¡€ä¸Šï¼Œä¹‹åä¼šåŠ å…¥4ä¸ªç‰¹æ®Šè¯æ±‡[CLS],[SEP],[PAD],[UNK]ã€‚[CLS]åŠ å…¥åˆ°å¥é¦–ï¼Œä¸å‚ä¸é¢„è®­ç»ƒï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œfine-tuneã€‚[SEP]ä½œä¸ºå¥å°¾ä»¥åŠåˆ†æ®µæ ‡å¿—ã€‚[PAD]æ˜¯å¡«å……ï¼Œä½¿å¾—å¥å­é•¿åº¦ä¸€æ ·ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†ï¼Œ[UNK]æ˜¯è¡¨æ˜ä¸åœ¨è¯è¡¨å½“ä¸­ã€‚å°†ä»–ä»¬è½¬æ¢æˆone-hot embeddingä¹‹åï¼Œæ¥ä¸€ä¸ªembeddingå±‚ï¼Œå°†è¯è½¬åŒ–ä¸ºæœ€åˆçš„è¯å‘é‡ã€‚</p>
<h4 id="segment-embedding"><a href="#segment-embedding" class="headerlink" title="segment embedding"></a>segment embedding</h4><p>segment embedding ä»…ä»…ä½œä¸ºåŒºåˆ†ä¸¤ä¸ªå¥å­æ¥ä½¿ç”¨ã€‚åœ¨é¢„è®­ç»ƒä¸­ï¼Œè¿˜è¦ä½¿ç”¨é¢„æµ‹å¥å­æ˜¯å¦ç›¸é‚»ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ä¹‹ä¸€ã€‚åœ¨è¾“å…¥æ—¶ï¼Œä¹Ÿä¼šç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚æ¥å½¢æˆsegment embedding.</p>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>position embedding çº¯ä½¿ç”¨nn.embedding è·å¾—ã€‚è®­ç»ƒäº†ä¸€ä¸ªä½ç½®<em>è¯è¡¨</em>ã€‚$W\in \mathbb{R^{512 \times 768}}$ã€‚è¾“å…¥å°±æ˜¯[0,1,2,â€¦,len_seq-1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, :seq_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h3 id="transformer-encoder-å±‚"><a href="#transformer-encoder-å±‚" class="headerlink" title="transformer encoder å±‚"></a>transformer encoder å±‚</h3><p>transformer encoderå±‚ä¸»è¦å¦‚ä¸‹å›¾æ‰€ç¤º</p>
<p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png" alt="image"></p>
<p>ä½¿ç”¨ä¸€ä¸ªä¸Šé¢æ‰€è¯´çš„è¾“å…¥ï¼Œç»è¿‡<code>Multi-Head Attention</code>ï¼Œåœ¨é€šè¿‡æ®‹å·®è¿æ¥ä»¥åŠ<code>Layer Normalization</code>ï¼Œä¹‹åé€šè¿‡<code>FFN</code>ä»¥åŠåˆä¸€ä¸ªæ®‹å·®è¿æ¥ä½œä¸ºè¾“å‡ºã€‚</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>BERTä½¿ç”¨çš„æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå³ç”¨äº$Q,K,V$å…¨éƒ¨æºè‡ªåŒä¸€ä¸ªå‘é‡ã€‚æ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨äº†<strong>ä¸Šä¸‹æ–‡</strong>çš„ä¿¡æ¯æ¥å¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œè¡¨ç¤ºã€‚è®¡ç®—æœºé€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œç†è§£ã€‚æ¯”å¦‚â€œå†¬å¤©åˆ°äº†ï¼Œå¤©æ°”å˜å†·äº†ã€‚â€BERTä¼šæ ¹æ®å¤§é‡è¯¥ç±»çš„æ–‡æœ¬ï¼Œå°†å†¬å¤©å’Œå†·çš„è¯­ä¹‰è¿›è¡Œèåˆã€‚Attentionå€¼çš„è®¡ç®—å…¬å¼å¦‚ä¸‹</p>
<p>$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_k}})V \in \mathbb{R^{len\times d}}$$</p>
<p>å…¶ä¸­ï¼Œ$Q,K,V\in \mathbb{R^{d\times k}},Q=xW_q,K=xW_k,V=xW_v$ï¼Œ$d$æ˜¯éšå±‚ç»´åº¦è¿™é‡Œå¯ä»¥æ³¨æ„åˆ°ï¼Œå› ä¸ºsoftmaxæ˜¯éçº¿æ€§çš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„çŸ©é˜µå˜æ¢æ˜¯æ²¡æ³•å•çº¯ä½¿ç”¨çº¿æ€§å˜æ¢ç”¨$Wx$ä»£æ›¿çš„ã€‚</p>
<hr>
<p><strong>å¤šå¤´</strong>åœ¨å“ªé‡Œ<strong>å¤šå¤´</strong>ã€‚ç›´æ¥å°†åŸæ¥çš„$x$è¿›è¡Œè½¬åŒ–ååˆ‡åˆ†ï¼Œè¯¦æƒ…çœ‹ä¸‹å›¾å°±æ‡‚äº†ã€‚æ¯ä¸€ä¸ªattentionç”Ÿæˆçš„ç»´åº¦éƒ½ä¸é«˜ï¼Œæ‹¼èµ·æ¥å°±è·ŸåŸæ¥ä¸€æ ·äº†ã€‚è¿™å¼ å›¾æœ‰ç‚¹é—®é¢˜ï¼Œå…¶å®BERTæ²¡æœ‰$W^O$ï¼Œå› ä¸º$Z_0,â€¦,Z_7$æ‹¼èµ·æ¥æ­£å¥½æ˜¯$Z$ã€‚</p>
<p><img src="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg" alt="img1"></p>
<blockquote>
<p>BERT å®ç°ä¸­ï¼Œå¤šå¤´çš„ç›®çš„æ˜¯é™ä½å‚æ•°çš„ä¸ªæ•°ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›ã€‚ ç±»ä¼¼äºCNNå¤šä¸ªå·ç§¯æ ¸ï¼Ÿ</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function) [1,1,1,0,0] -&gt; [0,0,0,-10000,-10000]</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>ps: ä»£ç é‡Œå®ç°attention_maskä½¿ç”¨äº†åŠ æ³•ï¼Œå› ä¸ºsoftmaxä¸­é’ˆå¯¹e^0ä¹Ÿä¼šè¾“å‡º1ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å¯¹äºå¿½è§†çš„tokenè¿›è¡Œ$e^{-inf}$ï¼Œæ‰èƒ½ä½¿ä»–åœ¨softmaxä¹‹åçš„æƒé‡ä¸º0ã€‚</p>
<h3 id="add-and-norm"><a href="#add-and-norm" class="headerlink" title="add and norm"></a>add and norm</h3><p>å¸¸è§çš„æ®‹å·®ç½‘ç»œæ–¹å¼æ¢¯åº¦æ¶ˆå¤±ï¼Œå¢åŠ æ¨¡å‹çš„è®­ç»ƒï¼Œæ‰“ç ´äº†ç½‘ç»œçš„å¯¹ç§°æ€§ï¼Œæå‡äº†ç½‘ç»œçš„è¡¨å¾èƒ½åŠ›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="FFN-and-Add-Norm"><a href="#FFN-and-Add-Norm" class="headerlink" title="FFN and Add Norm"></a>FFN and Add Norm</h3><p>å…ˆç»è¿‡ä¸­é—´å±‚3072ï¼Œhidden_sizeæ‰©å¤§4å€ã€‚ä¹‹åå†ç»è¿‡ä¸€ä¸ªç¼©å°äº†ã€‚æ³¨æ„è¿™é‡Œæœ€åæ‰æœ‰ä¸€ä¸ªdropoutã€‚æ¿€æ´»å‡½æ•°ç”¨çš„geluï¼Œæ¯”reluç¨å¾®ç¼“å’Œäº†ä¸€äº›ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="æœ€åå¤„ç†"><a href="#æœ€åå¤„ç†" class="headerlink" title="æœ€åå¤„ç†"></a>æœ€åå¤„ç†</h3><p>ç»¼ä¸Šï¼ŒæŠŠè¿™ä¸ªbert_layerå ä¸ª12å±‚å°±è¡Œäº†ã€‚ä½†æ˜¯åœ¨æœ€åè¾“å‡ºçš„æ—¶å€™[CLS]æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å¤„ç†ã€‚è¾“å‡ºçš„æ—¶å€™ä¼šç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚+ä¸€ä¸ª<code>tanh</code>æ¿€æ´»ã€‚</p>
<h3 id="NER-token-classification"><a href="#NER-token-classification" class="headerlink" title="NER  token classification"></a>NER  token classification</h3><p>åœ¨æ¯ä¸€ä¸ªtokenå¯¹åº”çš„è¾“å‡ºåŠ å…¥ä¸€ä¸ªçº¿æ€§åˆ†ç±»å±‚ï¼Œå¯¹åº”æ‰€æœ‰çš„å®ä½“ç±»å‹æ ‡ç­¾æ¯”å¦‚B-PER,I-PERã€‚</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109250703">https://zhuanlan.zhihu.com/p/109250703</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-07T09:40:38.000Z" title="2021-01-07T09:40:38.000Z">2021-01-07</time>å‘è¡¨</span><span class="level-item"><time dateTime="2021-01-08T10:29:15.321Z" title="2021-01-08T10:29:15.321Z">2021-01-08</time>æ›´æ–°</span><span class="level-item">1 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦168ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">å¦‚ä½•åœ¨colabç”¨pytorch_lightningç™½å«–TPU</a></h1><div class="content"><h1 id="å¦‚ä½•åœ¨colabç”¨pytorch-lightningç™½å«–TPU"><a href="#å¦‚ä½•åœ¨colabç”¨pytorch-lightningç™½å«–TPU" class="headerlink" title="å¦‚ä½•åœ¨colabç”¨pytorch_lightningç™½å«–TPU"></a>å¦‚ä½•åœ¨colabç”¨pytorch_lightningç™½å«–TPU</h1><h2 id="é¦–å…ˆ"><a href="#é¦–å…ˆ" class="headerlink" title="é¦–å…ˆ"></a>é¦–å…ˆ</h2><p>æˆ‘ä»¬å¾—ç†Ÿæ‚‰<code>pytorch_lightning</code>ä»¥åŠ<code>colab</code>çš„æ“ä½œã€‚åœ¨ä½¿ç”¨TPUä¹‹å‰ï¼Œå…ˆæŒ‚è½½å¥½æˆ‘ä»¬çš„äº‘ç«¯ç£ç›˜ä»¥åŠé…ç½®å¥½pytorchéœ€è¦çš„TPUç¯å¢ƒã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/gdrive&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&quot;gdrive/MyDrive/path_to_your_file&quot;</span>)</span><br><span class="line">! pip install pytorch_lightning</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¯•å›¾ä½¿ç”¨TPU</span></span><br><span class="line">!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py</span><br><span class="line">!python pytorch-xla-env-setup.py  --apt-packages libomp5 libopenblas-dev</span><br></pre></td></tr></table></figure>

<p>å®‰è£…å¥½<code>pytorch_lightning</code>ä»¥åŠé…ç½®å¥½ç¯å¢ƒä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹åœ¨<code>pytorch_lightning</code>æ¡†æ¶ä¸‹è·Ÿå†™GPUä»£ç ä¸€æ ·å†™TPUä»£ç ã€‚ä¸‹é¢è¯·çœ‹<del>VCR</del>ï¼Œä¸€ä¸ªå°æ —å­ã€‚</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-11T09:07:57.000Z" title="2020-12-11T09:07:57.000Z">2020-12-11</time>å‘è¡¨</span><span class="level-item"><time dateTime="2020-12-11T13:40:29.706Z" title="2020-12-11T13:40:29.706Z">2020-12-11</time>æ›´æ–°</span><span class="level-item">2 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦237ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizerå­¦ä¹ ç¬”è®°</a></h1><div class="content"><h1 id="ç¬”è®°"><a href="#ç¬”è®°" class="headerlink" title="ç¬”è®°"></a>ç¬”è®°</h1><blockquote>
<p>Transformers==4.0.0</p>
<p>ç”±äºæ¯æ¬¡è°ƒç”¨bertç­‰æ¨¡å‹éƒ½éœ€è¦ä½¿ç”¨æ¨¡å‹çš„tokenizerï¼Œæ‰€ä»¥å†™ä¸ªç¬”è®°ï¼Œæ–¹ä¾¿è‡ªå·±ä»¥åŠåäººæŸ¥é˜…ï¼Œï¼ˆå…¶å®çœ‹å®˜æ–¹æ–‡æ¡£ä¹Ÿå¯ä»¥ï¼Œä½†æ˜¯è‹±æ–‡çš„çœ‹ç€å¤´ç–¼ã€‚æœ‰é”™è¯¯ä¹Ÿè¯·ç•™è¨€å§ã€‚</p>
</blockquote>
<h2 id="base-PreTrainedTokenizerBase"><a href="#base-PreTrainedTokenizerBase" class="headerlink" title="base : PreTrainedTokenizerBase"></a>base : <code>PreTrainedTokenizerBase</code></h2><p>ä½œä¸ºåŸºç±»ï¼Œè¯¥ç±»æœ‰ç€æ‰€æœ‰tokenizeréƒ½å…·æœ‰çš„æ–¹æ³•è¿˜æœ‰å±æ€§ã€‚</p>
<h3 id="PreTrainedTokenizer"><a href="#PreTrainedTokenizer" class="headerlink" title="PreTrainedTokenizer"></a>PreTrainedTokenizer</h3><h2 id="ä½¿ç”¨multiprocess-åŠ é€Ÿtokenize"><a href="#ä½¿ç”¨multiprocess-åŠ é€Ÿtokenize" class="headerlink" title="ä½¿ç”¨multiprocess åŠ é€Ÿtokenize"></a>ä½¿ç”¨multiprocess åŠ é€Ÿtokenize</h2><p>ä»¿ç…§è¿™ä¸ªå°±å®Œäº‹äº†ã€‚<code>partial</code>å¯ä»¥å›ºå®šå‡½æ•°ä¸­çš„å‚æ•°ï¼Œç®€ç›´æ˜¯ä¸“é—¨ä¸ºå¤šè¿›ç¨‹å‡†å¤‡çš„ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squad_convert_example_to_features_init</span>(<span class="params">tokenizer_for_convert: PreTrainedTokenizerBase</span>):</span></span><br><span class="line">    <span class="keyword">global</span> tokenizer</span><br><span class="line">    tokenizer = tokenizer_for_convert</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = []</span><br><span class="line"></span><br><span class="line">threads = <span class="built_in">min</span>(threads, cpu_count())</span><br><span class="line"><span class="keyword">with</span> Pool(threads, initializer=squad_convert_example_to_features_init, initargs=				(tokenizer,)) <span class="keyword">as</span> p:</span><br><span class="line">  annotate_ = partial(</span><br><span class="line">    tokenzier.encode_plus, <span class="comment"># batch_encode_plus ä¸çŸ¥é“ä¸ºä»€ä¹ˆä¸work</span></span><br><span class="line">    max_seq_length=max_seq_length,</span><br><span class="line">    doc_stride=doc_stride,</span><br><span class="line">    max_query_length=max_query_length,</span><br><span class="line">    padding_strategy=padding_strategy,</span><br><span class="line">    is_training=is_training,</span><br><span class="line">  )</span><br><span class="line">  features = <span class="built_in">list</span>(</span><br><span class="line">    tqdm(</span><br><span class="line">      p.imap(annotate_, examples, chunksize=<span class="number">32</span>),</span><br><span class="line">      total=<span class="built_in">len</span>(examples),</span><br><span class="line">      desc=<span class="string">&quot;convert squad examples to features&quot;</span>,</span><br><span class="line">      disable=<span class="keyword">not</span> tqdm_enabled,</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-07T11:33:57.000Z" title="2020-12-07T11:33:57.000Z">2020-12-07</time>å‘è¡¨</span><span class="level-item"><time dateTime="2021-01-06T07:45:59.695Z" title="2021-01-06T07:45:59.695Z">2021-01-06</time>æ›´æ–°</span><span class="level-item">4 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦637ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/07/pytorch-cheat-list.html">pytorch cheat_list</a></h1><div class="content"><h1 id="pytorch-æ“ä½œå°è®¡"><a href="#pytorch-æ“ä½œå°è®¡" class="headerlink" title="pytorch æ“ä½œå°è®¡"></a>pytorch æ“ä½œå°è®¡</h1><blockquote>
<p>torch==1.7.0</p>
</blockquote>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><h3 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h3><p>å°†List[tensor]å˜æˆtensorã€‚<code>torch.stack(tensors,dim=0,out=None)</code>Concatenates a sequence of tensors along a new dimension.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">a = torch.stack([b, b], dim = <span class="number">0</span>) <span class="comment"># a.shape = (2,4)</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-gather"><a href="#torch-gather" class="headerlink" title="torch.gather"></a>torch.gather</h3><blockquote>
<p>torch.gather(<em>input</em>, <em>dim</em>, <em>index</em>, *, <em>sparse_grad=False</em>, <em>out=None</em>) â†’ Tensor</p>
</blockquote>
<p>è‹±æ–‡è§£é‡Šä¸ºGathers values along an axis specified by dim.</p>
<p>çœ‹åˆ°æ¯”è¾ƒæœ‰é“ç†çš„åº”ç”¨åœºæ™¯æ˜¯ï¼Œåœ¨å˜é•¿åºåˆ—ä¸­gatheråˆ°æœ€åä¸€ä¸ªæˆ–è€…è¯´å€’æ•°ç¬¬å‡ ä¸ªå…ƒç´ ã€‚ä¸€èˆ¬å˜é•¿åºåˆ—ä¸º<code>inputs = [[1,2,3,0,0], [2,3,4,5,0]]</code>ã€‚è¿™æ—¶å€™æƒ³è·å¾—æœ€åä¸€ä¸ªå…ƒç´ å°±å¯ä»¥ã€‚éœ€è¦æ³¨æ„çš„ç‚¹æ˜¯ï¼Œè¾“å‡ºçš„tensorå’Œindexæ˜¯ç›¸åŒ<code>shape</code>çš„ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">         	[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]])</span><br><span class="line">index = torch.tensor([[<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.long)</span><br><span class="line">last_inputs = torch.gather(inputs, dim=<span class="number">1</span>, index)</span><br><span class="line"><span class="string">&quot;&quot;&quot;tensor([[3],</span></span><br><span class="line"><span class="string">           [5]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="torch-expand"><a href="#torch-expand" class="headerlink" title="torch.expand"></a>torch.expand</h3><p>å°†tensoræ‰©å±•ç»´åº¦ï¼Œè‡ªåŠ¨å¤åˆ¶ï¼Œååˆ†å¥½ç”¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">5</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#tensor([[3, 1, 1],</span></span><br><span class="line"><span class="comment">#        [1, 2, 4]])</span></span><br><span class="line">a = a.unsqueeze(<span class="number">2</span>).expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[3, 3, 3],</span></span><br><span class="line"><span class="string">         [1, 1, 1],</span></span><br><span class="line"><span class="string">         [1, 1, 1]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[1, 1, 1],</span></span><br><span class="line"><span class="string">         [2, 2, 2],</span></span><br><span class="line"><span class="string">         [4, 4, 4]]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat"></a>torch.repeat</h3><p><code>repeat(*sizes) -&gt; Tensor</code>ï¼Œ é‡å¤å¤åˆ¶tensoråœ¨æŒ‡å®šçš„ç»´åº¦ä¸Šã€‚å…¶å®æœ‰ç‚¹ç±»ä¼¼äºå¹¿æ’­æ“ä½œäº†ï¼Ÿ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># x.shape=[3]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>) <span class="comment"># x.shape=[4,6]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># x.shape=[4,2,3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="F-softmax"><a href="#F-softmax" class="headerlink" title="F.softmax"></a>F.softmax</h3><p><code>F.softmax(Tensor, dim=None)</code> å¯¹äºå¤šç»´åº¦çŸ©é˜µå°±æ˜¯ einsum(â€˜ijk -&gt; jkâ€™, a) = torch.ones(a.shape[1:])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">a = F.softmax(a, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p>ä¹‹å‰ä¸€ç›´ä¾èµ–ç€<code>huggingface</code>çš„æ¨¡å‹åŠ è½½<code>from_pretrained</code>ï¼Œä½†å…¶å®åœ¨ä¸€èˆ¬ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œä½¿ç”¨<code>torch.load</code>çš„æ—¶å€™ä¼šæ›´å¤šï¼Œæ‰€ä»¥è®°å½•ä¸€ä¸‹<code>torch.load</code>æ–¹æ³•çš„ä½¿ç”¨åœºæ™¯ã€‚</p>
<h3 id="torch-load-amp-torch-save"><a href="#torch-load-amp-torch-save" class="headerlink" title="torch.load &amp; torch.save"></a>torch.load &amp; torch.save</h3><p>ä¸€èˆ¬æˆ‘ä»¬å°†æ¨¡å‹çš„å‚æ•°ä¿å­˜ï¼Œè€Œä¸ä¼šå»ä¿å­˜æ•´ä¸ªæ¨¡å‹çš„ç»“æ„ã€‚è¿™é‡Œå¦‚æœéœ€è¦<strong>éƒ¨åˆ†</strong>åŠ è½½å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨<code>strict=False</code>ã€‚è¿™é‡Œéœ€è¦æ³¨æ„åŠ è½½çš„æ˜¯<strong>å­—å…¸dict</strong>ï¼Œä¸æ˜¯æ¨¡å‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model ... after training</span></span><br><span class="line">torch.save(model.state_dict(), cached_file_path)</span><br><span class="line">model_state = torch.load(cached_file_path)</span><br><span class="line">model.load_state_dict(model_state, strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>













<h2 id="å¥‡æ·«æŠ€å·§"><a href="#å¥‡æ·«æŠ€å·§" class="headerlink" title="å¥‡æ·«æŠ€å·§"></a>å¥‡æ·«æŠ€å·§</h2><h3 id="whole-word-mask"><a href="#whole-word-mask" class="headerlink" title="whole word mask"></a>whole word mask</h3><p>åœ¨bertæˆ–è€…å…¶ä»–è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹ä¸€æ®µæ–‡æœ¬éœ€è¦å…ˆè¿›è¡Œtokenizeåˆ†è¯æ“ä½œï¼Œè€Œåˆ†è‹±æ–‡å•è¯çš„æ—¶å€™ï¼Œç”±äºOOVé—®é¢˜ï¼Œä¼šå°†æœ‰äº›wordåˆ†æˆtokençº§åˆ«çš„ï¼Œæ¯”å¦‚å°†<code>trying</code>åˆ†æˆ<code>try</code>,<code>##ing</code>ã€‚è€Œæˆ‘ä»¬æ¯”å¦‚åœ¨å»ºå›¾æˆ–è€…ä»¥wordä¸ºç²’åº¦çš„æ—¶å€™ï¼Œå°±éœ€è¦å°†tokençš„è¾“å‡ºå¹³å‡ç»™wordäº†ã€‚é‚£ä¹ˆå¦‚ä½•æ“ä½œå‘¢ï¼Ÿ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoder_output = encoder_outputs[i]  <span class="comment"># [slen, bert_hidden_size]</span></span><br><span class="line">word_num = <span class="number">123</span></span><br><span class="line">word_index=(torch.arange(word_num) + <span class="number">1</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">words = pos_id[i].unsqueeze(<span class="number">0</span>).expand(mention_num, <span class="number">-1</span>)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = (mention_index == mentions).<span class="built_in">float</span>()  <span class="comment"># [mention_num, slen]</span></span><br><span class="line"><span class="comment"># average word -&gt; mention</span></span><br><span class="line">word_total_numbers = torch.<span class="built_in">sum</span>(select_metrix, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = torch.where(word_total_numbers &gt; <span class="number">0</span>, select_metrix / word_total_numbers, select_metrix)</span><br><span class="line">x = torch.mm(select_metrix, encoder_output)</span><br></pre></td></tr></table></figure>



</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-04T12:42:54.000Z" title="2020-12-04T12:42:54.000Z">2020-12-04</time>å‘è¡¨</span><span class="level-item"><time dateTime="2020-12-04T12:44:25.242Z" title="2020-12-04T12:44:25.242Z">2020-12-04</time>æ›´æ–°</span><span class="level-item">å‡ ç§’è¯»å®Œ (å¤§çº¦22ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/04/dgl-%E5%AE%9E%E4%BE%8B%E5%AD%A6%E4%B9%A0.html">dgl å®ä¾‹å­¦ä¹ </a></h1><div class="content"><h1 id="å­¦ä¹ å¼€æºé¡¹ç›®"><a href="#å­¦ä¹ å¼€æºé¡¹ç›®" class="headerlink" title="å­¦ä¹ å¼€æºé¡¹ç›®"></a>å­¦ä¹ å¼€æºé¡¹ç›®</h1><p><a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">é¡¹ç›®ä¼ é€é—¨</a></p>
<h2 id="æ¨¡å‹æµç¨‹"><a href="#æ¨¡å‹æµç¨‹" class="headerlink" title="æ¨¡å‹æµç¨‹"></a>æ¨¡å‹æµç¨‹</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-03T13:15:59.000Z" title="2020-12-03T13:15:59.000Z">2020-12-03</time>å‘è¡¨</span><span class="level-item"><time dateTime="2020-12-11T07:58:48.893Z" title="2020-12-11T07:58:48.893Z">2020-12-11</time>æ›´æ–°</span><span class="level-item">1 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦154ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/03/Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks è®ºæ–‡è§£è¯»</a></h1><div class="content"><h1 id="Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-è®ºæ–‡è§£è¯»"><a href="#Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-è®ºæ–‡è§£è¯»" class="headerlink" title="Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks è®ºæ–‡è§£è¯»"></a>Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks è®ºæ–‡è§£è¯»</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.05092.pdf">è®ºæ–‡ä¼ é€é—¨</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">ä»£ç ä¼ é€é—¨</a></p>
<h2 id="ä»»åŠ¡"><a href="#ä»»åŠ¡" class="headerlink" title="ä»»åŠ¡"></a>ä»»åŠ¡</h2><p>ä»å¯¹è¯æ•°æ®é›†ä¸­æŠ½å–å‡ºå®ä½“ä¹‹é—´çš„è”ç³»ã€‚ç»™å®šä¸€æ®µå¯¹è¯ï¼Œè¾“å‡ºæŒ‡å®šä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œæ€»å…±æœ‰<code>37</code>ä¸­ä¸åŒçš„å…³ç³»ã€‚</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>é’ˆå¯¹å¯¹è¯ä¸­ä¸åŒçš„ä¿¡æ¯è¿›è¡Œembeddingï¼Œ</p>
<ol>
<li><code>utterance</code> ï¼Œå¯¹è¯ä¸­çš„<code>è¯</code>ä½¿ç”¨<code>LSTM</code>æ¨¡å‹è¿›è¡Œencodeã€‚</li>
<li><code>speaker</code> </li>
<li><code>argument</code></li>
<li><code>entity-type</code></li>
<li><code>word</code></li>
</ol>
<p>ç”±ç€å››ç§<code>embedding</code>æ¥æ„å»ºå›¾ã€‚ä¹‹åæ‹¼æ¥å­¦åˆ°çš„<code>argument embedding</code>é€šè¿‡åˆ†ç±»å™¨è¾“å‡ºå…³ç³»ã€‚</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-11-18T09:05:20.000Z" title="2020-11-18T09:05:20.000Z">2020-11-18</time>å‘è¡¨</span><span class="level-item"><time dateTime="2020-12-04T13:21:10.677Z" title="2020-12-04T13:21:10.677Z">2020-12-04</time>æ›´æ–°</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E8%A7%A3/">è®ºè§£</a></span><span class="level-item">9 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦1379ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%AE%BA%E8%A7%A3/2020/11/18/ALBERT-%E6%9B%B4%E5%B0%8F%E4%BD%86%E6%98%AF%E6%9B%B4%E6%85%A2%EF%BC%9F.html">ALBERT æ›´å°ä½†æ˜¯æ›´æ…¢ï¼Ÿ</a></h1><div class="content"><h1 id="ALBERT-æ›´å°ä½†æ˜¯æ›´æ…¢ï¼Ÿ"><a href="#ALBERT-æ›´å°ä½†æ˜¯æ›´æ…¢ï¼Ÿ" class="headerlink" title="ALBERT æ›´å°ä½†æ˜¯æ›´æ…¢ï¼Ÿ"></a>ALBERT æ›´å°ä½†æ˜¯æ›´æ…¢ï¼Ÿ</h1><p>æœ€è¿‘ç”±äºå‚åŠ é˜…è¯»ç†è§£æ¯”èµ›ï¼Œæ‰€ä»¥å¤§é‡æµ‹è¯•å„ç§æ¨¡å‹ï¼ŒæƒŠå¥‡åœ°å‘ç°åŸæœ¬ç°åœ¨é˜…è¯»ç†è§£æ¯”èµ›ä¸­SOTAçš„æ¨¡å‹å±…ç„¶æ˜¯ä¸èµ·çœ¼å¹¶ä¸”ä»¥å°æ¨¡å‹é—»åçš„<code>ALBERT</code>ã€‚è¿™è®©æˆ‘å¯¹è¿™ä¸ªâ€œå°â€æ¨¡å‹äº§ç”Ÿäº†å¥½å¥‡ã€‚ä»è€Œå†™ä¸€ä¸‹è¿™ä»½çš„è®ºæ–‡ç¬”è®°ã€‚</p>
<h2 id="æ‘˜è¦"><a href="#æ‘˜è¦" class="headerlink" title="æ‘˜è¦"></a>æ‘˜è¦</h2><p>æ¨¡å‹è¶Šå¤§ä¸‹æ¸¸æ•ˆæœè¶Šå¼ºæ˜¯ä¼—æ‰€å‘¨çŸ¥çš„é“ç†ï¼Œä½†æ˜¯ç”±äºç¡¬ä»¶è®¾å¤‡å’Œæ˜¾å­˜æ‰€é™ï¼Œæ‰€ä»¥æ¨¡å‹ä¸èƒ½æ— é™åˆ¶å¾—æ”¾å¤§ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªå…¨é¢é¢†å…ˆBERTæ¨¡å‹çš„<code>ALBERT</code>ï¼Œåœ¨æ¯”<code>BERT-LARGE</code>å‚æ•°å°çš„æƒ…å†µä¸‹è¶…è¿‡äº†å®ƒã€‚</p>
<h2 id="æœ‰ä½•åŒºåˆ«"><a href="#æœ‰ä½•åŒºåˆ«" class="headerlink" title="æœ‰ä½•åŒºåˆ«"></a>æœ‰ä½•åŒºåˆ«</h2><h3 id="1-embedding-å‚æ•°å‡å°‘"><a href="#1-embedding-å‚æ•°å‡å°‘" class="headerlink" title="1. embedding å‚æ•°å‡å°‘"></a>1. embedding å‚æ•°å‡å°‘</h3><p>åœ¨ä»one-hot embeddingåˆ°hidden size embeddingæœ‰ä¸€ä¸ª$V \times H$çš„å…¨è¿æ¥å±‚ï¼Œè¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªtrickï¼ŒåŠ äº†ä¸€ä¸ªhidden layerï¼Œä»è€Œä½¿å¾—å…¨è¿æ¥å±‚å˜æˆäº†$V\times E + E\times H$ã€‚è¿™æ ·å­æˆ‘ä»¬å°±å¯ä»¥ç”¨ä¸€ä¸ªå¾ˆå¤§çš„$H$äº†ï¼Œæ¯”å¦‚åœ¨<code>xxlarge</code>ä¸Šå°±æ˜¯$H=4096$ã€‚</p>
<h3 id="2-å±‚é—´å‚æ•°å…±äº«"><a href="#2-å±‚é—´å‚æ•°å…±äº«" class="headerlink" title="2.å±‚é—´å‚æ•°å…±äº«"></a>2.å±‚é—´å‚æ•°å…±äº«</h3><p>å¾ˆç®€å•ï¼Œå°±æ˜¯åŸæ¥æ¨¡å‹ç±»ä¼¼äº$F(x) = f_n(f_{n-1}(â€¦f_1(x)))$ï¼Œä½†æ˜¯ç°åœ¨å˜æˆäº†$F(x)=f(f(â€¦f(x)))$ã€‚æˆ‘ä¹Ÿåœ¨æƒ³ï¼Œè™½ç„¶$f(x)$æ˜¯ä¸€ä¸ªéçº¿æ€§çš„ï¼Œä½†è¿™ç§å½¢å¼æ˜¯ä¸æ˜¯å¯ä»¥æœ‰å‡½æ•°å»æ‹Ÿåˆ$F(x)$ï¼Œæ¯•ç«Ÿé‡å¤$f(x)$è¿™ä¸èƒ½ä¼˜åŒ–å—ï¼Ÿ å»å‹ç¼©<code>ALBERT</code>æ¨¡å‹çš„å¤§å°ã€‚</p>
<h3 id="3-SOP"><a href="#3-SOP" class="headerlink" title="3. SOP"></a>3. SOP</h3><p>æå‡ºäº†ä¸€ä¸ªæ–°çš„self supervised learning çš„ objectiveï¼Œæ—¢SOP(sentence ordering objectives)ã€‚ç±»ä¼¼äºBERTé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯è¿ç»­çš„ï¼ŒALBERTéœ€è¦é¢„æµ‹æ‰“ä¹±å¥å­çš„é¡ºåºã€‚</p>
<p><strong>å¹¶åœ¨åœ¨å¯¹æ¯”ä¸­ï¼ŒSOPå¯¹äºRACEä¹Ÿå°±æ˜¯é˜…è¯»ç†è§£ä»»åŠ¡æé«˜äº†2.3ä¸ªç‚¹ï¼Œå¾ˆå“‡å¡</strong></p>
<h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><p>å®éªŒéƒ¨åˆ†å…·ä½“æš‚ä¸”ä¸è¡¨ã€‚æˆ‘ç†è§£çš„æœ‰å‡ ç‚¹</p>
<ol>
<li>é¢å¤–çš„é¢†åŸŸå†…é¢„è®­ç»ƒæ˜¯æœ‰ç›Šçš„ï¼Œä½†æ˜¯é¢†åŸŸå¤–å¯èƒ½ä¼šæœ‰å®³</li>
<li>dropoutåœ¨æ¨¡å‹ä¸ä¼šover-fitçš„æƒ…å†µä¸‹å…¶å®å¯ä»¥å¿½ç•¥ï¼Œåœ¨batch normalizationå’Œdropoutå¯èƒ½ä¼šæŸå®³æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>hidden size 4096 æœ‰å¯èƒ½æ˜¯ALBERT æ€§èƒ½å¼ºçš„ä¸»è¦åŸå› ã€‚</li>
<li>è™½ç„¶å±‚é—´å‚æ•°å…±äº«ï¼Œç†è®ºä¸Šå¯ä»¥æ— é™æ·±ï¼Œä½†æ˜¯å®éªŒå‘ç°24å±‚å¹¶æ²¡æœ‰12å±‚æ•ˆæœå¥½ã€‚ç‰¹åˆ«å®½ä¹Ÿæ²¡æœ‰ç‰¹åˆ«å¥½ï¼Œè¿™éƒ½æ˜¯ç„å­¦è°ƒå‚ï¼Œå¾ˆéš¾äººå·¥åˆ¤æ–­ã€‚</li>
<li>æŒ‰ç†æ¥è¯´fffff(x) å¯èƒ½ä¼šå¯¼è‡´æ¯å±‚ä¹‹é—´çš„è¾“å‡ºè¿‡äºç›¸ä¼¼ï¼Œä½†åœ¨è¿™é‡Œå®éªŒå‘ç°ï¼Œå¹¶æ²¡æœ‰ã€‚éš¾é“æ˜¯embed layerå°±å¾ˆå¼ºäº†ï¼Ÿ <strong>çŒœæµ‹</strong></li>
</ol>
<h2 id="æ”¹è¿›ç‚¹"><a href="#æ”¹è¿›ç‚¹" class="headerlink" title="æ”¹è¿›ç‚¹"></a>æ”¹è¿›ç‚¹</h2><ol>
<li>ç¨€ç–çŸ©é˜µä¼˜åŒ–ï¼Œattention é­”æ”¹</li>
<li>SOPæ˜¯å¦å¯ä»¥æ³›ç”¨ã€‚</li>
</ol>
<hr>
<h2 id="æ¨¡å‹è§£è¯»"><a href="#æ¨¡å‹è§£è¯»" class="headerlink" title="æ¨¡å‹è§£è¯»"></a>æ¨¡å‹è§£è¯»</h2><h3 id="ä¸»ç±»"><a href="#ä¸»ç±»" class="headerlink" title="ä¸»ç±»"></a>ä¸»ç±»</h3><p>forwardå…ˆç»è¿‡<code>embeddings</code>å±‚å†ç»è¿‡<code>encoder</code>å±‚ã€‚è¿™é‡Œæ³¨æ„ï¼Œé»˜è®¤è¾“å…¥æ˜¯ç”¨äº†æœ€åä¸€ä¸ªéšå±‚æ‰€æœ‰tokençš„è¾“å‡ºå†ç»è¿‡ä¸€ä¸ªçº¿æ€§+<code>tanh</code>çš„æ“ä½œã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlbertModel</span>(<span class="params">AlbertPreTrainedModel</span>):</span></span><br><span class="line">    config_class = AlbertConfig</span><br><span class="line">    load_tf_weights = load_tf_weights_in_albert</span><br><span class="line">    base_model_prefix = <span class="string">&quot;albert&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.config = config</span><br><span class="line">        self.embeddings = AlbertEmbeddings(config)</span><br><span class="line">        self.encoder = AlbertTransformer(config)</span><br><span class="line">        <span class="keyword">if</span> add_pooling_layer:</span><br><span class="line">            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">            self.pooler_activation = nn.Tanh()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.pooler = <span class="literal">None</span></span><br><span class="line">            self.pooler_activation = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_resize_token_embeddings</span>(<span class="params">self, new_num_tokens</span>):</span></span><br><span class="line">        old_embeddings = self.embeddings.word_embeddings</span><br><span class="line">        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)</span><br><span class="line">        self.embeddings.word_embeddings = new_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prunes heads of the model.</span></span><br><span class="line"><span class="string">        heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125;</span></span><br><span class="line"><span class="string">        ALBERT has a different architecture in that its layers are shared across groups, which then has inner groups.</span></span><br><span class="line"><span class="string">        If an ALBERT model has 12 hidden layers and 2 hidden groups, with two inner groups, there</span></span><br><span class="line"><span class="string">        is a total of 4 different layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,</span></span><br><span class="line"><span class="string">        while [2,3] correspond to the two inner groups of the second hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Any layer with in index other than [0,1,2,3] will result in an error.</span></span><br><span class="line"><span class="string">        See base class PreTrainedModel for more information about head pruning</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            group_idx = <span class="built_in">int</span>(layer / self.config.inner_group_num)</span><br><span class="line">            inner_group_idx = <span class="built_in">int</span>(layer - group_idx * self.config.inner_group_num)</span><br><span class="line">            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING.format(&quot;batch_size, sequence_length&quot;))</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(</span></span><br><span class="line">        tokenizer_class=_TOKENIZER_FOR_DOC,</span><br><span class="line">        checkpoint=<span class="string">&quot;albert-base-v2&quot;</span>,</span><br><span class="line">        output_type=BaseModelOutputWithPooling,</span><br><span class="line">        config_class=_CONFIG_FOR_DOC,</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        extended_attention_mask = attention_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line">        extended_attention_mask = (<span class="number">1.0</span> - extended_attention_mask) * <span class="number">-10000.0</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, <span class="number">0</span>])) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPooling(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-12-25T06:17:24.000Z" title="2019-12-25T06:17:24.000Z">2019-12-25</time>å‘è¡¨</span><span class="level-item"><time dateTime="2019-12-25T06:57:28.797Z" title="2019-12-25T06:57:28.797Z">2019-12-25</time>æ›´æ–°</span><span class="level-item"><a class="link-muted" href="/categories/%E8%80%83%E7%A0%94/">è€ƒç ”</a></span><span class="level-item">9 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦1333ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%80%83%E7%A0%94/2019/12/25/%E8%80%83%E7%A0%94%E9%9A%8F%E6%83%B3.html">è€ƒç ”éšæƒ³</a></h1><div class="content"><h1 id="è€ƒç ”æ„Ÿæƒ³"><a href="#è€ƒç ”æ„Ÿæƒ³" class="headerlink" title="è€ƒç ”æ„Ÿæƒ³"></a>è€ƒç ”æ„Ÿæƒ³</h1><h2 id="æ„Ÿè§¦"><a href="#æ„Ÿè§¦" class="headerlink" title="æ„Ÿè§¦"></a>æ„Ÿè§¦</h2><p>ç°åœ¨æ˜¯12æœˆ25å·ï¼Œè€ƒå®Œç ”çš„ç¬¬ä¸‰å¤©ã€‚åœ¨ä»Šå¤©ä¹‹å‰çš„å‰ä¸€ä¸ªæ˜ŸæœŸé‡Œï¼Œæˆ‘æ¯å¤©éƒ½æ˜¯7ç‚¹å·¦å³è‡ªç„¶é†’ã€‚è€Œç»ˆäºåœ¨ä»Šå¤©ï¼Œæˆ‘ç¡åˆ°äº†ä¹ç‚¹ï¼Œä½†ä¾ç„¶æ²¡æœ‰ç¡é¥±ã€‚æˆ‘æƒ³è€ƒç ”å¸¦ç»™æˆ‘äº†å¾ˆå¤šï¼Œä¸åªæ˜¯é‚£æ‘èµ·æ¥åƒå±±ä¸€æ ·é«˜çš„è¾…å¯¼ä¹¦ï¼Œè¿˜æœ‰ä¸€äº›éšä¹‹å¸¦æ¥çš„ä¹ æƒ¯çš„æ”¹å˜ã€‚</p>
<h2 id="è€ƒç ”å¼€å§‹"><a href="#è€ƒç ”å¼€å§‹" class="headerlink" title="è€ƒç ”å¼€å§‹"></a>è€ƒç ”å¼€å§‹</h2><h3 id="é«˜è€ƒä¹‹å"><a href="#é«˜è€ƒä¹‹å" class="headerlink" title="é«˜è€ƒä¹‹å"></a>é«˜è€ƒä¹‹å</h3><p>æœ€å¼€å§‹å¬è¯´è€ƒç ”åº”è¯¥æ˜¯é«˜è€ƒå‡ºæˆç»©ä¹‹åå§ã€‚é‚£ä¸ªæ—¶å€™å› ä¸ºè€ƒçš„å¤ªå·®ï¼Œæ˜¯çœŸçš„å¯¹è‡ªå·±çš„äººç”Ÿå¤±å»äº†ä¿¡å¿ƒã€‚ä¹‹åå¬è¯´åˆ°äº†å¤§å­¦ä»¥åï¼Œè¿˜å¯ä»¥é ç€è€ƒç ”â€ç¿»èº«â€œï¼ˆå“ªæœ‰ä»€ä¹ˆç¿»èº«ï¼‰ã€‚ä¹‹åå°±ä¸‹å®šå†³å¿ƒï¼Œåœ¨å¤§å­¦æœŸé—´å†²å†²å†²ï¼Œä»¥è€ƒç ”è€ƒä¸Šæµ™å¤§ä¸ºç›®æ ‡ã€‚</p>
<h3 id="å¾—çŸ¥ä¸“ä¸šä¹‹å"><a href="#å¾—çŸ¥ä¸“ä¸šä¹‹å" class="headerlink" title="å¾—çŸ¥ä¸“ä¸šä¹‹å"></a>å¾—çŸ¥ä¸“ä¸šä¹‹å</h3><p>é«˜è€ƒåˆ†æ•°å‡ºæ¥ä¹‹åï¼Œè¿˜è¦è¿›è¡Œé€‰å­¦æ ¡å’Œé€‰ä¸“ä¸šï¼Œä½†æ˜¯å› ä¸ºåˆ†æ˜¯åœ¨æ˜¯å¤ªä½äº†ã€‚åªèƒ½éšä¾¿é€‰é€‰äº†ã€‚æœ€åæ¥åˆ°njtechçš„è¿™ä¸ªç¯å¢ƒç§‘å­¦ä¸“ä¸šã€‚æœ¬ç€å¹²ä¸€è¡Œçˆ±ä¸€è¡Œçš„å¿ƒæ€ï¼Œæˆ‘åœ¨ç½‘ä¸Šæœç´¢äº†å¾ˆå¤šå…³äºç¯å¢ƒç§‘å­¦çš„ä¿¡æ¯ã€‚ä½†æ˜¯è¶Šæœç´¢æˆ‘çš„å¿ƒæ˜¯è¶Šæ¥è¶Šæ‹”å‡‰æ‹”å‡‰çš„ã€‚ä½œä¸ºç¯åŒ–ç”Ÿæå››å¤§å‘ä¹‹é¦–ï¼Œç¯å¢ƒç»å¯¹ç®—å¾—ä¸Šæ˜¯ä»¥å…ˆè¾ˆä»¬çš„è¡€ä¸€èˆ¬åœ°äº‹å®å±•ç¤ºäº†ç¯å¢ƒä¸“ä¸šçš„ä¸å—å¾…è§ã€‚ä¹‹åï¼Œåœ¨çŸ¥ä¹çš„ç†æŸ“ä¹‹ä¸‹ï¼Œæˆ‘é€‰æ‹©äº†è½¬ä¸“ä¸šï¼</p>
<h3 id="å¤§ä¸€"><a href="#å¤§ä¸€" class="headerlink" title="å¤§ä¸€"></a>å¤§ä¸€</h3><p>å¤§ä¸€çš„è¿‡ç¨‹å¯ä»¥æ€»ç»“ä¸ºï¼šå®ˆæœ›å…ˆé”‹+åˆ·ç»©ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå•¥æ´»åŠ¨éƒ½æ²¡æœ‰å‚åŠ ã€‚</p>
<h3 id="è½¬ä¸“ä¸šä»¥å"><a href="#è½¬ä¸“ä¸šä»¥å" class="headerlink" title="è½¬ä¸“ä¸šä»¥å"></a>è½¬ä¸“ä¸šä»¥å</h3><p>ç»ˆäºåœ¨å¤§ä¸€ä¸‹çš„æ—¶å€™ï¼Œä»¥ç¯å¢ƒç§‘å­¦ä¸“ä¸šç¬¬ä¸€çš„æˆç»©è½¬å…¥äº†è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ã€‚å¹¶ä¸”åŠ å…¥äº†åœ¨å¤§ä¸€çš„æ—¶å€™å°±å¬è¯´è¿‡çš„acmæ¯”èµ›ã€‚ä½†æ˜¯æŠ•èº«äºacmæ¯”èµ›çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¹Ÿå¿˜è®°äº†æˆ‘è¿›å…¥å¤§å­¦çš„åˆå§‹åŠ¨æœºâ€”-è€ƒç ”æµ™å¤§ã€‚</p>
<h3 id="acmåŒºåŸŸèµ›3é“œ"><a href="#acmåŒºåŸŸèµ›3é“œ" class="headerlink" title="acmåŒºåŸŸèµ›3é“œ"></a>acmåŒºåŸŸèµ›3é“œ</h3><p>å¯èƒ½å› ä¸ºæš‘å‡çš„åŠå­¦åŠç©ï¼Œä¹Ÿå¯èƒ½æ˜¯å› ä¸ºè‡ªå·±çš„æ°´å“ä¸è¡Œæ™ºå•†ä¸å¤Ÿã€‚æˆ‘ä»¬é˜Ÿåœ¨2018å¹´çš„ä¸‹åŠå¹´ï¼Œå»äº†ä¸‰åœºåŒºåŸŸèµ›ï¼ˆåŒ…æ‹¬ä¸€åœºEC-Finalï¼‰å…¨éƒ¨éƒ½æ‹¿äº†é“œå¥–ã€‚è¿˜è®°å¾—ç¬¬ä¸€ä¸ªé“œçš„æ—¶å€™ï¼Œå¿ƒé‡Œæ˜¯ä½è½çš„ã€‚ä½†æ˜¯åœ¨å¬è¯´æœ‰å­¦é•¿ä¿ç ”å—å¤§ï¼Œæœ‰æ•°ç†å­¦é™¢çš„å­¦é•¿ä¿ç ”çš„æµ™å¤§è®¡ç®—æœºç›´åšï¼Œå¹¶ä¸”åœ¨ç½‘ä¸Šçœ‹äº†ä¸€ä¸‹ï¼Œä¿ç ”ä¸­æ‰“acmçš„è¿˜ç®—æ˜¯å°‘æ•°ï¼Œæˆ‘ä¿ç ”çš„å¿ƒæƒ…åˆé«˜æ¶¨äº†èµ·æ¥ã€‚</p>
<h3 id="2019å¹´4æœˆä»½"><a href="#2019å¹´4æœˆä»½" class="headerlink" title="2019å¹´4æœˆä»½"></a>2019å¹´4æœˆä»½</h3><p>4ï¼Œ5æœˆä»½æ˜¯acmé‚€è¯·èµ›å¼€å§‹çš„æ—¶å€™ï¼Œä½†ä¹Ÿæ˜¯ä¿ç ”ç”³è¯·å­¦æ ¡å¤ä»¤è¥çš„é«˜å³°æœŸï¼Œè¿™æ—¶å€™ä¸ç®¡æ˜¯å“ªé‡Œçš„å­¦ç”Ÿï¼Œéƒ½åœ¨ç–¯ç‹‚åœ°ç»™è€å¸ˆå‘é‚®ä»¶ï¼Œå’¨è¯¢ä¿ç ”çš„äº‹é¡¹ã€‚ä½†æ˜¯æˆ‘ç”±äºacmè¿˜è¦æ‰“ä¸ªé‚€è¯·èµ›ï¼Œç²¾åŠ›ç¡®å®ä¹Ÿåˆ†äº†ä¸€äº›ã€‚ç»“æœå°±ä¸Šäº†ä¸ªä¸Šç§‘å¤§å’Œæµ™è½¯çš„å¤ä»¤è¥ã€‚ï¼ˆå…¶å®è¿˜æ˜¯å› ä¸ºacmç‰Œå­ä¸å¤Ÿå“ï¼Œä¹Ÿæ²¡æœ‰å•¥å­ç§‘ç ”ç»å†ï¼‰</p>
<h3 id="æš‘å‡"><a href="#æš‘å‡" class="headerlink" title="æš‘å‡"></a>æš‘å‡</h3><p>æš‘å‡çš„æ—¶å€™ï¼Œå»äº†æµ™è½¯çš„å¤ä»¤è¥ï¼Œå¹¶ä¸”è€ƒç ”æ•°å­¦è¿‡äº†é«˜æ•°ï¼Œè‹±è¯­å•è¯ä¹ŸèƒŒäº†ä¸å°‘ã€‚ç»“æœå¤ä»¤è¥å½“åœºæ²¡ç»™æˆ‘ä¼˜ç§€è¥å‘˜ï¼Œä½†æ˜¯åœ¨åé¢çš„ä¼˜ç§€è¥å‘˜åå•ä¸­æ˜¯æœ‰æˆ‘çš„ã€‚ä¸è¿‡æˆ‘ä»¬å­¦é™¢ä¿ç ”è¦æ±‚çš„æ˜¯å¯¹æ–¹å­¦æ ¡çš„ç¡®ä¿æ¥å—è¯æ˜ï¼Œè¿™tmè°æœ‰å•Šã€‚åªèƒ½ç°æºœæºœåœ°å‡†å¤‡å»è€ƒç ”äº†ã€‚</p>
<h3 id="8æœˆ"><a href="#8æœˆ" class="headerlink" title="8æœˆ"></a>8æœˆ</h3><p>æ­¤æ—¶æ­£æ˜¯è€ƒç ”çš„å†²åˆºæœŸï¼Œä½†æ˜¯æˆ‘è€ƒæµ™å¤§è®¡ç§‘ï¼Œæ€•æ˜¯å»é€å‘½äº†ï¼Œï¼ˆæµ™å¤§è®¡ç§‘408è€ƒ4é—¨ä¸“ä¸šè¯¾ï¼Œå°±ç®—æ˜¯å†æ‰å®çš„åŸºç¡€ï¼Œä¹Ÿè¦ä¸ªé•¿æ—¶é—´çš„å‡†å¤‡ï¼Œè€Œä¸”æˆ‘çš„æ•°å­¦ä¸€ç›´å±äºåˆ·ä¸ªç»©ç‚¹çš„æ°´å“ï¼Œå¹¶ä¸èƒ½åº”ä»˜è€ƒç ”è¿™ä¸ªé«˜å¼ºåº¦è€ƒè¯•ã€‚ï¼‰æ‰€ä»¥æ ¹æ®æˆ‘çœ‹åˆ°çš„æµ™è½¯åˆä½œä¼ä¸šçš„é¡¹ç›®ï¼Œæˆ‘å†³å®štmè€ƒæµ™è½¯ï¼Œè½¯ä»¶å°±è½¯ä»¶ï¼Œèµ·ç ä¹Ÿæ˜¯ä¸ªæµ™å¤§ã€‚</p>
<h3 id="8-12æœˆ"><a href="#8-12æœˆ" class="headerlink" title="8-12æœˆ"></a>8-12æœˆ</h3><p>å†²å†²å†²ï¼ç–¯ç‹‚å¤ä¹ ï¼Œè™½ç„¶æœ‰æ—¶å€™ä¹Ÿæ¾æ‡ˆä¸ª1å¤©ï¼Œä½†æ˜¯è¿™å››ä¸ªæœˆæˆ‘çš„è€ƒç ”å¤ä¹ æ•ˆç‡è¿˜æ˜¯å¯ä»¥çš„ã€‚å…»æˆäº†ä¸€äº›å­¦ä¹ çš„ä¹ æƒ¯ï¼Œæ¯”å¦‚å†™æ„Ÿæƒ³ï¼Œæ‘˜å½•é”™é¢˜ã€‚</p>
<h2 id="è€ƒç ”ä¸­"><a href="#è€ƒç ”ä¸­" class="headerlink" title="è€ƒç ”ä¸­"></a>è€ƒç ”ä¸­</h2><p>è½¬çœ¼ä¹‹é—´å°±åˆ°äº†è€ƒç ”çš„æ—¶é—´äº†ï¼Œ12æœˆ21æ—¥-22æ—¥ã€‚</p>
<h3 id="é…’åº—ä¸­å‡Œä¹±"><a href="#é…’åº—ä¸­å‡Œä¹±" class="headerlink" title="é…’åº—ä¸­å‡Œä¹±"></a>é…’åº—ä¸­å‡Œä¹±</h3><p>è™½ç„¶æˆ‘çš„è€ƒç‚¹ç¦»å­¦æ ¡è›®è¿‘çš„ï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯é€‰æ‹©äº†è®¢äº†ä¸ªé…’åº—ï¼Œæ—¢å¯ä»¥ä¸­åˆçš„æ—¶å€™èººä¸€ä¼šå„¿ï¼Œä¹Ÿå¯ä»¥é˜²æ­¢æ„å¤–ã€‚æ‰€ä»¥æˆ‘åœ¨å‘¨å››çš„æ™šä¸Šï¼Œè¿˜å»è¯•ç¡äº†ä¸€ä¸‹ã€‚ç»“æœå°±tmå‡ºç°æ„å¤–äº†ã€‚</p>
<blockquote>
<p>è®ºé…’åº—éš”éŸ³ä¸å¥½+æ—è¾¹æ˜¯ä¸€å¯¹æ¿€æƒ…ä¼¼ç«çš„æƒ…ä¾£çš„ä½æˆ¿ä½“éªŒ</p>
</blockquote>
<p>æ‰€ä»¥æˆ‘æ¯…ç„¶å†³ç„¶åœ°é€‰æ‹©å‘¨äº”è¿™ä¸ªæœ€é‡è¦çš„ç¡è§‰æ—¶é—´åœ¨å®¿èˆé‡Œç¡ã€‚</p>
<h3 id="å‘¨äº”"><a href="#å‘¨äº”" class="headerlink" title="å‘¨äº”"></a>å‘¨äº”</h3><p>å®¿èˆä¸­è¾—è½¬éš¾çœ ï¼Œå› ä¸ºå‘¨å…­è€ƒçš„æ˜¯æ”¿æ²»å’Œè‹±è¯­ï¼Œæ‰€ä»¥æœ‰å¤§é‡çš„éœ€è¦èƒŒè¯µçš„å†…å®¹ã€‚æˆ‘é€‰æ‹©äº†åœ¨10.30ç¡è§‰ï¼Œä¹‹åæ—©ä¸Š6.20å·¦å³çˆ¬èµ·æ¥å†è¿‡ä¸€éã€‚</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-09-12T11:46:24.000Z" title="2019-09-12T11:46:24.000Z">2019-09-12</time>å‘è¡¨</span><span class="level-item"><time dateTime="2019-09-12T11:46:48.663Z" title="2019-09-12T11:46:48.663Z">2019-09-12</time>æ›´æ–°</span><span class="level-item"><a class="link-muted" href="/categories/%E8%80%83%E7%A0%94/">è€ƒç ”</a></span><span class="level-item">1 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦188ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%80%83%E7%A0%94/2019/09/12/9-9-9-15%E8%AE%A1%E5%88%92.html">9.9-9.15è®¡åˆ’</a></h1><div class="content"><h1 id="ä¸€å‘¨è®¡åˆ’-9-9-9-15"><a href="#ä¸€å‘¨è®¡åˆ’-9-9-9-15" class="headerlink" title="ä¸€å‘¨è®¡åˆ’ 9.9 - 9.15"></a>ä¸€å‘¨è®¡åˆ’ 9.9 - 9.15</h1><h2 id="æ•°å­¦"><a href="#æ•°å­¦" class="headerlink" title="æ•°å­¦"></a>æ•°å­¦</h2><ul>
<li><input disabled="" type="checkbox"> å®Œæˆçº¿æ€§ä»£æ•°éƒ¨åˆ†ï¼Œä¸€å¤©ä¸€ç« ææ°¸ä¹å¼ºåŒ–</li>
<li><input disabled="" type="checkbox"> æ¯å¤©å›é¡¾é”™é¢˜ä»¥åŠä¸€å…ƒå‡½æ•°ç§¯åˆ†çš„ä¹ é¢˜ 5é¢˜</li>
</ul>
<h2 id="è‹±è¯­"><a href="#è‹±è¯­" class="headerlink" title="è‹±è¯­"></a>è‹±è¯­</h2><ul>
<li><input disabled="" type="checkbox"> å•è¯è¾¾æ ‡ã€‚</li>
<li><input disabled="" type="checkbox"> ä¸‰å¤©é˜…è¯»æ¨¡æ‹Ÿ+å®Œå‹+æ–°é¢˜å‹ ç¬¬ä¸€å¤©é›†ä¸­åšï¼Œ ç¬¬äºŒç¬¬ä¸‰å¤©ç²¾è¯»</li>
<li><input disabled="" type="checkbox"> ä¸‰å¤©ä¸­æœ€åä¸€å¤©èƒŒä¸€ç¯‡èŒƒæ–‡ã€‚</li>
</ul>
<h2 id="æ”¿æ²»"><a href="#æ”¿æ²»" class="headerlink" title="æ”¿æ²»"></a>æ”¿æ²»</h2><p>ã€ŠçŸ¥è¯†ç‚¹ç²¾è®²ç²¾ç»ƒã€‹ã€Šè®²çœŸé¢˜ã€‹ã€Š1000é¢˜ã€‹</p>
<ul>
<li><input disabled="" type="checkbox"> ä¸€å¤©ä¸€ç« ï¼Œå¦‚æœå­¦ç´¯äº†çœ‹è§†é¢‘ï¼Œçœ‹å®Œä¸€ç« ç«‹åˆ»åšå®Œ1000é¢˜ä¸­å¯¹åº”å•å…ƒ</li>
<li><input disabled="" type="checkbox"> è¯¾ä½™æ—¶é—´çœ‹å¾æ¶›è§†é¢‘</li>
</ul>
<h2 id="ä¸“ä¸šè¯¾"><a href="#ä¸“ä¸šè¯¾" class="headerlink" title="ä¸“ä¸šè¯¾"></a>ä¸“ä¸šè¯¾</h2><ul>
<li><input disabled="" type="checkbox"> ä¸€å¤©åŠç«  æ•°æ®ç»“æ„é€‰æ‹©é¢˜ï¼Œäº‰å–è¿™å‘¨é€‰æ‹©é¢˜ç»“æŸï¼Œå¹¶ä¸”æ‘˜é”™é¢˜ã€‚</li>
<li><input disabled="" type="checkbox"> Cè¯­è¨€æ•™ææ— èŠçš„æ—¶å€™çœ‹ä¸€ä¸‹ã€‚</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">ä¸Šä¸€é¡µ</a></div><div class="pagination-next"><a href="/page/2/">ä¸‹ä¸€é¡µ</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/10/">10</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/1.gif" alt="Chea Sim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chea Sim</p><p class="is-size-6 is-block">Student from Z</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Ning Bo</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">æ–‡ç« </p><a href="/archives"><p class="title">98</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">åˆ†ç±»</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">æ ‡ç­¾</p><a href="/tags"><p class="title">81</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/CheaSim" target="_blank" rel="noopener">å…³æ³¨æˆ‘</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/CheaSim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/www.cheasim.com"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">é“¾æ¥</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">åˆ†ç±»</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1600/"><span class="level-start"><span class="level-item">1600</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/acm/"><span class="level-start"><span class="level-item">acm</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile" href="/categories/acm%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">acmæ¨¡æ¿</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="level-start"><span class="level-item">acè‡ªåŠ¨æœº</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cf/"><span class="level-start"><span class="level-item">cf</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/cf1500/"><span class="level-start"><span class="level-item">cf1500</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/dfs/"><span class="level-start"><span class="level-item">dfs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/dp/"><span class="level-start"><span class="level-item">dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hash/"><span class="level-start"><span class="level-item">hash</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/html/"><span class="level-start"><span class="level-item">html</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pat/"><span class="level-start"><span class="level-item">pat</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%B8%93%E9%A1%B9%E7%BB%83%E4%B9%A0/"><span class="level-start"><span class="level-item">ä¸“é¡¹ç»ƒä¹ </span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%BF%9D%E7%A0%94/"><span class="level-start"><span class="level-item">ä¿ç ”</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">å‰ç«¯</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">å­¦ä¹ ç¬”è®°</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">å­¦ä¹ è®¡åˆ’</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">å¼€å‘</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%90%AC%E7%A0%96/"><span class="level-start"><span class="level-item">æ¬ç –</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A6%82%E7%8E%87dp/"><span class="level-start"><span class="level-item">æ¦‚ç‡dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">æ¨¡æ¿</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="level-start"><span class="level-item">çº¿æ®µæ ‘</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">è€ƒç ”</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94%E8%B7%AF%E6%BC%AB%E6%BC%AB/"><span class="level-start"><span class="level-item">è€ƒç ”è·¯æ¼«æ¼«</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="level-start"><span class="level-item">è“æ¡¥æ¯</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E8%A7%A3/"><span class="level-start"><span class="level-item">è®ºè§£</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">æœ€æ–°æ–‡ç« </h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-14T06:15:14.000Z">2021-01-14</time></p><p class="title"><a href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">åŸºäºBERTçš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-12T13:55:19.000Z">2021-01-12</time></p><p class="title"><a href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLPåŸºç¡€</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-07T09:40:38.000Z">2021-01-07</time></p><p class="title"><a href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">å¦‚ä½•åœ¨colabç”¨pytorch_lightningç™½å«–TPU</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-11T09:07:57.000Z">2020-12-11</time></p><p class="title"><a href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizerå­¦ä¹ ç¬”è®°</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-07T11:33:57.000Z">2020-12-07</time></p><p class="title"><a href="/uncategorized/2020/12/07/pytorch-cheat-list.html">pytorch cheat_list</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">å½’æ¡£</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">ä¸€æœˆ 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">åäºŒæœˆ 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">åä¸€æœˆ 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">åäºŒæœˆ 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">ä¹æœˆ 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">å…«æœˆ 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">ä¸ƒæœˆ 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">äº”æœˆ 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">å››æœˆ 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">ä¸‰æœˆ 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">åä¸€æœˆ 2018</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">åæœˆ 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">ä¹æœˆ 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">å…«æœˆ 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">æ ‡ç­¾</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LCA/"><span class="tag">LCA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/acm/"><span class="tag">acm</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="tag">acè‡ªåŠ¨æœº</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ajax/"><span class="tag">ajax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bert/"><span class="tag">bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bilstm/"><span class="tag">bilstm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cf/"><span class="tag">cf</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/colab/"><span class="tag">colab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crf/"><span class="tag">crf</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dfs/"><span class="tag">dfs</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dgl/"><span class="tag">dgl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dp/"><span class="tag">dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dptree/"><span class="tag">dptree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/echarts/"><span class="tag">echarts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gnn/"><span class="tag">gnn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/greedy/"><span class="tag">greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hash/"><span class="tag">hash</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hdoj/"><span class="tag">hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/html/"><span class="tag">html</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/js/"><span class="tag">js</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lca/"><span class="tag">lca</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pat/"><span class="tag">pat</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/re/"><span class="tag">re</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/struts2/"><span class="tag">struts2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tokenizer/"><span class="tag">tokenizer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">ä¸“é¢˜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86/"><span class="tag">äºŒåˆ†</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E7%94%9F/"><span class="tag">äººç”Ÿ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%9D%E7%A0%94/"><span class="tag">ä¿ç ”</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AB%E6%95%B0%E7%A0%81/"><span class="tag">å…«æ•°ç </span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">å›¾è®º</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">å­—ç¬¦ä¸²</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2hash/"><span class="tag">å­—ç¬¦ä¸²hash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">å­¦ä¹ ç¬”è®°</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="tag">å­¦ä¹ è®¡åˆ’</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"><span class="tag">å°æŠ€å·§</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tag">å¼€å‘</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%BD%E5%8F%96/"><span class="tag">æŠ½å–</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">æ•°å­¦</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">æ•°æ®ç»“æ„</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/"><span class="tag">æœ€å¤§å­æ®µå’Œ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%84%E9%80%A0/"><span class="tag">æ„é€ </span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E5%BD%A2dp/"><span class="tag">æ ‘å½¢dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"><span class="tag">æ ‘çŠ¶æ•°ç»„</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8Chdoj/"><span class="tag">æ ‘çŠ¶æ•°ç»„ï¼Œhdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/"><span class="tag">æ ‘é“¾å‰–åˆ†</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87dp/"><span class="tag">æ¦‚ç‡dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%8B%9F/"><span class="tag">æ¨¡æ‹Ÿ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%9D%BF/"><span class="tag">æ¨¡æ¿</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%86%E6%90%9C/"><span class="tag">çˆ†æœ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%9B%E5%AE%A2%E7%BD%91/"><span class="tag">ç‰›å®¢ç½‘</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%8A%B6%E5%8E%8Bdp/"><span class="tag">çŠ¶å‹dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="tag">çº¿æ®µæ ‘</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"><span class="tag">ç»„åˆæ•°å­¦</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%86%E8%8A%82/"><span class="tag">ç»†èŠ‚</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%A9%E7%82%B9/"><span class="tag">ç¼©ç‚¹</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C/"><span class="tag">ç½‘ç»œ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%B5%9B/"><span class="tag">ç½‘ç»œèµ›</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">è€ƒç ”</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8C%83%E5%9B%B4/"><span class="tag">èŒƒå›´</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="tag">è“æ¡¥æ¯</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A1%A5%E9%A2%98/"><span class="tag">è¡¥é¢˜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%BE%8B/"><span class="tag">è§„å¾‹</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">è®¡åˆ’</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/"><span class="tag">è®¡ç®—å‡ ä½•</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E8%A7%A3/"><span class="tag">è®ºè§£</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%AA%E5%BF%83/"><span class="tag">è´ªå¿ƒ</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">é€’å½’</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">é…ç½®</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/"><span class="tag">é¢„å¤„ç†</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">è®¢é˜…æ›´æ–°</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="è®¢é˜…"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 CheaSim</span>Â Â Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>Â &amp;Â <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">å…±<span id="busuanzi_value_site_uv">0</span>ä¸ªè®¿å®¢</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="å›åˆ°é¡¶ç«¯" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "æ­¤ç½‘ç«™ä½¿ç”¨Cookieæ¥æ”¹å–„æ‚¨çš„ä½“éªŒã€‚",
          dismiss: "çŸ¥é“äº†ï¼",
          allow: "å…è®¸ä½¿ç”¨Cookie",
          deny: "æ‹’ç»",
          link: "äº†è§£æ›´å¤š",
          policy: "Cookieæ”¿ç­–",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆ..."></div><a class="searchbox-close" href="javascript:;">Ã—</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆ...","untitled":"(æ— æ ‡é¢˜)","posts":"æ–‡ç« ","pages":"é¡µé¢","categories":"åˆ†ç±»","tags":"æ ‡ç­¾"});
        });</script></body></html>