<!doctype html>
<html lang="zh"><canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/fireworks.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>CheaSim Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="cheasim&#039;s blog"><meta name="msapplication-TileImage" content="/img/1.gif"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="cheasim&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="ACM chicken"><meta property="og:type" content="blog"><meta property="og:title" content="CheaSim Blog"><meta property="og:url" content="https://www.cheasim.com/"><meta property="og:site_name" content="CheaSim Blog"><meta property="og:description" content="ACM chicken"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.cheasim.com/img/og_image.png"><meta property="article:author" content="CheaSim"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cheasim.com"},"headline":"CheaSim Blog","image":["https://www.cheasim.com/img/og_image.png"],"author":{"@type":"Person","name":"CheaSim"},"description":"ACM chicken"}</script><link rel="icon" href="/img/1.gif"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-R7VVCLV2ZB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-R7VVCLV2ZB');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-20T07:26:03.000Z" title="2021-01-20T07:26:03.000Z">2021-01-20</time>发表</span><span class="level-item"><time dateTime="2021-01-20T07:26:41.601Z" title="2021-01-20T07:26:41.601Z">2021-01-20</time>更新</span><span class="level-item">5 分钟读完 (大约819个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></h1><div class="content"><h1 id="UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-读书笔记"><a href="#UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-读书笔记" class="headerlink" title="UNIFIEDQA: Crossing Format Boundaries with a Single QA System 读书笔记"></a>UNIFIEDQA: Crossing Format Boundaries with a Single QA System 读书笔记</h1><p>针对不同形式的QA做了一个整合的工作。对于抽取式、多选式、对错形式的QA数据集，使用了一个单一的模型进行训练，并且测试。</p>
<p>使用的模型是T-5、BART。</p>
<h2 id="1-看过摘要之后，自己提出几个问题"><a href="#1-看过摘要之后，自己提出几个问题" class="headerlink" title="1. 看过摘要之后，自己提出几个问题"></a>1. 看过摘要之后，自己提出几个问题</h2><ol>
<li>这篇文章说跨越了多个QA数据集的鸿沟，怎么跨越的？</li>
<li>效果这么好，那原来的模型怎么不去试一试？是模型好，还是多数据集综合训练训得好？</li>
<li>GPT-3 可以做到这一点吗？</li>
</ol>
<h2 id="2-对应问题的解答；"><a href="#2-对应问题的解答；" class="headerlink" title="2. 对应问题的解答；"></a>2. 对应问题的解答；</h2><h3 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h3><p>对于各种数据集，把他们使用<code>text-to-text</code>的方法输入到模型中。由于是生成模型，所以<code>input</code>和<code>output</code>是一样的。将不同数据集的<code>question</code>，<code>article</code>，<code>option</code>这几项用几个特殊标识串起来，就ok了。 </p>
<h3 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h3><p>观察了一下文章中的数据集表格做得挺漂亮的。把数据集的基本信息还有比较关键的<code>best published</code>这个。</p>
<p>![image-20201113204535500](/Users/cheasim/Library/Application Support/typora-user-images/image-20201113204535500.png)</p>
<p>这篇文章把心路过程也写出来了，之前有一个试点实验(pilot study)。就是看在其他格式下数据集上的训练是否也对原始的有效果。这里可以看出确实有效果，在有些数据集上效果还挺明显的。![image-20201113205728295](/Users/cheasim/Library/Application Support/typora-user-images/image-20201113205728295.png)</p>
<h3 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h3><p>没钱，没资源玩GPT。</p>
<h2 id="3-用自己的话阐述文章的核心问题和思路；"><a href="#3-用自己的话阐述文章的核心问题和思路；" class="headerlink" title="3. 用自己的话阐述文章的核心问题和思路；"></a>3. 用自己的话阐述文章的核心问题和思路；</h2><p>核心问题：提出一个大一统的模型，可以处理所有形式的QA dataset。直觉上，所有的QA任务都是针对挖掘原文问题中的语义信息，并且与答案进行逻辑匹配，亦或者是类似于常识推理的在模型原有知识的基础上，对于问题的理解并且回答。</p>
<p>思路：首先判别模型是没法做到了，因为输出的格式不一样，抽取式阅读理解和多选阅读理解输出的东西不一样，虽然可以把多选转化为抽取。。 生成模型就是有着和人一样的无限潜能。之后的<code>pliot study</code>不错，就是简单地试一试<strong>额外</strong>训练其他形式的数据集，看看对原始数据集是否有增益。</p>
<h2 id="4-可能改进的地方；"><a href="#4-可能改进的地方；" class="headerlink" title="4.可能改进的地方；"></a>4.可能改进的地方；</h2><ol>
<li>攻击一下，是不是加个同领域预训练也能提高效果。里面试点实验都是准确率比较低的数据集上测试的。低的提高那肯定简单~</li>
<li>之前还看到有一篇将所有数据集转化为一种形式的，这两篇是否有共同之处。</li>
<li>全是实验，没有对理论的分析或者是啥的。。</li>
<li>输入格式是否还可以改进一下？</li>
</ol>
<h2 id="5-自己画一遍文章的流程图；"><a href="#5-自己画一遍文章的流程图；" class="headerlink" title="5.自己画一遍文章的流程图；"></a>5.自己画一遍文章的流程图；</h2><p>流程没有创新，就是<code>input</code>自己定义了格式。</p>
<h2 id="6-捋一遍算法流程"><a href="#6-捋一遍算法流程" class="headerlink" title="6.捋一遍算法流程."></a>6.捋一遍算法流程.</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-14T06:15:14.000Z" title="2021-01-14T06:15:14.000Z">2021-01-14</time>发表</span><span class="level-item"><time dateTime="2021-01-15T07:00:44.187Z" title="2021-01-15T07:00:44.187Z">2021-01-15</time>更新</span><span class="level-item">19 分钟读完 (大约2846个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></h1><div class="content"><h1 id="毕设复习"><a href="#毕设复习" class="headerlink" title="毕设复习"></a>毕设复习</h1><p>由于是基于知识图谱的领域内问答系统，所以分为两个步骤，不是end2end。</p>
<ol>
<li>命名实体识别</li>
<li>属性映射步骤</li>
</ol>
<p>实体识别是为了找到问题中的实体，属性映射是为了找到实体对应在知识库中的属性。输出的结果是一个规则构成的“「实体」的「属性」是「尾实体」”</p>
<p>命名实体识别是通过BERT+CRF。</p>
<p>属性映射分为两步</p>
<ol>
<li>通过规则，在知识库中找到实体的所有属性，之后和原句匹配，匹配成功作为属性输出</li>
<li>匹配不成果，将所有属性以“「问题」「属性」”计算分数，取匹配分数最高的作为答案输出。</li>
</ol>
<p>1.15 搞定 实体属性对齐方面知识点。 之后再搞定面试RNN + 面试5道题 + leetcode10道题。 链表 字符串着重。</p>
<h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul>
<li>实体链接没有实现，只是匹配+数据库查询。</li>
</ul>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>由于数据本身是一个问答数据，所以我们需要先对数据进行处理，生成三元组对。大概有600w个实体，训练的一个1poch需要5小时。问题数量25000。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;question id=1&gt;	《机械设计基础》这本书的作者是谁？</span><br><span class="line">&lt;triple id=1&gt;	机械设计基础 ||| 作者 ||| 杨可桢，程光蕴，李仲生</span><br><span class="line">&lt;answer id=1&gt;	杨可桢，程光蕴，李仲生</span><br><span class="line">==================================================</span><br><span class="line">&lt;question id=2&gt;	《高等数学》是哪个出版社出版的？</span><br><span class="line">&lt;triple id=2&gt;	高等数学 ||| 出版社 ||| 武汉大学出版社</span><br><span class="line">&lt;answer id=2&gt;	武汉大学出版社</span><br><span class="line">==================================================</span><br><span class="line">&lt;question id=3&gt;	《线性代数》这本书的出版时间是什么？</span><br><span class="line">&lt;triple id=3&gt;	线性代数 ||| 出版时间 ||| 2013-12-30</span><br><span class="line">&lt;answer id=3&gt;	2013-12-30</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure>

<p>我们通过问题</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>代码来自transformers==3.3.1</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>BERT模型的输入由三部分得到，token embdding, segment embedding, position embedding。</p>
<h4 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h4><p>对于所有文字来说，计算机都是无法理解的，需要转化为浮点向量或者整型向量。BERT采用的是WordPiece tokenization，是一种数据驱动的分词算法，他以char作为最小的粒度，不断地寻找出现最多以char为单位组成的token，之后将word进行分词，分为一个一个的token。比如ing这个会经常出现在英文当中，所以WordPiece 会吧”I am playing the computer games”分为”I am play ##ing the computer games”。为了解决OOV问题。词表中有30522个词。</p>
<p>在WordPiece 分词的基础上，之后会加入4个特殊词汇[CLS],[SEP],[PAD],[UNK]。[CLS]加入到句首，不参与预训练，针对下游任务进行fine-tune。[SEP]作为句尾以及分段标志。[PAD]是填充，使得句子长度一样，方便批处理，[UNK]是表明不在词表当中。将他们转换成one-hot embedding之后，接一个embedding层，将词转化为最初的词向量。</p>
<h4 id="segment-embedding"><a href="#segment-embedding" class="headerlink" title="segment embedding"></a>segment embedding</h4><p>segment embedding 仅仅作为区分两个句子来使用。在预训练中，还要使用预测句子是否相邻作为预训练任务之一。在输入时，也会经过一个线性层来形成segment embedding.</p>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>position embedding 纯使用nn.embedding 获得。训练了一个位置<em>词表</em>。$W\in \mathbb{R^{512 \times 768}}$。输入就是[0,1,2,…,len_seq-1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, :seq_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h3 id="transformer-encoder-层"><a href="#transformer-encoder-层" class="headerlink" title="transformer encoder 层"></a>transformer encoder 层</h3><p>transformer encoder层主要如下图所示</p>
<p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png" alt="image"></p>
<p>使用一个上面所说的输入，经过<code>Multi-Head Attention</code>，在通过残差连接以及<code>Layer Normalization</code>，之后通过<code>FFN</code>以及又一个残差连接作为输出。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>BERT使用的是自注意力机制，即用于$Q,K,V$全部源自同一个向量。注意力机制使用了<strong>上下文</strong>的信息来对每一个token进行表示。计算机通过利用上下文的信息，对每一个token进行理解。比如“冬天到了，天气变冷了。”BERT会根据大量该类的文本，将冬天和冷的语义进行融合。Attention值的计算公式如下</p>
<p>$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_k}})V \in \mathbb{R^{len\times d}}$$</p>
<p>其中，$Q,K,V\in \mathbb{R^{d\times k}},Q=xW_q,K=xW_k,V=xW_v$，$d$是隐层维度这里可以注意到，因为softmax是非线性的，所以这里的矩阵变换是没法单纯使用线性变换用$Wx$代替的。</p>
<hr>
<p><strong>多头</strong>在哪里<strong>多头</strong>。直接将原来的$x$进行转化后切分，详情看下图就懂了。每一个attention生成的维度都不高，拼起来就跟原来一样了。这张图有点问题，其实BERT没有$W^O$，因为$Z_0,…,Z_7$拼起来正好是$Z$。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg" alt="img1"></p>
<blockquote>
<p>BERT 实现中，多头的目的是降低参数的个数，增加表达能力。 类似于CNN多个卷积核？</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function) [1,1,1,0,0] -&gt; [0,0,0,-10000,-10000]</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>ps: 代码里实现attention_mask使用了加法，因为softmax中针对e^0也会输出1，所以我们要对于忽视的token进行$e^{-inf}$，才能使他在softmax之后的权重为0。</p>
<h3 id="add-and-norm"><a href="#add-and-norm" class="headerlink" title="add and norm"></a>add and norm</h3><p>常见的残差网络方式梯度消失，增加模型的训练，打破了网络的对称性，提升了网络的表征能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="FFN-and-Add-Norm"><a href="#FFN-and-Add-Norm" class="headerlink" title="FFN and Add Norm"></a>FFN and Add Norm</h3><p>先经过中间层3072，hidden_size扩大4倍。之后再经过一个缩小了。注意这里最后才有一个dropout。激活函数用的gelu，比relu稍微缓和了一些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="最后处理"><a href="#最后处理" class="headerlink" title="最后处理"></a>最后处理</h3><p>综上，把这个bert_layer叠个12层就行了。但是在最后输出的时候[CLS]有一个特殊的处理。输出的时候会经过一个线性层+一个<code>tanh</code>激活。</p>
<h3 id="NER-token-classification"><a href="#NER-token-classification" class="headerlink" title="NER  token classification"></a>NER  token classification</h3><p>在每一个token对应的输出加入一个线性分类层，对应所有的实体类型标签比如B-PER,I-PER。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109250703">https://zhuanlan.zhihu.com/p/109250703</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
</blockquote>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><p>CRF Conditional Random Field条件随机场。是一种无向图模型，在给定需要标记的观测序列的条件下，计算整一个序列的联合概率。</p>
<p>$\Theta(x_1,…,x_m,s_1,…,s_m) \in \mathbb{R^d}$</p>
<p>由于BERT模型只会针对每一个token而不是一个实体输出标签概率，而较少地考虑到token标签之间的关系。所以我们需要增强模型对于相邻token标签之间关系的理解。比如”杭州是浙江省的省会城市”，“杭”和”州”可能都被识别为地名，但是这里应该识别“杭州”一个整体的地名，就不应该是“杭”-B-LOC，“州”-B-LOC而应该是“杭”-B-LOC，“州”-I-LOC。</p>
<p>CRF的损失函数为$l(\theta)=\cfrac{P_{RealPath}}{P_1+P_2+…+P_N}$</p>
<p>条件随机场为$P(y|x)=\exp[\sum^{SeqLen}_{k=1}\lambda_k \sum^{Cond}_{i=2}t_k(y_{i-1},y_i,x,i)+\sum_l \mu_l \sum_i s_l(y_i,x,i)]$</p>
<p>$t_k$是转移特征函数，$s_l$是状态特征函数。</p>
<p>由于BERT已经产生了状态特征函数，即每一个token的标签概率值，CRF只需要去求转移特征函数，即一个长度为标签个数的转移矩阵即可。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>CRF相对于HMM使用了上下文的信息，不单单只依据前一个的状态来预测后一个的状态。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94457579">https://zhuanlan.zhihu.com/p/94457579</a></p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-12T13:55:19.000Z" title="2021-01-12T13:55:19.000Z">2021-01-12</time>发表</span><span class="level-item"><time dateTime="2021-01-17T05:53:46.298Z" title="2021-01-17T05:53:46.298Z">2021-01-17</time>更新</span><span class="level-item">30 分钟读完 (大约4480个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></h1><div class="content"><h1 id="nlp-深度学习基础"><a href="#nlp-深度学习基础" class="headerlink" title="nlp 深度学习基础"></a>nlp 深度学习基础</h1><p>将每一个模型以</p>
<ol>
<li>简单介绍</li>
<li>解决的问题</li>
<li>代码</li>
<li>优缺点</li>
<li>使用tips</li>
</ol>
<p>来归类。比较现代的会分析分析。</p>
<h2 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h2><p>全称(term frequency-inverse document frequency)，TF指的是词频，IDF指的是逆文本频率指数。他们的计算公式如下：</p>
<p>$TF_w=\cfrac{N_w}{N}$，一个词在该句子中出现的频率。</p>
<p>$IDF_w=\log{\cfrac{Y}{Y_w+1}}$，$Y_w$是所有文档中包含该词的文档个数。+1是方式分母为0.</p>
<p>TF-IDF的<strong>思想</strong>在于，一个词如果在一段小文本中出现得越多，那么他对这段文本的权重就越大，但是如果在所有的文本中，他出现的次数都很多，就像计算信息熵一样，在所有情况下出现的概率很大时，那么词的信息就很少。所有使用IDF来抵消一些常用词的影响。综上，计算公式为</p>
<p>$TFIDF_w=TF_w \times IDF_w$</p>
<blockquote>
<p>注意，对于不同的样本的同一个词，$TF$可能是不同的，但是$IDF$是相同的。</p>
</blockquote>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>TF-IDF相当于在以前把关键字作为短文本表示的基础上加入了一个正则化，削弱了高频词的权重。在一些简单的文本匹配(对于给定的问题，与已知文本的词语将TFIDF加和得到相似度)，文本分类上可以起到一定的效果。</p>
<hr>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFIDF</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    self.documents_list = documents_list</span><br><span class="line">    self.tf = []</span><br><span class="line">    self.idf = &#123;&#125;</span><br><span class="line">    df = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> documents_list:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> word <span class="keyword">in</span> document:</span><br><span class="line">        temp[word] = temp.get(word, <span class="number">0</span>) + <span class="number">1.</span>/<span class="built_in">len</span>(document)</span><br><span class="line">     	self.tf.append(temp)</span><br><span class="line">      <span class="comment"># 出现过的词，都+1</span></span><br><span class="line">   		<span class="keyword">for</span> k <span class="keyword">in</span> temp.keys():</span><br><span class="line">        df[k] = df.get(k, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> df.items():</span><br><span class="line">      self.idf[k] = np.log(<span class="built_in">len</span>(documents_list) / (v + <span class="number">1</span>))</span><br><span class="line">    self.tfidf = []</span><br><span class="line">		<span class="keyword">for</span> tf_sentence <span class="keyword">in</span> self.tf:</span><br><span class="line">      temp = &#123;&#125;</span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> tf_sentence.items():</span><br><span class="line">        temp[k] = v * self.idf[k]</span><br><span class="line">      self.tfidf.append(temp)</span><br><span class="line">tfidf = TFIDF([<span class="string">&#x27;I have a pen&#x27;</span>.split(), <span class="string">&#x27;I have an apple&#x27;</span>.split(), <span class="string">&#x27;Bang, apple pen&#x27;</span>.split()])</span><br><span class="line">print(tfidf.tf)</span><br><span class="line">print(tfidf.idf)</span><br><span class="line">print(tfidf.tfidf)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>一种无监督的生成句子词语向量的方法。</li>
<li>可以很快地找到一句话的关键字</li>
<li>耗费的计算资源较少。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>句子向量，或者词语向量没有上下文信息，词语与词语之间的位置关系也没有融入到表示当中。</li>
<li>会将生僻词作为关键词，但其实生僻词意义不大。</li>
<li>人名地名比较难以区分。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113017752">https://zhuanlan.zhihu.com/p/113017752</a></p>
</blockquote>
<p>ps: python dict是真好用。</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>word2vec说明白了也就是和TFIDF一样的<strong>将词语使用一个$f(x)$映射到数值的向量空间当中</strong>。由于词语不像是像素点有着天然的数值表示，word2vec针对词语转化为计算机可以理解的表示。</p>
<p>word2vec的<strong>思想</strong>是一个词的意思由它旁边的词构成。就像老话说的好，物以类聚，人以群分。但其实word2vec的损失函数和模型解决的问题是有一点割裂开的。word2vec想解决的问题是生成稠密的word embedding，而优化损失函数的目的是让两个词在window下的关系符合文本。损失函数是想，在一个窗口下，如果我中心词是”帅哥”,那么模型应该能推测前两个词是”我”和”是”。因为在样本中”我是帅哥”出现过很多次。之后由于损失函数的计算中，预测的条件概率是通过词向量的相似度来计算的，所以在优化模型中，也就达成了相似词汇产生相似词向量的目的。</p>
<p>word2vec又分为两种模型：</p>
<ol>
<li>skip-gram： 使用中心词周围的词来预测中心词。</li>
<li>CBOW： 使用中心词来预测周围的词。</li>
</ol>
<p>每一个词汇表示成为两个$d$维的向量，用来计算条件概率。$v_i \in \mathbb{R^d}$.之后每一个window下在中心词预测上下文的条件概率就是。</p>
<p>$$P(O=o|C=c)=\cfrac{\exp(u_o^Tv_c)}{\sum_{w \in Vocab} \exp(u_w^Tv_c)}$$</p>
<p>使用极大似然估计就是，$L(\theta)=\prod_{t=1}^T \prod_{-m \leq j \leq m ,j \neq 0} P(w_{t+j} | w_t; \theta)$</p>
<p>之后对极大似然估计常规操作，取log再正负相反，从而作为损失函数求最小。</p>
<p>![image-20210113160811012](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113160811012.png)</p>
<p>$W_{V\times N}$就是由中心词汇组成的矩阵，$W’_{N \times V}$就是上下文词汇表示组成的矩阵。</p>
<h3 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip gram"></a>skip gram</h3><p>每一个词汇表示成为两个$d$维的向量，用来计算条件概率。$v_i \in \mathbb{R^d}$.这里我们直接想象成深度学习的模型，那么我们的输入是一个one hot embedding，输出是维度为词表的向量。之后我们要使得向量在上下文词上的值接近为1(在激活归一化之后)。由于词表一般很长，所以训练skip gram的时候有一个trick。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>使用平均加权的one hot embedding输入上下文词汇，去预测中心词汇。</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>对于整个词表计算一次softmax的开销是很大的$O(|V|)$其中$|V|&gt;&gt;|d|$，所以我们需要构建一种不一样的softmax来处理这个问题。这个就是Hierarchical softmax 等级制的softmax，它首先会将词表进行分层，构建成一颗平衡二叉树，🌲上的节点就是我们要判断的word，我们知道平衡二叉树的深度是$O(\log n)$。经过$\log n$次的判断之后，就可以计算损失函数了。一次forward复杂度是$O(\log |V| * d<em>2)$，其中2是在左右选择，相比于$O(|V|+d</em>|V|)$减少了很多。计算P需要把从根节点到叶子节点上的每个节点挨个算一遍概率。</p>
<p>![image-20210113164654789](/Users/cheasim/Library/Application Support/typora-user-images/image-20210113164654789.png)</p>
<h4 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h4><p>在一次训练的时候，skip gram 只会输入一个词，很稀疏，浪费了其他的embedding训练。在训练的时候，不使用矩阵直接乘，而是使用挑选比如(1+10)10个负样本更新矩阵。挑选的公式为出现评率比较大的。</p>
<p>$$P(w_i)=\cfrac{f(w_i)^{0.75}}{\sum_{j=0}^nf(w_j)^{0.75}}$$</p>
<h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>解决one-hot embedding中过于稀疏，以及难以表达语义特征的问题。</p>
<hr>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2Vec</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, documents_list</span>):</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>



<hr>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li><p>无监督训练生成词向量</p>
</li>
<li><p>对于相似的词汇有着很好的解释性，Man - King = Woman - Queen</p>
</li>
</ul>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><p>无法一词多义。</p>
</li>
<li><p>训练时没有加入位置信息，训练效率较低。</p>
</li>
</ul>
<h2 id="text-cnn"><a href="#text-cnn" class="headerlink" title="text-cnn"></a>text-cnn</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">Convolutional Naural Networks for Sentence Classification</a></p>
<p>拷贝忍者卡卡sei，直接抄CV的CNN就完事了。我们可以将一句话利用word embedding看成是一副图像，比如长度为10的句子，词向量维度为300。那么这个句子的输入就是$10 \times 300$的矩阵。之后我们就可以像图像一样处理文本了。</p>
<p><img src="https://pic2.zhimg.com/80/v2-38e6e46009ea88c06465ed0770051c4d_720w.jpg" alt="image-20210113173917546"></p>
<p>模型可以分为三层。</p>
<ol>
<li>输入层是一个$k \times n$的矩阵</li>
<li>卷积层与CV有一些区别，因为我们需要把词向量看做是一个整体，所以不会在横（纵？）方向上进行卷积，卷积窗口只会上下移动。核大小为$filter_size \times embedding_size$.文中定义<code>filiter_size</code>为[3,4,5]。将局部的信息聚合。每一个不同的卷积核都会生成不同的<code>feature map</code>，比如输入是$10 \times 300$，之后经过128个大小为3的卷积操作，会生成128个维度为10的向量。</li>
<li>pooling层，由于要处理变长文本，所以是对每一个feature map上取最大值作为输出，所以最终得到的是一个128维度的向量。</li>
<li>FFN和Softmax 常规操作，分类模型获得每一类的概率。</li>
</ol>
<h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>将CNN引入到NLP当中，从而减少了模型的参数，并在CNN在捕捉局部信息时有奇效。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul>
<li>跨时代地提出了CNN在NLP领域的应用</li>
<li>实验做得很详细，针对预训练，随机生成的词向量都进行了比对。（是不是这个给bert一点思考，不需要一个word embedding，随机初始化就好了）</li>
</ul>
<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>CNN卷积对于句子来说还是太小了。没有全局信息。一个CNN只能估计5-gram的信息</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102426363">https://zhuanlan.zhihu.com/p/102426363</a></p>
</blockquote>
<h2 id="rnn-lstm"><a href="#rnn-lstm" class="headerlink" title="rnn lstm"></a>rnn lstm</h2><p>RNN想对于CNN模型来说多了很多变种。首先来说一下RNN的<strong>思想</strong>吧。RNN灵感来源于人类进行阅读过程中，会从左到右一个字一个字地读入文字，之后再得到自己的理解。那么是否有模型能够捕捉这种从左到右的时序信息呢？那就是RNN(Recurrent Neural Network)。RNN由于结构精巧有很多变种。</p>
<p><img src="https://github.com/YZHANG1270/Markdown_pic/blob/master/2018/11/RNN_01/001.png?raw=true" alt="rnn"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM使用门机制来进行有效地方式了梯度爆炸或者梯度消失。他三个门的公式分别是输入门，遗忘门，输出门。</p>
<p>$f_i=$</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/">http://codewithzhangyi.com/2018/10/31/NLP%E7%AC%94%E8%AE%B0-RNN/</a></p>
</blockquote>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>代码来自transformers==3.3.1</p>
<h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><p>BERT模型的输入由三部分得到，token embdding, segment embedding, position embedding。</p>
<h4 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h4><p>对于所有文字来说，计算机都是无法理解的，需要转化为浮点向量或者整型向量。BERT采用的是WordPiece tokenization，是一种数据驱动的分词算法，他以char作为最小的粒度，不断地寻找出现最多以char为单位组成的token，之后将word进行分词，分为一个一个的token。比如ing这个会经常出现在英文当中，所以WordPiece 会吧”I am playing the computer games”分为”I am play ##ing the computer games”。为了解决OOV问题。词表中有30522个词。</p>
<p>在WordPiece 分词的基础上，之后会加入4个特殊词汇[CLS],[SEP],[PAD],[UNK]。[CLS]加入到句首，不参与预训练，针对下游任务进行fine-tune。[SEP]作为句尾以及分段标志。[PAD]是填充，使得句子长度一样，方便批处理，[UNK]是表明不在词表当中。将他们转换成one-hot embedding之后，接一个embedding层，将词转化为最初的词向量。</p>
<h4 id="segment-embedding"><a href="#segment-embedding" class="headerlink" title="segment embedding"></a>segment embedding</h4><p>segment embedding 仅仅作为区分两个句子来使用。在预训练中，还要使用预测句子是否相邻作为预训练任务之一。在输入时，也会经过一个线性层来形成segment embedding.</p>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>position embedding 纯使用nn.embedding 获得。训练了一个位置<em>词表</em>。$W\in \mathbb{R^{512 \times 768}}$。输入就是[0,1,2,…,len_seq-1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, <span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, :seq_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h3 id="transformer-encoder-层"><a href="#transformer-encoder-层" class="headerlink" title="transformer encoder 层"></a>transformer encoder 层</h3><p>transformer encoder层主要如下图所示</p>
<p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190213113502155-341362210.png" alt="image"></p>
<p>使用一个上面所说的输入，经过<code>Multi-Head Attention</code>，在通过残差连接以及<code>Layer Normalization</code>，之后通过<code>FFN</code>以及又一个残差连接作为输出。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>BERT使用的是自注意力机制，即用于$Q,K,V$全部源自同一个向量。注意力机制使用了<strong>上下文</strong>的信息来对每一个token进行表示。计算机通过利用上下文的信息，对每一个token进行理解。比如“冬天到了，天气变冷了。”BERT会根据大量该类的文本，将冬天和冷的语义进行融合。Attention值的计算公式如下</p>
<p>$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_k}})V \in \mathbb{R^{len\times d}}$$</p>
<p>其中，$Q,K,V\in \mathbb{R^{d\times k}},Q=xW_q,K=xW_k,V=xW_v$，$d$是隐层维度这里可以注意到，因为softmax是非线性的，所以这里的矩阵变换是没法单纯使用线性变换用$Wx$代替的。</p>
<hr>
<p><strong>多头</strong>在哪里<strong>多头</strong>。直接将原来的$x$进行转化后切分，详情看下图就懂了。每一个attention生成的维度都不高，拼起来就跟原来一样了。这张图有点问题，其实BERT没有$W^O$，因为$Z_0,…,Z_7$拼起来正好是$Z$。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_720w.jpg" alt="img1"></p>
<blockquote>
<p>BERT 实现中，多头的目的是降低参数的个数，增加表达能力。 类似于CNN多个卷积核？</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">&quot;heads (%d)&quot;</span> % (config.hidden_size, config.num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mixed_key_layer = self.key(encoder_hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(encoder_hidden_states)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mixed_key_layer = self.key(hidden_states)</span><br><span class="line">            mixed_value_layer = self.value(hidden_states)</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line">        key_layer = self.transpose_for_scores(mixed_key_layer)</span><br><span class="line">        value_layer = self.transpose_for_scores(mixed_value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function) [1,1,1,0,0] -&gt; [0,0,0,-10000,-10000]</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=<span class="number">-1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (self.all_head_size,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>ps: 代码里实现attention_mask使用了加法，因为softmax中针对e^0也会输出1，所以我们要对于忽视的token进行$e^{-inf}$，才能使他在softmax之后的权重为0。</p>
<h3 id="add-and-norm"><a href="#add-and-norm" class="headerlink" title="add and norm"></a>add and norm</h3><p>常见的残差网络方式梯度消失，增加模型的训练，打破了网络的对称性，提升了网络的表征能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="FFN-and-Add-Norm"><a href="#FFN-and-Add-Norm" class="headerlink" title="FFN and Add Norm"></a>FFN and Add Norm</h3><p>先经过中间层3072，hidden_size扩大4倍。之后再经过一个缩小了。注意这里最后才有一个dropout。激活函数用的gelu，比relu稍微缓和了一些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">      </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<h3 id="最后处理"><a href="#最后处理" class="headerlink" title="最后处理"></a>最后处理</h3><p>综上，把这个bert_layer叠个12层就行了。但是在最后输出的时候[CLS]有一个特殊的处理。输出的时候会经过一个线性层+一个<code>tanh</code>激活。</p>
<h3 id="NER-token-classification"><a href="#NER-token-classification" class="headerlink" title="NER  token classification"></a>NER  token classification</h3><p>在每一个token对应的输出加入一个线性分类层，对应所有的实体类型标签比如B-PER,I-PER。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109250703">https://zhuanlan.zhihu.com/p/109250703</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-01-07T09:40:38.000Z" title="2021-01-07T09:40:38.000Z">2021-01-07</time>发表</span><span class="level-item"><time dateTime="2021-01-08T10:29:15.321Z" title="2021-01-08T10:29:15.321Z">2021-01-08</time>更新</span><span class="level-item">1 分钟读完 (大约168个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></h1><div class="content"><h1 id="如何在colab用pytorch-lightning白嫖TPU"><a href="#如何在colab用pytorch-lightning白嫖TPU" class="headerlink" title="如何在colab用pytorch_lightning白嫖TPU"></a>如何在colab用pytorch_lightning白嫖TPU</h1><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>我们得熟悉<code>pytorch_lightning</code>以及<code>colab</code>的操作。在使用TPU之前，先挂载好我们的云端磁盘以及配置好pytorch需要的TPU环境。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/gdrive&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&quot;gdrive/MyDrive/path_to_your_file&quot;</span>)</span><br><span class="line">! pip install pytorch_lightning</span><br><span class="line"></span><br><span class="line"><span class="comment"># 试图使用TPU</span></span><br><span class="line">!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py</span><br><span class="line">!python pytorch-xla-env-setup.py  --apt-packages libomp5 libopenblas-dev</span><br></pre></td></tr></table></figure>

<p>安装好<code>pytorch_lightning</code>以及配置好环境之后，我们就可以开始在<code>pytorch_lightning</code>框架下跟写GPU代码一样写TPU代码。下面请看<del>VCR</del>，一个小栗子。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-11T09:07:57.000Z" title="2020-12-11T09:07:57.000Z">2020-12-11</time>发表</span><span class="level-item"><time dateTime="2020-12-11T13:40:29.706Z" title="2020-12-11T13:40:29.706Z">2020-12-11</time>更新</span><span class="level-item">2 分钟读完 (大约237个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></h1><div class="content"><h1 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h1><blockquote>
<p>Transformers==4.0.0</p>
<p>由于每次调用bert等模型都需要使用模型的tokenizer，所以写个笔记，方便自己以及后人查阅，（其实看官方文档也可以，但是英文的看着头疼。有错误也请留言吧。</p>
</blockquote>
<h2 id="base-PreTrainedTokenizerBase"><a href="#base-PreTrainedTokenizerBase" class="headerlink" title="base : PreTrainedTokenizerBase"></a>base : <code>PreTrainedTokenizerBase</code></h2><p>作为基类，该类有着所有tokenizer都具有的方法还有属性。</p>
<h3 id="PreTrainedTokenizer"><a href="#PreTrainedTokenizer" class="headerlink" title="PreTrainedTokenizer"></a>PreTrainedTokenizer</h3><h2 id="使用multiprocess-加速tokenize"><a href="#使用multiprocess-加速tokenize" class="headerlink" title="使用multiprocess 加速tokenize"></a>使用multiprocess 加速tokenize</h2><p>仿照这个就完事了。<code>partial</code>可以固定函数中的参数，简直是专门为多进程准备的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squad_convert_example_to_features_init</span>(<span class="params">tokenizer_for_convert: PreTrainedTokenizerBase</span>):</span></span><br><span class="line">    <span class="keyword">global</span> tokenizer</span><br><span class="line">    tokenizer = tokenizer_for_convert</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = []</span><br><span class="line"></span><br><span class="line">threads = <span class="built_in">min</span>(threads, cpu_count())</span><br><span class="line"><span class="keyword">with</span> Pool(threads, initializer=squad_convert_example_to_features_init, initargs=				(tokenizer,)) <span class="keyword">as</span> p:</span><br><span class="line">  annotate_ = partial(</span><br><span class="line">    tokenzier.encode_plus, <span class="comment"># batch_encode_plus 不知道为什么不work</span></span><br><span class="line">    max_seq_length=max_seq_length,</span><br><span class="line">    doc_stride=doc_stride,</span><br><span class="line">    max_query_length=max_query_length,</span><br><span class="line">    padding_strategy=padding_strategy,</span><br><span class="line">    is_training=is_training,</span><br><span class="line">  )</span><br><span class="line">  features = <span class="built_in">list</span>(</span><br><span class="line">    tqdm(</span><br><span class="line">      p.imap(annotate_, examples, chunksize=<span class="number">32</span>),</span><br><span class="line">      total=<span class="built_in">len</span>(examples),</span><br><span class="line">      desc=<span class="string">&quot;convert squad examples to features&quot;</span>,</span><br><span class="line">      disable=<span class="keyword">not</span> tqdm_enabled,</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-07T11:33:57.000Z" title="2020-12-07T11:33:57.000Z">2020-12-07</time>发表</span><span class="level-item"><time dateTime="2021-01-06T07:45:59.695Z" title="2021-01-06T07:45:59.695Z">2021-01-06</time>更新</span><span class="level-item">4 分钟读完 (大约637个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/07/pytorch-cheat-list.html">pytorch cheat_list</a></h1><div class="content"><h1 id="pytorch-操作小计"><a href="#pytorch-操作小计" class="headerlink" title="pytorch 操作小计"></a>pytorch 操作小计</h1><blockquote>
<p>torch==1.7.0</p>
</blockquote>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><h3 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h3><p>将List[tensor]变成tensor。<code>torch.stack(tensors,dim=0,out=None)</code>Concatenates a sequence of tensors along a new dimension.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">a = torch.stack([b, b], dim = <span class="number">0</span>) <span class="comment"># a.shape = (2,4)</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-gather"><a href="#torch-gather" class="headerlink" title="torch.gather"></a>torch.gather</h3><blockquote>
<p>torch.gather(<em>input</em>, <em>dim</em>, <em>index</em>, *, <em>sparse_grad=False</em>, <em>out=None</em>) → Tensor</p>
</blockquote>
<p>英文解释为Gathers values along an axis specified by dim.</p>
<p>看到比较有道理的应用场景是，在变长序列中gather到最后一个或者说倒数第几个元素。一般变长序列为<code>inputs = [[1,2,3,0,0], [2,3,4,5,0]]</code>。这时候想获得最后一个元素就可以。需要注意的点是，输出的tensor和index是相同<code>shape</code>的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">         	[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]])</span><br><span class="line">index = torch.tensor([[<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.long)</span><br><span class="line">last_inputs = torch.gather(inputs, dim=<span class="number">1</span>, index)</span><br><span class="line"><span class="string">&quot;&quot;&quot;tensor([[3],</span></span><br><span class="line"><span class="string">           [5]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="torch-expand"><a href="#torch-expand" class="headerlink" title="torch.expand"></a>torch.expand</h3><p>将tensor扩展维度，自动复制，十分好用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">5</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#tensor([[3, 1, 1],</span></span><br><span class="line"><span class="comment">#        [1, 2, 4]])</span></span><br><span class="line">a = a.unsqueeze(<span class="number">2</span>).expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[3, 3, 3],</span></span><br><span class="line"><span class="string">         [1, 1, 1],</span></span><br><span class="line"><span class="string">         [1, 1, 1]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[1, 1, 1],</span></span><br><span class="line"><span class="string">         [2, 2, 2],</span></span><br><span class="line"><span class="string">         [4, 4, 4]]])&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-repeat"><a href="#torch-repeat" class="headerlink" title="torch.repeat"></a>torch.repeat</h3><p><code>repeat(*sizes) -&gt; Tensor</code>， 重复复制tensor在指定的维度上。其实有点类似于广播操作了？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># x.shape=[3]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>) <span class="comment"># x.shape=[4,6]</span></span><br><span class="line">x.repeat(<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># x.shape=[4,2,3]</span></span><br></pre></td></tr></table></figure>



<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="F-softmax"><a href="#F-softmax" class="headerlink" title="F.softmax"></a>F.softmax</h3><p><code>F.softmax(Tensor, dim=None)</code> 对于多维度矩阵就是 einsum(‘ijk -&gt; jk’, a) = torch.ones(a.shape[1:])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">a = torch.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">a = F.softmax(a, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p>之前一直依赖着<code>huggingface</code>的模型加载<code>from_pretrained</code>，但其实在一般任务场景下，使用<code>torch.load</code>的时候会更多，所以记录一下<code>torch.load</code>方法的使用场景。</p>
<h3 id="torch-load-amp-torch-save"><a href="#torch-load-amp-torch-save" class="headerlink" title="torch.load &amp; torch.save"></a>torch.load &amp; torch.save</h3><p>一般我们将模型的参数保存，而不会去保存整个模型的结构。这里如果需要<strong>部分</strong>加载参数，可以使用<code>strict=False</code>。这里需要注意加载的是<strong>字典dict</strong>，不是模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model ... after training</span></span><br><span class="line">torch.save(model.state_dict(), cached_file_path)</span><br><span class="line">model_state = torch.load(cached_file_path)</span><br><span class="line">model.load_state_dict(model_state, strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>













<h2 id="奇淫技巧"><a href="#奇淫技巧" class="headerlink" title="奇淫技巧"></a>奇淫技巧</h2><h3 id="whole-word-mask"><a href="#whole-word-mask" class="headerlink" title="whole word mask"></a>whole word mask</h3><p>在bert或者其他语言模型中，对一段文本需要先进行tokenize分词操作，而分英文单词的时候，由于OOV问题，会将有些word分成token级别的，比如将<code>trying</code>分成<code>try</code>,<code>##ing</code>。而我们比如在建图或者以word为粒度的时候，就需要将token的输出平均给word了。那么如何操作呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoder_output = encoder_outputs[i]  <span class="comment"># [slen, bert_hidden_size]</span></span><br><span class="line">word_num = <span class="number">123</span></span><br><span class="line">word_index=(torch.arange(word_num) + <span class="number">1</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">words = pos_id[i].unsqueeze(<span class="number">0</span>).expand(mention_num, <span class="number">-1</span>)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = (mention_index == mentions).<span class="built_in">float</span>()  <span class="comment"># [mention_num, slen]</span></span><br><span class="line"><span class="comment"># average word -&gt; mention</span></span><br><span class="line">word_total_numbers = torch.<span class="built_in">sum</span>(select_metrix, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>).expand(<span class="number">-1</span>, slen)  <span class="comment"># [mention_num, slen]</span></span><br><span class="line">select_metrix = torch.where(word_total_numbers &gt; <span class="number">0</span>, select_metrix / word_total_numbers, select_metrix)</span><br><span class="line">x = torch.mm(select_metrix, encoder_output)</span><br></pre></td></tr></table></figure>



</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-04T12:42:54.000Z" title="2020-12-04T12:42:54.000Z">2020-12-04</time>发表</span><span class="level-item"><time dateTime="2020-12-04T12:44:25.242Z" title="2020-12-04T12:44:25.242Z">2020-12-04</time>更新</span><span class="level-item">几秒读完 (大约22个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/04/dgl-%E5%AE%9E%E4%BE%8B%E5%AD%A6%E4%B9%A0.html">dgl 实例学习</a></h1><div class="content"><h1 id="学习开源项目"><a href="#学习开源项目" class="headerlink" title="学习开源项目"></a>学习开源项目</h1><p><a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">项目传送门</a></p>
<h2 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-12-03T13:15:59.000Z" title="2020-12-03T13:15:59.000Z">2020-12-03</time>发表</span><span class="level-item"><time dateTime="2020-12-11T07:58:48.893Z" title="2020-12-11T07:58:48.893Z">2020-12-11</time>更新</span><span class="level-item">1 分钟读完 (大约154个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/uncategorized/2020/12/03/Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读</a></h1><div class="content"><h1 id="Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-论文解读"><a href="#Dialogue-Relation-Extraction-with-Document-level-Heterogeneous-Graph-Attention-Networks-论文解读" class="headerlink" title="Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读"></a>Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks 论文解读</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.05092.pdf">论文传送门</a>，<a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialog-HGAT">代码传送门</a></p>
<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>从对话数据集中抽取出实体之间的联系。给定一段对话，输出指定两个实体之间的关系，总共有<code>37</code>中不同的关系。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>针对对话中不同的信息进行embedding，</p>
<ol>
<li><code>utterance</code> ，对话中的<code>话</code>使用<code>LSTM</code>模型进行encode。</li>
<li><code>speaker</code> </li>
<li><code>argument</code></li>
<li><code>entity-type</code></li>
<li><code>word</code></li>
</ol>
<p>由着四种<code>embedding</code>来构建图。之后拼接学到的<code>argument embedding</code>通过分类器输出关系。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-11-18T09:05:20.000Z" title="2020-11-18T09:05:20.000Z">2020-11-18</time>发表</span><span class="level-item"><time dateTime="2020-12-04T13:21:10.677Z" title="2020-12-04T13:21:10.677Z">2020-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E8%A7%A3/">论解</a></span><span class="level-item">9 分钟读完 (大约1379个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%AE%BA%E8%A7%A3/2020/11/18/ALBERT-%E6%9B%B4%E5%B0%8F%E4%BD%86%E6%98%AF%E6%9B%B4%E6%85%A2%EF%BC%9F.html">ALBERT 更小但是更慢？</a></h1><div class="content"><h1 id="ALBERT-更小但是更慢？"><a href="#ALBERT-更小但是更慢？" class="headerlink" title="ALBERT 更小但是更慢？"></a>ALBERT 更小但是更慢？</h1><p>最近由于参加阅读理解比赛，所以大量测试各种模型，惊奇地发现原本现在阅读理解比赛中SOTA的模型居然是不起眼并且以小模型闻名的<code>ALBERT</code>。这让我对这个“小”模型产生了好奇。从而写一下这份的论文笔记。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>模型越大下游效果越强是众所周知的道理，但是由于硬件设备和显存所限，所以模型不能无限制得放大。这篇文章提出了一个全面领先BERT模型的<code>ALBERT</code>，在比<code>BERT-LARGE</code>参数小的情况下超过了它。</p>
<h2 id="有何区别"><a href="#有何区别" class="headerlink" title="有何区别"></a>有何区别</h2><h3 id="1-embedding-参数减少"><a href="#1-embedding-参数减少" class="headerlink" title="1. embedding 参数减少"></a>1. embedding 参数减少</h3><p>在从one-hot embedding到hidden size embedding有一个$V \times H$的全连接层，这里使用了一个trick，加了一个hidden layer，从而使得全连接层变成了$V\times E + E\times H$。这样子我们就可以用一个很大的$H$了，比如在<code>xxlarge</code>上就是$H=4096$。</p>
<h3 id="2-层间参数共享"><a href="#2-层间参数共享" class="headerlink" title="2.层间参数共享"></a>2.层间参数共享</h3><p>很简单，就是原来模型类似于$F(x) = f_n(f_{n-1}(…f_1(x)))$，但是现在变成了$F(x)=f(f(…f(x)))$。我也在想，虽然$f(x)$是一个非线性的，但这种形式是不是可以有函数去拟合$F(x)$，毕竟重复$f(x)$这不能优化吗？ 去压缩<code>ALBERT</code>模型的大小。</p>
<h3 id="3-SOP"><a href="#3-SOP" class="headerlink" title="3. SOP"></a>3. SOP</h3><p>提出了一个新的self supervised learning 的 objective，既SOP(sentence ordering objectives)。类似于BERT预测两个句子是否是连续的，ALBERT需要预测打乱句子的顺序。</p>
<p><strong>并在在对比中，SOP对于RACE也就是阅读理解任务提高了2.3个点，很哇塞</strong></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分具体暂且不表。我理解的有几点</p>
<ol>
<li>额外的领域内预训练是有益的，但是领域外可能会有害</li>
<li>dropout在模型不会over-fit的情况下其实可以忽略，在batch normalization和dropout可能会损害模型的性能。</li>
<li>hidden size 4096 有可能是ALBERT 性能强的主要原因。</li>
<li>虽然层间参数共享，理论上可以无限深，但是实验发现24层并没有12层效果好。特别宽也没有特别好，这都是玄学调参，很难人工判断。</li>
<li>按理来说fffff(x) 可能会导致每层之间的输出过于相似，但在这里实验发现，并没有。难道是embed layer就很强了？ <strong>猜测</strong></li>
</ol>
<h2 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h2><ol>
<li>稀疏矩阵优化，attention 魔改</li>
<li>SOP是否可以泛用。</li>
</ol>
<hr>
<h2 id="模型解读"><a href="#模型解读" class="headerlink" title="模型解读"></a>模型解读</h2><h3 id="主类"><a href="#主类" class="headerlink" title="主类"></a>主类</h3><p>forward先经过<code>embeddings</code>层再经过<code>encoder</code>层。这里注意，默认输入是用了最后一个隐层所有token的输出再经过一个线性+<code>tanh</code>的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlbertModel</span>(<span class="params">AlbertPreTrainedModel</span>):</span></span><br><span class="line">    config_class = AlbertConfig</span><br><span class="line">    load_tf_weights = load_tf_weights_in_albert</span><br><span class="line">    base_model_prefix = <span class="string">&quot;albert&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"></span><br><span class="line">        self.config = config</span><br><span class="line">        self.embeddings = AlbertEmbeddings(config)</span><br><span class="line">        self.encoder = AlbertTransformer(config)</span><br><span class="line">        <span class="keyword">if</span> add_pooling_layer:</span><br><span class="line">            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">            self.pooler_activation = nn.Tanh()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.pooler = <span class="literal">None</span></span><br><span class="line">            self.pooler_activation = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_resize_token_embeddings</span>(<span class="params">self, new_num_tokens</span>):</span></span><br><span class="line">        old_embeddings = self.embeddings.word_embeddings</span><br><span class="line">        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)</span><br><span class="line">        self.embeddings.word_embeddings = new_embeddings</span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Prunes heads of the model.</span></span><br><span class="line"><span class="string">        heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125;</span></span><br><span class="line"><span class="string">        ALBERT has a different architecture in that its layers are shared across groups, which then has inner groups.</span></span><br><span class="line"><span class="string">        If an ALBERT model has 12 hidden layers and 2 hidden groups, with two inner groups, there</span></span><br><span class="line"><span class="string">        is a total of 4 different layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,</span></span><br><span class="line"><span class="string">        while [2,3] correspond to the two inner groups of the second hidden layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Any layer with in index other than [0,1,2,3] will result in an error.</span></span><br><span class="line"><span class="string">        See base class PreTrainedModel for more information about head pruning</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            group_idx = <span class="built_in">int</span>(layer / self.config.inner_group_num)</span><br><span class="line">            inner_group_idx = <span class="built_in">int</span>(layer - group_idx * self.config.inner_group_num)</span><br><span class="line">            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING.format(&quot;batch_size, sequence_length&quot;))</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(</span></span><br><span class="line">        tokenizer_class=_TOKENIZER_FOR_DOC,</span><br><span class="line">        checkpoint=<span class="string">&quot;albert-base-v2&quot;</span>,</span><br><span class="line">        output_type=BaseModelOutputWithPooling,</span><br><span class="line">        config_class=_CONFIG_FOR_DOC,</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(input_shape, device=device)</span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        extended_attention_mask = attention_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line">        extended_attention_mask = (<span class="number">1.0</span> - extended_attention_mask) * <span class="number">-10000.0</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line"></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, <span class="number">0</span>])) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPooling(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-12-25T06:17:24.000Z" title="2019-12-25T06:17:24.000Z">2019-12-25</time>发表</span><span class="level-item"><time dateTime="2019-12-25T06:57:28.797Z" title="2019-12-25T06:57:28.797Z">2019-12-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%80%83%E7%A0%94/">考研</a></span><span class="level-item">9 分钟读完 (大约1333个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%E8%80%83%E7%A0%94/2019/12/25/%E8%80%83%E7%A0%94%E9%9A%8F%E6%83%B3.html">考研随想</a></h1><div class="content"><h1 id="考研感想"><a href="#考研感想" class="headerlink" title="考研感想"></a>考研感想</h1><h2 id="感触"><a href="#感触" class="headerlink" title="感触"></a>感触</h2><p>现在是12月25号，考完研的第三天。在今天之前的前一个星期里，我每天都是7点左右自然醒。而终于在今天，我睡到了九点，但依然没有睡饱。我想考研带给我了很多，不只是那摞起来像山一样高的辅导书，还有一些随之带来的习惯的改变。</p>
<h2 id="考研开始"><a href="#考研开始" class="headerlink" title="考研开始"></a>考研开始</h2><h3 id="高考之后"><a href="#高考之后" class="headerlink" title="高考之后"></a>高考之后</h3><p>最开始听说考研应该是高考出成绩之后吧。那个时候因为考的太差，是真的对自己的人生失去了信心。之后听说到了大学以后，还可以靠着考研”翻身“（哪有什么翻身）。之后就下定决心，在大学期间冲冲冲，以考研考上浙大为目标。</p>
<h3 id="得知专业之后"><a href="#得知专业之后" class="headerlink" title="得知专业之后"></a>得知专业之后</h3><p>高考分数出来之后，还要进行选学校和选专业，但是因为分是在是太低了。只能随便选选了。最后来到njtech的这个环境科学专业。本着干一行爱一行的心态，我在网上搜索了很多关于环境科学的信息。但是越搜索我的心是越来越拔凉拔凉的。作为环化生材四大坑之首，环境绝对算得上是以先辈们的血一般地事实展示了环境专业的不受待见。之后，在知乎的熏染之下，我选择了转专业！</p>
<h3 id="大一"><a href="#大一" class="headerlink" title="大一"></a>大一</h3><p>大一的过程可以总结为：守望先锋+刷绩点。除此之外，啥活动都没有参加。</p>
<h3 id="转专业以后"><a href="#转专业以后" class="headerlink" title="转专业以后"></a>转专业以后</h3><p>终于在大一下的时候，以环境科学专业第一的成绩转入了计算机科学与技术学院。并且加入了在大一的时候就听说过的acm比赛。但是投身于acm比赛的过程中，我也忘记了我进入大学的初始动机—-考研浙大。</p>
<h3 id="acm区域赛3铜"><a href="#acm区域赛3铜" class="headerlink" title="acm区域赛3铜"></a>acm区域赛3铜</h3><p>可能因为暑假的半学半玩，也可能是因为自己的水品不行智商不够。我们队在2018年的下半年，去了三场区域赛（包括一场EC-Final）全部都拿了铜奖。还记得第一个铜的时候，心里是低落的。但是在听说有学长保研南大，有数理学院的学长保研的浙大计算机直博，并且在网上看了一下，保研中打acm的还算是少数，我保研的心情又高涨了起来。</p>
<h3 id="2019年4月份"><a href="#2019年4月份" class="headerlink" title="2019年4月份"></a>2019年4月份</h3><p>4，5月份是acm邀请赛开始的时候，但也是保研申请学校夏令营的高峰期，这时候不管是哪里的学生，都在疯狂地给老师发邮件，咨询保研的事项。但是我由于acm还要打个邀请赛，精力确实也分了一些。结果就上了个上科大和浙软的夏令营。（其实还是因为acm牌子不够响，也没有啥子科研经历）</p>
<h3 id="暑假"><a href="#暑假" class="headerlink" title="暑假"></a>暑假</h3><p>暑假的时候，去了浙软的夏令营，并且考研数学过了高数，英语单词也背了不少。结果夏令营当场没给我优秀营员，但是在后面的优秀营员名单中是有我的。不过我们学院保研要求的是对方学校的确保接受证明，这tm谁有啊。只能灰溜溜地准备去考研了。</p>
<h3 id="8月"><a href="#8月" class="headerlink" title="8月"></a>8月</h3><p>此时正是考研的冲刺期，但是我考浙大计科，怕是去送命了，（浙大计科408考4门专业课，就算是再扎实的基础，也要个长时间的准备，而且我的数学一直属于刷个绩点的水品，并不能应付考研这个高强度考试。）所以根据我看到的浙软合作企业的项目，我决定tm考浙软，软件就软件，起码也是个浙大。</p>
<h3 id="8-12月"><a href="#8-12月" class="headerlink" title="8-12月"></a>8-12月</h3><p>冲冲冲！疯狂复习，虽然有时候也松懈个1天，但是这四个月我的考研复习效率还是可以的。养成了一些学习的习惯，比如写感想，摘录错题。</p>
<h2 id="考研中"><a href="#考研中" class="headerlink" title="考研中"></a>考研中</h2><p>转眼之间就到了考研的时间了，12月21日-22日。</p>
<h3 id="酒店中凌乱"><a href="#酒店中凌乱" class="headerlink" title="酒店中凌乱"></a>酒店中凌乱</h3><p>虽然我的考点离学校蛮近的，但是我还是选择了订了个酒店，既可以中午的时候躺一会儿，也可以防止意外。所以我在周四的晚上，还去试睡了一下。结果就tm出现意外了。</p>
<blockquote>
<p>论酒店隔音不好+旁边是一对激情似火的情侣的住房体验</p>
</blockquote>
<p>所以我毅然决然地选择周五这个最重要的睡觉时间在宿舍里睡。</p>
<h3 id="周五"><a href="#周五" class="headerlink" title="周五"></a>周五</h3><p>宿舍中辗转难眠，因为周六考的是政治和英语，所以有大量的需要背诵的内容。我选择了在10.30睡觉，之后早上6.20左右爬起来再过一遍。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/10/">10</a></li></ul></nav></div><!--!--><div class="column column-right is-4-tablet is-3-desktop is-3-widescreen  order-3"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/1.gif" alt="Chea Sim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chea Sim</p><p class="is-size-6 is-block">Student from Z</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Ning Bo</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">99</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">83</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/CheaSim" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/CheaSim"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/www.cheasim.com"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1600/"><span class="level-start"><span class="level-item">1600</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/acm/"><span class="level-start"><span class="level-item">acm</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile" href="/categories/acm%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">acm模板</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="level-start"><span class="level-item">ac自动机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cf/"><span class="level-start"><span class="level-item">cf</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/cf1500/"><span class="level-start"><span class="level-item">cf1500</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/dfs/"><span class="level-start"><span class="level-item">dfs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/dp/"><span class="level-start"><span class="level-item">dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hash/"><span class="level-start"><span class="level-item">hash</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/html/"><span class="level-start"><span class="level-item">html</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/pat/"><span class="level-start"><span class="level-item">pat</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%B8%93%E9%A1%B9%E7%BB%83%E4%B9%A0/"><span class="level-start"><span class="level-item">专项练习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E4%BF%9D%E7%A0%94/"><span class="level-start"><span class="level-item">保研</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">前端</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">学习笔记</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="level-start"><span class="level-item">学习计划</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%80%E5%8F%91/"><span class="level-start"><span class="level-item">开发</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%90%AC%E7%A0%96/"><span class="level-start"><span class="level-item">搬砖</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A6%82%E7%8E%87dp/"><span class="level-start"><span class="level-item">概率dp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%A8%A1%E6%9D%BF/"><span class="level-start"><span class="level-item">模板</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="level-start"><span class="level-item">线段树</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94/"><span class="level-start"><span class="level-item">考研</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%80%83%E7%A0%94%E8%B7%AF%E6%BC%AB%E6%BC%AB/"><span class="level-start"><span class="level-item">考研路漫漫</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="level-start"><span class="level-item">蓝桥杯</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E8%A7%A3/"><span class="level-start"><span class="level-item">论解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-20T07:26:03.000Z">2021-01-20</time></p><p class="title"><a href="/uncategorized/2021/01/20/UNIFIEDQA-Crossing-Format-Boundaries-with-a-Single-QA-System-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html">UNIFIEDQA Crossing Format Boundaries with a Single QA System 读书笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-14T06:15:14.000Z">2021-01-14</time></p><p class="title"><a href="/uncategorized/2021/01/14/%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F.html">基于BERT的知识库问答系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-12T13:55:19.000Z">2021-01-12</time></p><p class="title"><a href="/uncategorized/2021/01/12/NLP%E5%9F%BA%E7%A1%80.html">NLP基础</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-07T09:40:38.000Z">2021-01-07</time></p><p class="title"><a href="/uncategorized/2021/01/07/%E5%A6%82%E4%BD%95%E5%9C%A8colab%E7%94%A8pytorch-lightning%E7%99%BD%E5%AB%96TPU.html">如何在colab用pytorch_lightning白嫖TPU</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-11T09:07:57.000Z">2020-12-11</time></p><p class="title"><a href="/uncategorized/2020/12/11/HuggingFace-PretrainTokenizer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">HuggingFace PretrainTokenizer学习笔记</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LCA/"><span class="tag">LCA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QA/"><span class="tag">QA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/acm/"><span class="tag">acm</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ac%E8%87%AA%E5%8A%A8%E6%9C%BA/"><span class="tag">ac自动机</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ajax/"><span class="tag">ajax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bert/"><span class="tag">bert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bilstm/"><span class="tag">bilstm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cf/"><span class="tag">cf</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/colab/"><span class="tag">colab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crf/"><span class="tag">crf</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dfs/"><span class="tag">dfs</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dgl/"><span class="tag">dgl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dp/"><span class="tag">dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dptree/"><span class="tag">dptree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/echarts/"><span class="tag">echarts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gnn/"><span class="tag">gnn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/greedy/"><span class="tag">greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hash/"><span class="tag">hash</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hdoj/"><span class="tag">hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/html/"><span class="tag">html</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/js/"><span class="tag">js</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lca/"><span class="tag">lca</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pat/"><span class="tag">pat</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/re/"><span class="tag">re</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/struts2/"><span class="tag">struts2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tokenizer/"><span class="tag">tokenizer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86/"><span class="tag">二分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E7%94%9F/"><span class="tag">人生</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BF%9D%E7%A0%94/"><span class="tag">保研</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AB%E6%95%B0%E7%A0%81/"><span class="tag">八数码</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2hash/"><span class="tag">字符串hash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"><span class="tag">学习计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B0%8F%E6%8A%80%E5%B7%A7/"><span class="tag">小技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tag">开发</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%BD%E5%8F%96/"><span class="tag">抽取</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">数据结构</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%AE%B5%E5%92%8C/"><span class="tag">最大子段和</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%84%E9%80%A0/"><span class="tag">构造</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E5%BD%A2dp/"><span class="tag">树形dp</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"><span class="tag">树状数组</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8Chdoj/"><span class="tag">树状数组，hdoj</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/"><span class="tag">树链剖分</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87dp/"><span class="tag">概率dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%8B%9F/"><span class="tag">模拟</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E6%9D%BF/"><span class="tag">模板</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%86%E6%90%9C/"><span class="tag">爆搜</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%9B%E5%AE%A2%E7%BD%91/"><span class="tag">牛客网</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%8A%B6%E5%8E%8Bdp/"><span class="tag">状压dp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"><span class="tag">线段树</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"><span class="tag">组合数学</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%86%E8%8A%82/"><span class="tag">细节</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%A9%E7%82%B9/"><span class="tag">缩点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C/"><span class="tag">网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E7%BB%9C%E8%B5%9B/"><span class="tag">网络赛</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%80%83%E7%A0%94/"><span class="tag">考研</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%8C%83%E5%9B%B4/"><span class="tag">范围</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"><span class="tag">蓝桥杯</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A1%A5%E9%A2%98/"><span class="tag">补题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%BE%8B/"><span class="tag">规律</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E5%88%92/"><span class="tag">计划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95/"><span class="tag">计算几何</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E8%A7%A3/"><span class="tag">论解</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%AA%E5%BF%83/"><span class="tag">贪心</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE/"><span class="tag">配置</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%8F%E6%80%A7/"><span class="tag">随性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A2%84%E5%A4%84%E7%90%86/"><span class="tag">预处理</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/1.gif" alt="CheaSim Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 CheaSim</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>